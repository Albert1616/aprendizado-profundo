{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdd34df3",
   "metadata": {},
   "source": [
    "# Detecção de Landmarks\n",
    "\n",
    "Este notebook introduz o conceito de **Detecção de Landmarks (Landmark Detection)**, uma tarefa fundamental em Visão Computacional. O objetivo é identificar e localizar pontos-chave de interesse (landmarks) em um objeto dentro de uma imagem. Esta técnica é a base para muitas aplicações, como rastreamento facial, análise de pose, e reconhecimento de gestos. Inicialmente, construiremos um modelo convolucional simples para encontrar landmarks em um dataset sintético. Em seguida, exploraremos como utilizar bibliotecas de ponta, como o **MediaPipe**, para realizar detecções de landmarks complexas de forma eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b249bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df956639",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9182036d",
   "metadata": {},
   "source": [
    "## Criação do Dataset Sintético\n",
    "\n",
    "Para treinar nosso modelo, precisamos de dados. Criaremos um dataset sintético de imagens simples e definiremos os landmarks.\n",
    "\n",
    "A classe `SyntheticArmDataset` gerará um braço articulado. O processo para gerar cada amostra é:\n",
    "\n",
    "1.  Definir um ponto de origem (ombro) em uma posição aleatória.\n",
    "2.  Definir um comprimento (len1) e um ângulo (theta1) aleatórios para o segmento do braço.\n",
    "3.  Calcular a posição do cotovelo usando trigonometria.\n",
    "4.  Definir um comprimento (len2) e um ângulo *relativo* (theta2) aleatórios para o antebraço.\n",
    "5.  Calcular a posição do pulso com base na posição do cotovelo e nos novos parâmetros.\n",
    "6.  Desenhar os dois segmentos na imagem usando `cv2.line` com espessura para simular cilindros.\n",
    "7.  Armazenar os 3 landmarks (6 coordenadas) e normalizá-los para o intervalo `[-1, 1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c8984",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticArmDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000, img_size=(96, 96), num_landmarks=3):\n",
    "        self.num_samples = num_samples\n",
    "        self.img_size = img_size\n",
    "        self.num_landmarks = num_landmarks\n",
    "        self.img_height, self.img_width = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Criar uma imagem preta\n",
    "        image = np.zeros((self.img_height, self.img_width), dtype=np.float32)\n",
    "        h, w = self.img_size\n",
    "        \n",
    "        # Parâmetros do braço\n",
    "        segment_thickness = 4\n",
    "        len1 = np.random.uniform(h * 0.2, h * 0.4) # Comprimento braço\n",
    "        len2 = np.random.uniform(h * 0.2, h * 0.4) # Comprimento antebraço\n",
    "        \n",
    "        # Ângulos\n",
    "        theta1 = np.random.uniform(0, 2 * math.pi) # Ângulo do braço\n",
    "        theta2 = np.random.uniform(-math.pi / 1.5, math.pi / 1.5) # Ângulo relativo do cotovelo\n",
    "        \n",
    "        # Ponto 1: Ombro (Shoulder)\n",
    "        # Manter o ombro um pouco longe das bordas\n",
    "        x_s = np.random.randint(w * 0.2, w * 0.8)\n",
    "        y_s = np.random.randint(h * 0.2, h * 0.8)\n",
    "        \n",
    "        # Ponto 2: Cotovelo (Elbow)\n",
    "        x_e = x_s + len1 * math.cos(theta1)\n",
    "        y_e = y_s + len1 * math.sin(theta1)\n",
    "        \n",
    "        # Ponto 3: Pulso (Wrist)\n",
    "        # O ângulo do antebraço é a soma do ângulo do braço + ângulo relativo do cotovelo\n",
    "        x_w = x_e + len2 * math.cos(theta1 + theta2)\n",
    "        y_w = y_e + len2 * math.sin(theta1 + theta2)\n",
    "        \n",
    "        # Garantir que os pontos estão dentro dos limites da imagem\n",
    "        points = np.array([x_s, y_s, x_e, y_e, x_w, y_w])\n",
    "        points = np.clip(points, 0, max(h, w) - 1).astype(int)\n",
    "        \n",
    "        (x_s, y_s, x_e, y_e, x_w, y_w) = points\n",
    "        \n",
    "        # Desenhar os \"cilindros\" (linhas grossas)\n",
    "        cv2.line(image, (x_s, y_s), (x_e, y_e), (255), segment_thickness)\n",
    "        cv2.line(image, (x_e, y_e), (x_w, y_w), (255), segment_thickness)\n",
    "        \n",
    "        # Normalizar imagem\n",
    "        image = image / 255.0\n",
    "        image_tensor = torch.from_numpy(image).unsqueeze(0).float() # (1, H, W)\n",
    "        \n",
    "        # Preparar landmarks (x1, y1, x2, y2, x3, y3)\n",
    "        # Usamos os valores *antes* do clip para a regressão,\n",
    "        # mas convertemos os 'points' (pós-clip) para float32 para normalização.\n",
    "        landmarks = points.astype(np.float32)\n",
    "        \n",
    "        # Normalizar landmarks para [-1, 1]\n",
    "        landmarks[0::2] = (landmarks[0::2] - w / 2) / (w / 2) # Coordenadas X\n",
    "        landmarks[1::2] = (landmarks[1::2] - h / 2) / (h / 2) # Coordenadas Y\n",
    "        \n",
    "        landmarks_tensor = torch.from_numpy(landmarks)\n",
    "        \n",
    "        return image_tensor, landmarks_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49621263",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (128, 128)\n",
    "NUM_LANDMARKS = 3\n",
    "NUM_OUTPUTS = NUM_LANDMARKS * 2\n",
    "\n",
    "# Criar datasets e dataloaders\n",
    "train_dataset = SyntheticArmDataset(num_samples=10000, img_size=IMG_SIZE, num_landmarks=NUM_LANDMARKS)\n",
    "val_dataset = SyntheticArmDataset(num_samples=500, img_size=IMG_SIZE, num_landmarks=NUM_LANDMARKS)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df419c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar um exemplo\n",
    "sample_image, sample_landmarks = train_dataset[0]\n",
    "\n",
    "# Desnormalizar landmarks para visualização\n",
    "lm_unnormalized = sample_landmarks.numpy().copy()\n",
    "lm_unnormalized[0::2] = (lm_unnormalized[0::2] * (IMG_SIZE[1] / 2)) + (IMG_SIZE[1] / 2)\n",
    "lm_unnormalized[1::2] = (lm_unnormalized[1::2] * (IMG_SIZE[0] / 2)) + (IMG_SIZE[0] / 2)\n",
    "\n",
    "xs = lm_unnormalized[0::2] # [x_s, x_e, x_w]\n",
    "ys = lm_unnormalized[1::2] # [y_s, y_e, y_w]\n",
    "\n",
    "plt.imshow(sample_image.squeeze(), cmap='gray')\n",
    "plt.scatter(xs, ys, c='red', s=40)\n",
    "plt.title(\"Exemplo de Braço Sintético e 3 Landmarks\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b68ea37",
   "metadata": {},
   "source": [
    "## Definição do Modelo CNN\n",
    "\n",
    "Definimos uma arquitetura de CNN simples. A rede consiste em blocos convolucionais para extração de características, seguidos por camadas totalmente conectadas (Lineares) para regredir as coordenadas dos landmarks.\n",
    "\n",
    "A camada de saída final deve ter `NUM_OUTPUTS` neurônios, correspondendo às `NUM_LANDMARKS * 2` coordenadas (x, y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071c9dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarkCNN(nn.Module):\n",
    "    def __init__(self, img_channels, num_outputs):\n",
    "        super(LandmarkCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.AdaptiveAvgPool2d((8, 8)) # (N, 128, 8, 8)\n",
    "        )\n",
    "        \n",
    "        self.flatten_features = 128 * 8 * 8\n",
    "        \n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(self.flatten_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_outputs)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.regressor(x)\n",
    "        return x\n",
    "\n",
    "model = LandmarkCNN(img_channels=1, num_outputs=NUM_OUTPUTS).to(device)\n",
    "print(f\"Modelo criado com {NUM_OUTPUTS} saídas.\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d9cc15",
   "metadata": {},
   "source": [
    "## Treinamento do Modelo\n",
    "\n",
    "Para treinar o modelo, precisamos definir uma função de perda (Loss Function) e um otimizador.\n",
    "\n",
    "### Função de Perda\n",
    "\n",
    "Como esta é uma tarefa de regressão (prever coordenadas), a função de perda apropriada é o **Erro Quadrático Médio (Mean Squared Error - MSE)**. O MSE calcula a média das diferenças quadráticas entre as coordenadas previstas e as coordenadas reais (ground truth).\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{k} \\sum_{j=1}^{k} (y_{ij} - \\hat{y}_{ij})^2\n",
    "$$\n",
    "\n",
    "Onde:\n",
    "$N$ é o número de amostras no batch.\n",
    "$k$ é o número de valores de saída (nossos `NUM_OUTPUTS`).\n",
    "$y_{ij}$ é a j-ésima coordenada real para a i-ésima amostra.\n",
    "$\\hat{y}_{ij}$ é a j-ésima coordenada prevista para a i-ésima amostra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e64b64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de323bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, landmarks in train_loader:\n",
    "        images = images.to(device)\n",
    "        landmarks = landmarks.to(device)\n",
    "        \n",
    "        # Zerar gradientes\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, landmarks)\n",
    "        \n",
    "        # Backward pass e otimização\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    \n",
    "    # Validação\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, landmarks in val_loader:\n",
    "            images = images.to(device)\n",
    "            landmarks = landmarks.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, landmarks)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            \n",
    "    val_loss = val_loss / len(val_dataset)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.6f}, Val Loss: {val_loss:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca05429",
   "metadata": {},
   "source": [
    "## Visualização dos Resultados\n",
    "\n",
    "Vamos agora pegar algumas amostras do conjunto de validação, passar pelo modelo treinado e visualizar as previsões de landmarks em comparação com o ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d904938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, dataset, num_samples=5):\n",
    "    model.eval()\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 5))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Pegar amostra\n",
    "        image_tensor, gt_landmarks = dataset[i]\n",
    "        image_tensor_batch = image_tensor.unsqueeze(0).to(device) # Adicionar dimensão do batch\n",
    "        \n",
    "        # Fazer previsão\n",
    "        with torch.no_grad():\n",
    "            pred_landmarks = model(image_tensor_batch).cpu().squeeze()\n",
    "            \n",
    "        # Preparar imagem para plotagem\n",
    "        image = image_tensor.squeeze().numpy()\n",
    "        \n",
    "        # Desnormalizar Ground Truth (shape [6])\n",
    "        gt_coords = gt_landmarks.numpy().copy()\n",
    "        gt_xs = (gt_coords[0::2] * (IMG_SIZE[1] / 2)) + (IMG_SIZE[1] / 2)\n",
    "        gt_ys = (gt_coords[1::2] * (IMG_SIZE[0] / 2)) + (IMG_SIZE[0] / 2)\n",
    "        \n",
    "        # Desnormalizar Previsões (shape [6])\n",
    "        pred_coords = pred_landmarks.detach().numpy().copy()\n",
    "        pred_xs = (pred_coords[0::2] * (IMG_SIZE[1] / 2)) + (IMG_SIZE[1] / 2)\n",
    "        pred_ys = (pred_coords[1::2] * (IMG_SIZE[0] / 2)) + (IMG_SIZE[0] / 2)\n",
    "\n",
    "        # Plotar\n",
    "        ax = axes[i]\n",
    "        ax.imshow(image, cmap='gray', interpolation=None)\n",
    "        ax.scatter(gt_xs, gt_ys, c='red', s=50, label='Ground Truth', marker='x')\n",
    "        ax.scatter(pred_xs, pred_ys, c='blue', s=50, label='Prediction', marker='+')\n",
    "        ax.set_title(f\"Amostra {i}\")\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', ncol=2)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Ajustar para a legenda\n",
    "    plt.show()\n",
    "\n",
    "# Visualizar nos dados de validação\n",
    "visualize_predictions(model, val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3a12e4",
   "metadata": {},
   "source": [
    "# Detecção de Landmarks com MediaPipe\n",
    "\n",
    "Construir e treinar modelos de landmarks do zero é instrutivo, mas para aplicações do mundo real, muitas vezes utilizamos soluções pré-treinadas que são altamente otimizadas e precisas. O **MediaPipe** do Google é uma biblioteca de código aberto fantástica para pipelines de ML multimodais (vídeo, áudio, texto).\n",
    "\n",
    "Ele oferece modelos robustos e em tempo real para tarefas como detecção de face (Face Mesh), mãos (Hand Tracking) e pose (Pose Estimation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3eb538",
   "metadata": {},
   "source": [
    "## Instalação\n",
    "\n",
    "Inicialmente, instalamos as dependências necessárias (`opencv-python`, `mediapipe`) e baixamos as imagens de exemplo (`person-full-body.jpg`, `hand.jpg`) que serão utilizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacdeae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python\n",
    "# !pip install mediapipe\n",
    "# !wget https://raw.githubusercontent.com/spmallick/learnopencv/blob/master/Introduction-to-MediaPipe/person-full-body.jpg\n",
    "# !wget https://raw.githubusercontent.com/spmallick/learnopencv/master/Introduction-to-MediaPipe/hand.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f03f665",
   "metadata": {},
   "source": [
    "### Importações Principais\n",
    "\n",
    "Agora, importamos as bibliotecas que serão usadas:\n",
    "* `cv2`: OpenCV, para manipulação de imagens (leitura, conversão de cores).\n",
    "* `mediapipe`: A biblioteca principal para acessar os modelos de detecção.\n",
    "* `matplotlib.pyplot`: Para visualização das imagens no notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b62a7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0074c9",
   "metadata": {},
   "source": [
    "## Detecção de Pose (Corpo Inteiro)\n",
    "\n",
    "Começamos com a solução `mp.solutions.pose`.\n",
    "\n",
    "### Carregar e Visualizar Imagem de Teste\n",
    "\n",
    "Carregamos a imagem `person-full-body.jpg` usando o OpenCV. Note que o OpenCV carrega imagens no formato BGR (Blue-Green-Red). Para exibição correta com o Matplotlib (que espera RGB), usamos a sintaxe `[...,::-1]` para inverter a ordem dos canais de cor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ef9c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('person-full-body.jpg')\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(img[...,::-1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3be7b3",
   "metadata": {},
   "source": [
    "### Inicialização do Modelo de Pose\n",
    "\n",
    "Instanciamos o modelo `mp_pose.Pose`. Os parâmetros de configuração são:\n",
    "\n",
    "* `static_image_mode=True`: Otimiza o pipeline para imagens estáticas (vídeos seriam `False`).\n",
    "* `model_complexity=2`: Define a complexidade do modelo (0, 1 ou 2). Modelos mais complexos são mais precisos, mas mais lentos.\n",
    "* `enable_segmentation=True`: Habilita o modelo a gerar também uma máscara de segmentação da pessoa (além dos landmarks).\n",
    "* `min_detection_confidence=0.5`: A confiança mínima (50%) para que uma detecção de pessoa seja considerada válida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4a8833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa a solução de pose do MediaPipe\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "pose = mp_pose.Pose(static_image_mode=True,\n",
    "                    model_complexity=2,\n",
    "                    enable_segmentation=True,\n",
    "                    min_detection_confidence=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42838973",
   "metadata": {},
   "source": [
    "### Processamento da Imagem\n",
    "\n",
    "Os modelos MediaPipe esperam imagens no formato RGB. Portanto, convertemos a imagem `img` (BGR) para `rgb_img` (RGB) usando `cv2.cvtColor` antes de passá-la para o método `pose.process()`. O objeto `results` conterá todos os dados de saída do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9091d246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte a imagem BGR para RGB\n",
    "rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Processa a imagem e obtém os resultados\n",
    "results = pose.process(rgb_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f737dd",
   "metadata": {},
   "source": [
    "### Desenho dos Landmarks\n",
    "\n",
    "Para visualizar o resultado, criamos uma cópia da imagem original. Em seguida, usamos a utilidade `mp_drawing.draw_landmarks` para desenhar:\n",
    "\n",
    "1.  Os landmarks (pontos) detectados, acessados via `results.pose_landmarks`.\n",
    "2.  As conexões entre os landmarks, definidas em `mp_pose.POSE_CONNECTIONS`.\n",
    "\n",
    "Personalizamos a aparência dos pontos e conexões usando `mp_drawing.DrawingSpec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4e3e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa as utilidades de desenho\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "annotated_img = img.copy()\n",
    "\n",
    "# Desenha os landmarks da pose na imagem\n",
    "mp_drawing.draw_landmarks(annotated_img,\n",
    "                          results.pose_landmarks,\n",
    "                          mp_pose.POSE_CONNECTIONS,\n",
    "                          # Especifica o estilo dos pontos\n",
    "                          landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=3, circle_radius=3),\n",
    "                          # Especifica o estilo das conexões\n",
    "                          connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 200, 0), thickness=5, circle_radius=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22736cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibe a imagem anotada\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.imshow(annotated_img[...,::-1])\n",
    "plt.title('Pose');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342e1535",
   "metadata": {},
   "source": [
    "## Detecção de Landmarks de Mão\n",
    "\n",
    "A seguir, utilizamos a solução `mp.solutions.hands` para detectar os 21 landmarks de cada mão.\n",
    "\n",
    "### Inicialização do Modelo de Mãos\n",
    "\n",
    "Inicializamos `mp_hands.Hands`, especificando:\n",
    "* `static_image_mode=True`: Para imagens estáticas.\n",
    "* `max_num_hands=2`: O número máximo de mãos a detectar.\n",
    "* `min_detection_confidence=0.5`: Confiança mínima de 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899bee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa a solução de mãos do MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84193954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega a imagem da mão\n",
    "hand_img = cv2.imread('hand.jpg')\n",
    "\n",
    "# Redimensiona a imagem para processamento mais rápido\n",
    "hand_img = cv2.resize(hand_img, None, fx=0.1, fy=0.1)\n",
    "\n",
    "# Converte BGR para RGB\n",
    "rgb_img = cv2.cvtColor(hand_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Processa a imagem\n",
    "results = hands.process(rgb_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97587771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa os estilos de desenho padrão\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "annotated_img = hand_img.copy()\n",
    "\n",
    "# Itera sobre cada mão detectada\n",
    "if results.multi_hand_landmarks:\n",
    "  for hand_landmarks in results.multi_hand_landmarks:\n",
    "    mp_drawing.draw_landmarks(annotated_img,\n",
    "                              hand_landmarks,\n",
    "                              mp_hands.HAND_CONNECTIONS,\n",
    "                              mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                              mp_drawing_styles.get_default_hand_connections_style())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17ba02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "plt.subplot(121)\n",
    "plt.imshow(hand_img[...,::-1])\n",
    "plt.title('Hand Image')\n",
    "plt.subplot(122)\n",
    "plt.imshow(annotated_img[...,::-1])\n",
    "plt.title('Hand Detection');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2d72d5",
   "metadata": {},
   "source": [
    "## Detecção de Malha Facial (Face Mesh)\n",
    "\n",
    "Finalmente, exploramos o `mp.solutions.face_mesh`, que detecta 468 landmarks no rosto, permitindo aplicações detalhadas como rastreamento de íris e contornos faciais.\n",
    "\n",
    "### Inicialização do Modelo Face Mesh\n",
    "\n",
    "Inicializamos `mp_face_mesh.FaceMesh`. O parâmetro de destaque é:\n",
    "* `refine_landmarks=True`: Isso habilita o modelo a refinar as coordenadas ao redor dos olhos e lábios ativa a detecção dos landmarks da **íris**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f61aa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa a solução de malha facial\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, \n",
    "                                  max_num_faces=1, \n",
    "                                  refine_landmarks=True, \n",
    "                                  min_detection_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09888569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte BGR para RGB\n",
    "rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Processa a imagem\n",
    "results = face_mesh.process(rgb_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b587f7",
   "metadata": {},
   "source": [
    "### Desenho das Variações da Malha Facial\n",
    "\n",
    "O MediaPipe Face Mesh fornece diferentes conjuntos de conexões para visualização. Para demonstrar, criamos três cópias da imagem.\n",
    "\n",
    "Iteramos sobre os rostos detectados (`results.multi_face_landmarks`) e desenhamos:\n",
    "1.  `FACEMESH_TESSELATION`: A malha facial completa (Tesselation).\n",
    "2.  `FACEMESH_CONTOURS`: Apenas os contornos do rosto, olhos, nariz e boca.\n",
    "3.  `FACEMESH_IRISES`: Os landmarks específicos da íris (só funciona se `refine_landmarks=True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fd3b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria cópias da imagem para diferentes visualizações\n",
    "img_mesh = img.copy()\n",
    "img_contours = img.copy()\n",
    "img_iris = img.copy()\n",
    "\n",
    "if results.multi_face_landmarks:\n",
    "  for face_landmarks in results.multi_face_landmarks:\n",
    "    # 1. Desenha a malha (Tesselation)\n",
    "    mp_drawing.draw_landmarks(img_mesh,\n",
    "                              face_landmarks,\n",
    "                              mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                              landmark_drawing_spec=None,\n",
    "                              connection_drawing_spec=mp_drawing.DrawingSpec(color=(0, 200, 0), thickness=1, circle_radius=1))\n",
    "    \n",
    "    # 2. Desenha os contornos\n",
    "    mp_drawing.draw_landmarks(img_contours,\n",
    "                              face_landmarks,\n",
    "                              mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                              landmark_drawing_spec=None,\n",
    "                              connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style())\n",
    "    \n",
    "    # 3. Desenha as íris (requer refine_landmarks=True)\n",
    "    mp_drawing.draw_landmarks(img_iris,\n",
    "                              face_landmarks,\n",
    "                              mp_face_mesh.FACEMESH_IRISES,\n",
    "                              landmark_drawing_spec=None,\n",
    "                              connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_iris_connections_style())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a39f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibe as três variações de malha facial, com zoom na região do rosto\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "# Fatiamento da imagem [Y_start:Y_end, X_start:X_end]\n",
    "face_region = (slice(0, 500), slice(700, 1350))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.imshow(img_mesh[...,::-1][face_region])\n",
    "plt.title('Face Mesh');\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.imshow(img_contours[...,::-1][face_region])\n",
    "plt.title('Face Contours');\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.imshow(img_iris[...,::-1][face_region])\n",
    "plt.title('Iris');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
