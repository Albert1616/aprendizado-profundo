{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdd34df3",
   "metadata": {},
   "source": [
    "# Detecção de Landmarks\n",
    "\n",
    "Este notebook introduz o conceito de **Detecção de Landmarks (Landmark Detection)**, uma tarefa fundamental em Visão Computacional. O objetivo é identificar e localizar pontos-chave de interesse (landmarks) em um objeto dentro de uma imagem. Esta técnica é a base para muitas aplicações, como rastreamento facial, análise de pose, e reconhecimento de gestos. Inicialmente, construiremos um modelo convolucional simples para encontrar landmarks em um dataset sintético. Em seguida, exploraremos como utilizar bibliotecas de ponta, como o **MediaPipe**, para realizar detecções de landmarks complexas de forma eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b249bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df956639",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9182036d",
   "metadata": {},
   "source": [
    "## Criação do Dataset Sintético\n",
    "\n",
    "Para treinar nosso modelo, precisamos de dados. Criaremos um dataset sintético de imagens simples e definiremos os landmarks.\n",
    "\n",
    "A classe `SyntheticArmDataset` gerará um braço articulado. O processo para gerar cada amostra é:\n",
    "\n",
    "1.  Definir um ponto de origem (ombro) em uma posição aleatória.\n",
    "2.  Definir um comprimento (len1) e um ângulo (theta1) aleatórios para o segmento do braço.\n",
    "3.  Calcular a posição do cotovelo usando trigonometria.\n",
    "4.  Definir um comprimento (len2) e um ângulo *relativo* (theta2) aleatórios para o antebraço.\n",
    "5.  Calcular a posição do pulso com base na posição do cotovelo e nos novos parâmetros.\n",
    "6.  Desenhar os dois segmentos na imagem usando `cv2.line` com espessura para simular cilindros.\n",
    "7.  Armazenar os 3 landmarks (6 coordenadas) e normalizá-los para o intervalo `[-1, 1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c8984",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticArmDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000, img_size=(96, 96), num_landmarks=3):\n",
    "        self.num_samples = num_samples\n",
    "        self.img_size = img_size\n",
    "        self.num_landmarks = num_landmarks\n",
    "        self.img_height, self.img_width = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Criar uma imagem preta\n",
    "        image = np.zeros((self.img_height, self.img_width), dtype=np.float32)\n",
    "        h, w = self.img_size\n",
    "        \n",
    "        # Parâmetros do braço\n",
    "        segment_thickness = 4\n",
    "        len1 = np.random.uniform(h * 0.2, h * 0.4) # Comprimento braço\n",
    "        len2 = np.random.uniform(h * 0.2, h * 0.4) # Comprimento antebraço\n",
    "        \n",
    "        # Ângulos\n",
    "        theta1 = np.random.uniform(0, 2 * math.pi) # Ângulo do braço\n",
    "        theta2 = np.random.uniform(-math.pi / 1.5, math.pi / 1.5) # Ângulo relativo do cotovelo\n",
    "        \n",
    "        # Ponto 1: Ombro (Shoulder)\n",
    "        # Manter o ombro um pouco longe das bordas\n",
    "        x_s = np.random.randint(w * 0.2, w * 0.8)\n",
    "        y_s = np.random.randint(h * 0.2, h * 0.8)\n",
    "        \n",
    "        # Ponto 2: Cotovelo (Elbow)\n",
    "        x_e = x_s + len1 * math.cos(theta1)\n",
    "        y_e = y_s + len1 * math.sin(theta1)\n",
    "        \n",
    "        # Ponto 3: Pulso (Wrist)\n",
    "        # O ângulo do antebraço é a soma do ângulo do braço + ângulo relativo do cotovelo\n",
    "        x_w = x_e + len2 * math.cos(theta1 + theta2)\n",
    "        y_w = y_e + len2 * math.sin(theta1 + theta2)\n",
    "        \n",
    "        # Garantir que os pontos estão dentro dos limites da imagem\n",
    "        points = np.array([x_s, y_s, x_e, y_e, x_w, y_w])\n",
    "        points = np.clip(points, 0, max(h, w) - 1).astype(int)\n",
    "        \n",
    "        (x_s, y_s, x_e, y_e, x_w, y_w) = points\n",
    "        \n",
    "        # Desenhar os \"cilindros\" (linhas grossas)\n",
    "        cv2.line(image, (x_s, y_s), (x_e, y_e), (255), segment_thickness)\n",
    "        cv2.line(image, (x_e, y_e), (x_w, y_w), (255), segment_thickness)\n",
    "        \n",
    "        # Normalizar imagem\n",
    "        image = image / 255.0\n",
    "        image_tensor = torch.from_numpy(image).unsqueeze(0).float() # (1, H, W)\n",
    "        \n",
    "        # Preparar landmarks (x1, y1, x2, y2, x3, y3)\n",
    "        # Usamos os valores *antes* do clip para a regressão,\n",
    "        # mas convertemos os 'points' (pós-clip) para float32 para normalização.\n",
    "        landmarks = points.astype(np.float32)\n",
    "        \n",
    "        # Normalizar landmarks para [-1, 1]\n",
    "        landmarks[0::2] = (landmarks[0::2] - w / 2) / (w / 2) # Coordenadas X\n",
    "        landmarks[1::2] = (landmarks[1::2] - h / 2) / (h / 2) # Coordenadas Y\n",
    "        \n",
    "        landmarks_tensor = torch.from_numpy(landmarks)\n",
    "        \n",
    "        return image_tensor, landmarks_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49621263",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (128, 128)\n",
    "NUM_LANDMARKS = 3\n",
    "NUM_OUTPUTS = NUM_LANDMARKS * 2\n",
    "\n",
    "# Criar datasets e dataloaders\n",
    "train_dataset = SyntheticArmDataset(num_samples=10000, img_size=IMG_SIZE, num_landmarks=NUM_LANDMARKS)\n",
    "val_dataset = SyntheticArmDataset(num_samples=500, img_size=IMG_SIZE, num_landmarks=NUM_LANDMARKS)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df419c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar um exemplo\n",
    "sample_image, sample_landmarks = train_dataset[0]\n",
    "\n",
    "# Desnormalizar landmarks para visualização\n",
    "lm_unnormalized = sample_landmarks.numpy().copy()\n",
    "lm_unnormalized[0::2] = (lm_unnormalized[0::2] * (IMG_SIZE[1] / 2)) + (IMG_SIZE[1] / 2)\n",
    "lm_unnormalized[1::2] = (lm_unnormalized[1::2] * (IMG_SIZE[0] / 2)) + (IMG_SIZE[0] / 2)\n",
    "\n",
    "xs = lm_unnormalized[0::2] # [x_s, x_e, x_w]\n",
    "ys = lm_unnormalized[1::2] # [y_s, y_e, y_w]\n",
    "\n",
    "plt.imshow(sample_image.squeeze(), cmap='gray')\n",
    "plt.scatter(xs, ys, c='red', s=40)\n",
    "plt.title(\"Exemplo de Braço Sintético e 3 Landmarks\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b68ea37",
   "metadata": {},
   "source": [
    "## Definição do Modelo CNN\n",
    "\n",
    "Definimos uma arquitetura de CNN simples. A rede consiste em blocos convolucionais para extração de características, seguidos por camadas totalmente conectadas (Lineares) para regredir as coordenadas dos landmarks.\n",
    "\n",
    "A camada de saída final deve ter `NUM_OUTPUTS` neurônios, correspondendo às `NUM_LANDMARKS * 2` coordenadas (x, y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071c9dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarkCNN(nn.Module):\n",
    "    def __init__(self, img_channels, num_outputs):\n",
    "        super(LandmarkCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.AdaptiveAvgPool2d((8, 8)) # (N, 128, 8, 8)\n",
    "        )\n",
    "        \n",
    "        self.flatten_features = 128 * 8 * 8\n",
    "        \n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(self.flatten_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_outputs)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.regressor(x)\n",
    "        return x\n",
    "\n",
    "model = LandmarkCNN(img_channels=1, num_outputs=NUM_OUTPUTS).to(device)\n",
    "print(f\"Modelo criado com {NUM_OUTPUTS} saídas.\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d9cc15",
   "metadata": {},
   "source": [
    "## Treinamento do Modelo\n",
    "\n",
    "Para treinar o modelo, precisamos definir uma função de perda (Loss Function) e um otimizador.\n",
    "\n",
    "### Função de Perda\n",
    "\n",
    "Como esta é uma tarefa de regressão (prever coordenadas), a função de perda apropriada é o **Erro Quadrático Médio (Mean Squared Error - MSE)**. O MSE calcula a média das diferenças quadráticas entre as coordenadas previstas e as coordenadas reais (ground truth).\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{k} \\sum_{j=1}^{k} (y_{ij} - \\hat{y}_{ij})^2\n",
    "$$\n",
    "\n",
    "Onde:\n",
    "$N$ é o número de amostras no batch.\n",
    "$k$ é o número de valores de saída (nossos `NUM_OUTPUTS`).\n",
    "$y_{ij}$ é a j-ésima coordenada real para a i-ésima amostra.\n",
    "$\\hat{y}_{ij}$ é a j-ésima coordenada prevista para a i-ésima amostra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e64b64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de323bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, landmarks in train_loader:\n",
    "        images = images.to(device)\n",
    "        landmarks = landmarks.to(device)\n",
    "        \n",
    "        # Zerar gradientes\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, landmarks)\n",
    "        \n",
    "        # Backward pass e otimização\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    \n",
    "    # Validação\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, landmarks in val_loader:\n",
    "            images = images.to(device)\n",
    "            landmarks = landmarks.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, landmarks)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            \n",
    "    val_loss = val_loss / len(val_dataset)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.6f}, Val Loss: {val_loss:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca05429",
   "metadata": {},
   "source": [
    "## Visualização dos Resultados\n",
    "\n",
    "Vamos agora pegar algumas amostras do conjunto de validação, passar pelo modelo treinado e visualizar as previsões de landmarks em comparação com o ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d904938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, dataset, num_samples=5):\n",
    "    model.eval()\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 5))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Pegar amostra\n",
    "        image_tensor, gt_landmarks = dataset[i]\n",
    "        image_tensor_batch = image_tensor.unsqueeze(0).to(device) # Adicionar dimensão do batch\n",
    "        \n",
    "        # Fazer previsão\n",
    "        with torch.no_grad():\n",
    "            pred_landmarks = model(image_tensor_batch).cpu().squeeze()\n",
    "            \n",
    "        # Preparar imagem para plotagem\n",
    "        image = image_tensor.squeeze().numpy()\n",
    "        \n",
    "        # Desnormalizar Ground Truth (shape [6])\n",
    "        gt_coords = gt_landmarks.numpy().copy()\n",
    "        gt_xs = (gt_coords[0::2] * (IMG_SIZE[1] / 2)) + (IMG_SIZE[1] / 2)\n",
    "        gt_ys = (gt_coords[1::2] * (IMG_SIZE[0] / 2)) + (IMG_SIZE[0] / 2)\n",
    "        \n",
    "        # Desnormalizar Previsões (shape [6])\n",
    "        pred_coords = pred_landmarks.detach().numpy().copy()\n",
    "        pred_xs = (pred_coords[0::2] * (IMG_SIZE[1] / 2)) + (IMG_SIZE[1] / 2)\n",
    "        pred_ys = (pred_coords[1::2] * (IMG_SIZE[0] / 2)) + (IMG_SIZE[0] / 2)\n",
    "\n",
    "        # Plotar\n",
    "        ax = axes[i]\n",
    "        ax.imshow(image, cmap='gray', interpolation=None)\n",
    "        ax.scatter(gt_xs, gt_ys, c='red', s=50, label='Ground Truth', marker='x')\n",
    "        ax.scatter(pred_xs, pred_ys, c='blue', s=50, label='Prediction', marker='+')\n",
    "        ax.set_title(f\"Amostra {i}\")\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', ncol=2)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Ajustar para a legenda\n",
    "    plt.show()\n",
    "\n",
    "# Visualizar nos dados de validação\n",
    "visualize_predictions(model, val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3a12e4",
   "metadata": {},
   "source": [
    "# Detecção de Landmarks com MediaPipe\n",
    "\n",
    "Construir e treinar modelos de landmarks do zero é instrutivo, mas para aplicações do mundo real, muitas vezes utilizamos soluções pré-treinadas que são altamente otimizadas e precisas. O **MediaPipe** do Google é uma biblioteca de código aberto fantástica para pipelines de ML multimodais (vídeo, áudio, texto).\n",
    "\n",
    "Ele oferece modelos robustos e em tempo real para tarefas como detecção de face (Face Mesh), mãos (Hand Tracking) e pose (Pose Estimation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3eb538",
   "metadata": {},
   "source": [
    "## Instalação\n",
    "\n",
    "Inicialmente, instalamos as dependências necessárias (`opencv-python`, `mediapipe`) e baixamos as imagens de exemplo (`person-full-body.jpg`, `hand.jpg`) que serão utilizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacdeae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python\n",
    "# !pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b62a7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Silenciar avisos do MediaPipe (opcional)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0074c9",
   "metadata": {},
   "source": [
    "## Detecção de Pose (Corpo Inteiro)\n",
    "\n",
    "Nesta seção, inicializamos a solução `mp.solutions.pose`. Este modelo detecta 33 landmarks do corpo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e04f47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar módulos específicos de Pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2e8315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar a imagem\n",
    "image_pose = cv2.imread('pose.jpg')\n",
    "image_pose = cv2.cvtColor(image_pose, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15650b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o modelo e processar a imagem\n",
    "with mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5) as pose:\n",
    "\n",
    "    # Processar\n",
    "    results = pose.process(image_pose)\n",
    "\n",
    "    # Desenhar os landmarks\n",
    "    annotated_image = image_pose.copy()\n",
    "    mp_drawing.draw_landmarks(\n",
    "        annotated_image,\n",
    "        results.pose_landmarks,\n",
    "        mp_pose.POSE_CONNECTIONS,\n",
    "        landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0,0,255), thickness=2, circle_radius=2),\n",
    "        connection_drawing_spec=mp_drawing.DrawingSpec(color=(0,255,0), thickness=2)\n",
    "    )\n",
    "    \n",
    "    # Exibir resultado\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(annotated_image)\n",
    "    plt.title('Detecção de Pose')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035febf6",
   "metadata": {},
   "source": [
    "## Detecção de Mãos\n",
    "\n",
    "Agora, usamos a solução `mp.solutions.hands`. Este modelo detecta 21 landmarks para cada mão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e1e4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar módulos específicos de Mãos\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34eac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar a imagem\n",
    "image_hand = cv2.imread('hand.jpg')\n",
    "image_hand = cv2.cvtColor(image_hand, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4485cfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o modelo e processar\n",
    "with mp_hands.Hands(\n",
    "    static_image_mode=True,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5) as hands:\n",
    "    \n",
    "    # Processar\n",
    "    results = hands.process(image_hand)\n",
    "\n",
    "    # Desenhar landmarks\n",
    "    annotated_image = image_hand.copy()\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            annotated_image,\n",
    "            hand_landmarks,\n",
    "            mp_hands.HAND_CONNECTIONS,\n",
    "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "            mp_drawing_styles.get_default_hand_connections_style()\n",
    "        )\n",
    "    \n",
    "    # Exibir resultado\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(annotated_image)\n",
    "    plt.title('Detecção de Mãos')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6d2024",
   "metadata": {},
   "source": [
    "## Detecção de Malha Facial (Face Mesh)\n",
    "\n",
    "Por fim, usamos `mp.solutions.face_mesh` para detectar 478 landmarks faciais (incluindo íris, se `refine_landmarks=True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95b415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar módulos específicos de Face Mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_face_mesh = mp.solutions.face_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c88d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar a imagem\n",
    "image_face = cv2.imread('face.jpg')\n",
    "image_face = cv2.cvtColor(image_face, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49684060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o modelo e processar\n",
    "with mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=True,\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5) as face_mesh:\n",
    "    \n",
    "    # Processar\n",
    "    results = face_mesh.process(image_face)\n",
    "\n",
    "    # Desenhar landmarks\n",
    "    annotated_image = image_face.copy()\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            h, w, _ = image_face.shape\n",
    "            x_points = [lm.x * w for lm in face_landmarks.landmark]\n",
    "            y_points = [lm.y * h for lm in face_landmarks.landmark]\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.imshow(image_face)\n",
    "            plt.scatter(x_points, y_points, s=5, c='lime')\n",
    "            plt.title('Landmarks Faciais - Apenas Pontos')\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "        \n",
    "        # 1. Tessellation (malha completa)\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image=annotated_image,\n",
    "            landmark_list=face_landmarks,\n",
    "            connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style()\n",
    "        )\n",
    "\n",
    "        # 2. Contornos (linhas do rosto)\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image=annotated_image,\n",
    "            landmark_list=face_landmarks,\n",
    "            connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style()\n",
    "        )\n",
    "\n",
    "        # 3. Íris\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image=annotated_image,\n",
    "            landmark_list=face_landmarks,\n",
    "            connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_iris_connections_style()\n",
    "        )\n",
    "\n",
    "    # Exibir resultado\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(annotated_image)\n",
    "    plt.title('Detecção de Malha Facial (Contornos e Íris)')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
