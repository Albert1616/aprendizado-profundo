{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67500cb9",
   "metadata": {},
   "source": [
    "# Prática: Classificação com Redes Neurais em PyTorch\n",
    "\n",
    "## Introdução\n",
    "\n",
    "Este notebook é um exercício prático para a construção, treinamento e avaliação de uma rede neural para um problema de classificação. As células de texto (Markdown) fornecerão o embasamento teórico, e sua tarefa será implementar a lógica correspondente nas células de código subsequentes.\n",
    "\n",
    "O objetivo é solidificar o entendimento sobre o pipeline de um projeto em PyTorch, desde a manipulação de dados até a avaliação do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e66facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e7920b",
   "metadata": {},
   "source": [
    "## 1. Geração e Visualização do Dataset\n",
    "\n",
    "Para que a aplicação de uma rede neural seja justificada, o problema de classificação não deve ser linearmente separável. Utilizaremos o `numpy` para gerar tal dataset.\n",
    "\n",
    "Para fins de visualização e para aumentar a dimensionalidade, uma terceira feature será artificialmente criada como uma combinação não-linear das duas features originais. Por fim, os dados serão normalizados, o que é uma prática recomendada que auxilia na estabilidade e velocidade do treinamento de redes neurais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0553580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geração e preparação do dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def generate_spiral_data(n_samples_per_class=1000, n_turns=3, noise=0.3):\n",
    "    \"\"\"\n",
    "    Gera um dataset de duas espirais 3D entrelaçadas.\n",
    "    \"\"\"\n",
    "    # Geração dos pontos da primeira espiral (Classe 0)\n",
    "    t = np.linspace(0, n_turns * 2 * np.pi, n_samples_per_class)\n",
    "    x1 = t * np.cos(t)\n",
    "    y1 = t * np.sin(t)\n",
    "    z1 = t\n",
    "    \n",
    "    # Adição de ruído gaussiano\n",
    "    X1 = np.vstack((x1, y1, z1)).T\n",
    "    X1 += noise * np.random.randn(*X1.shape)\n",
    "    y1 = np.zeros(n_samples_per_class)\n",
    "    \n",
    "    # Geração dos pontos da segunda espiral (Classe 1), defasada em 180 graus\n",
    "    x2 = t * np.cos(t + np.pi)\n",
    "    y2 = t * np.sin(t + np.pi)\n",
    "    z2 = t\n",
    "    \n",
    "    # Adição de ruído gaussiano\n",
    "    X2 = np.vstack((x2, y2, z2)).T\n",
    "    X2 += noise * np.random.randn(*X2.shape)\n",
    "    y2 = np.ones(n_samples_per_class)\n",
    "    \n",
    "    # Combinação e embaralhamento dos dados\n",
    "    X = np.vstack((X1, X2))\n",
    "    y = np.concatenate((y1, y2))\n",
    "    \n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Geração dos dados\n",
    "X, y = generate_spiral_data()\n",
    "\n",
    "# Normalização dos dados, uma prática padrão\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Shape de X (features): {X.shape}\")\n",
    "print(f\"Shape de y (labels): {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aea204",
   "metadata": {},
   "source": [
    "### Visualização 3D dos Dados\n",
    "\n",
    "A visualização do dataset em um espaço tridimensional nos permite obter uma intuição sobre a complexidade da fronteira de decisão que o modelo precisará aprender para separar as classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3274bc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(\n",
    "    x=X[:, 0],\n",
    "    y=X[:, 1],\n",
    "    z=X[:, 2],\n",
    "    color=y,\n",
    "    color_continuous_scale=px.colors.qualitative.Vivid,\n",
    "    title=\"Dataset Sintético 3D para Classificação\"\n",
    ")\n",
    "fig.update_traces(marker=dict(size=3))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b328864b",
   "metadata": {},
   "source": [
    "## 2. Preparação dos Dados para o PyTorch\n",
    "\n",
    "Nesta seção, você irá encapsular os dados NumPy em classes `Dataset` e `DataLoader` do PyTorch, que são abstrações fundamentais para o carregamento e a iteração sobre os dados de forma eficiente durante o treinamento.\n",
    "\n",
    "### 2.1. A Classe `Dataset`\n",
    "É necessário criar uma classe que herde de `torch.utils.data.Dataset`. Esta classe customizada deve implementar três métodos:\n",
    "- `__init__(self, features, labels)`: O construtor, onde os dados são recebidos. É aqui que os arrays NumPy devem ser convertidos para tensores do PyTorch. As features (X) devem ser do tipo `torch.float32` e os rótulos (y) do tipo `torch.long`.\n",
    "- `__len__(self)`: Método que retorna o número total de amostras no dataset.\n",
    "- `__getitem__(self, idx)`: Método que permite o acesso a uma amostra específica do dataset através de um índice `idx`. Ele deve retornar um par `(feature, label)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3fc56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# class ...(Dataset):\n",
    "#     # Implemente aqui os métodos __init__, __len__ e __getitem__\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3cad01",
   "metadata": {},
   "source": [
    "### 2.2. Divisão dos Dados e `DataLoader`\n",
    "Com a classe `Dataset` definida, o próximo passo é:\n",
    "1.  Dividir os arrays `X` e `y` em conjuntos de treinamento e teste utilizando `train_test_split`.\n",
    "2.  Instanciar a sua classe Dataset para cada um desses conjuntos.\n",
    "3.  Criar instâncias de `DataLoader` para os dois datasets. O `DataLoader` é um iterador que agrupa os dados em mini-lotes (`mini-batches`), com a opção de embaralhá-los a cada época, uma prática essencial para o conjunto de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b913207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, ...\n",
    "# train_dataset = ...\n",
    "# test_dataset = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf94f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# batch_size = ...\n",
    "# train_loader = ...\n",
    "# test_loader = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deb2194",
   "metadata": {},
   "source": [
    "## 3. Arquitetura da Rede Neural\n",
    "\n",
    "A arquitetura do modelo será composta por camadas ocultas e uma camada de saída.\n",
    "- As camadas ocultas são responsáveis por aprender representações complexas dos dados e podem ser construídas com `nn.Linear` e `nn.ReLU`.\n",
    "- A **camada de saída** deve ter **apenas 1 neurônio**.\n",
    "- Após a última camada linear, deve ser aplicada uma função de ativação `nn.Sigmoid()`.\n",
    "\n",
    "A saída do modelo será um único valor entre 0 e 1 para cada amostra de entrada, que pode ser interpretado como a probabilidade da amostra pertencer à classe 1.\n",
    "$$ \\hat{y} = \\sigma(W_{\\text{out}} \\cdot a_{\\text{hidden}} + b_{\\text{out}}) $$\n",
    "Onde $a_{\\text{hidden}}$ é a ativação da última camada oculta e $\\sigma$ é a função Sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a44bc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# class ...(nn.Module):\n",
    "    # Implemente os métodos __init__ e forward\n",
    "\n",
    "# model = ...\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04682af0",
   "metadata": {},
   "source": [
    "## 4. Função de Custo e Otimizador\n",
    "\n",
    "A **função de custo** para este problema será a `nn.BCELoss` (*Binary Cross-Entropy Loss*). Esta função mede o erro entre a probabilidade prevista pelo modelo e o rótulo verdadeiro (0 ou 1), sendo a escolha canônica para classificação binária.\n",
    "\n",
    "Para o **otimizador**, utilizaremos o `torch.optim.SGD`, que implementa o algoritmo de descida do gradiente estocástico. Sua principal configuração é a **taxa de aprendizado** (`learning_rate`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc9e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição da loss e do otimizador\n",
    "\n",
    "# learning_rate = ...\n",
    "# loss_fn = ...\n",
    "# optimizer = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae5aadc",
   "metadata": {},
   "source": [
    "## 5. Loop de Treinamento\n",
    "\n",
    "O loop de treinamento segue uma estrutura de 5 passos, iterando sobre os dados por um número definido de épocas. Para cada lote de dados, o ciclo é:\n",
    "\n",
    "1.  **Forward Pass**: Propagar os dados de entrada pelo modelo para obter as predições.\n",
    "2.  **Cálculo da Perda**: Calcular a perda comparando as predições com os rótulos verdadeiros.\n",
    "3.  **Zerar Gradientes**: Limpar os gradientes da iteração anterior (`optimizer.zero_grad()`).\n",
    "4.  **Backward Pass (Backpropagation)**: Calcular os gradientes da perda em relação a cada parâmetro (`loss.backward()`).\n",
    "5.  **Atualização dos Pesos**: Atualizar os pesos do modelo usando o otimizador (`optimizer.step()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dc2e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementação do loop de treinamento\n",
    "\n",
    "# num_epochs = ...\n",
    "# for epoch in range(num_epochs):\n",
    "#     ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c969e3",
   "metadata": {},
   "source": [
    "## 6. Visualização e Avaliação do Desempenho\n",
    "\n",
    "### Curvas de Aprendizagem\n",
    "Após o treinamento, é fundamental analisar as curvas de aprendizagem. A plotagem da perda e da acurácia ao longo das épocas nos permite diagnosticar se o modelo aprendeu corretamente e se há sinais de problemas como *overfitting* ou *underfitting*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb80179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot das curvas de loss e acurácia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a9d45f",
   "metadata": {},
   "source": [
    "### Avaliação Final no Conjunto de Teste\n",
    "A avaliação final deve ser feita no conjunto de teste, que o modelo não viu durante o treinamento. Isso fornece uma estimativa imparcial de sua capacidade de generalização.\n",
    "\n",
    "Para a fase de avaliação (inferência), é importante colocar o modelo em modo de avaliação com `model.eval()` e realizar os cálculos dentro de um bloco `with torch.no_grad()` para desativar o cálculo de gradientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32e597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação no dataset de teste\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#    ...\n",
    "#    # predicted = (outputs > 0.5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccef100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = px.scatter_3d(\n",
    "#     x=...,\n",
    "#     y=...,\n",
    "#     z=...,\n",
    "#     color=...,\n",
    "#     color_continuous_scale=px.colors.qualitative.Vivid,\n",
    "#     title=\"Predições no Conjunto de Testes\"\n",
    "# )\n",
    "# fig.update_traces(marker=dict(size=3))\n",
    "# fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
