{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "962fb60c",
   "metadata": {},
   "source": [
    "# Modelos Lineares: De Regressão a Classificação\n",
    "\n",
    "### Conteúdo Abordado\n",
    "\n",
    "1.  **Regressão Linear**\n",
    "    * Intuição e Formulação Matemática\n",
    "    * Geração de Dados Sintéticos\n",
    "    * Função de Custo: Erro Quadrático Médio (MSE)\n",
    "    * Solução Analítica: A Equação Normal\n",
    "    * Solução Iterativa: Gradiente Descendente Estocástico (SGD)\n",
    "    * Implementação e Treinamento com PyTorch\n",
    "    * Visualização da Curva de Aprendizado\n",
    "    * Inferência com o Modelo Treinado\n",
    "2.  **Regressão Logística**\n",
    "    * Da Regressão à Classificação\n",
    "    * Função de Ativação Sigmoid\n",
    "    * Geração de Dados Sintéticos para Classificação\n",
    "    * Função de Custo: Entropia Cruzada Binária (Log-Loss)\n",
    "    * Implementação e Treinamento com PyTorch\n",
    "    * Visualização das Curvas de Aprendizado (Custo e Acurácia)\n",
    "    * Inferência e Fronteira de Decisão"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e813c84",
   "metadata": {},
   "source": [
    "## 1. Introdução aos Modelos Lineares\n",
    "\n",
    "Modelos lineares são a base de muitos algoritmos de machine learning e formam o pilar fundamental para a compreensão de redes neurais mais complexas. A ideia central é que podemos modelar a relação entre um conjunto de variáveis de entrada (features) e uma variável de saída (alvo) através de uma combinação linear dessas entradas.\n",
    "\n",
    "A simplicidade desses modelos os torna altamente interpretáveis e computacionalmente eficientes. Eles servem como um excelente ponto de partida para problemas de regressão (prever um valor contínuo) e classificação (prever uma categoria discreta).\n",
    "\n",
    "Neste notebook, exploraremos dois dos modelos lineares mais importantes: a **Regressão Linear** e a **Regressão Logística**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9feb3e",
   "metadata": {},
   "source": [
    "## 2. Regressão Linear\n",
    "\n",
    "A Regressão Linear é utilizada para prever um valor de saída contínuo, $y$, a partir de um conjunto de variáveis de entrada, $x$. O modelo assume que a relação entre a entrada e a saída é linear.\n",
    "\n",
    "### Intuição e Formulação Matemática\n",
    "\n",
    "Para um único atributo (feature) $x$, a hipótese de um modelo de regressão linear é representada pela equação de uma reta:\n",
    "\n",
    "$$ \\hat{y} = w x + b $$\n",
    "\n",
    "Onde:\n",
    "- $\\hat{y}$ é o valor previsto.\n",
    "- $x$ é a variável de entrada.\n",
    "- $w$ é o **peso** (weight), que corresponde ao coeficiente angular da reta (inclinação).\n",
    "- $b$ é o **bias** (ou intercepto), que corresponde ao ponto onde a reta cruza o eixo y.\n",
    "\n",
    "O objetivo do aprendizado é encontrar os valores ótimos de $w$ e $b$ que melhor se ajustam aos dados de treinamento.\n",
    "\n",
    "Quando temos múltiplos atributos, $x_1, x_2, \\dots, x_n$, a equação se generaliza para:\n",
    "\n",
    "$$ \\hat{y} = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b $$\n",
    "\n",
    "Em notação vetorial, podemos simplificar a escrita:\n",
    "\n",
    "$$ \\hat{y} = \\mathbf{w}^T \\mathbf{x} + b $$\n",
    "\n",
    "Onde:\n",
    "- $\\mathbf{w}$ é o vetor de pesos.\n",
    "- $\\mathbf{x}$ é o vetor de atributos de entrada.\n",
    "- $\\mathbf{w}^T$ é a transposta do vetor de pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2311043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geração de Dados Sintéticos para Regressão\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configurações para reprodutibilidade\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Definindo os parâmetros verdadeiros do nosso modelo (que tentaremos aprender)\n",
    "true_weight = 2.5\n",
    "true_bias = 0.8\n",
    "\n",
    "num_points = 50\n",
    "# Gerando os dados de entrada (features)\n",
    "X = torch.randn(num_points, 1) * 10\n",
    "\n",
    "# Gerando os dados de saída (alvos) com base nos parâmetros e adicionando ruído\n",
    "# y = w*X + b + ruído\n",
    "noise = torch.randn(num_points, 1) * 16\n",
    "y = true_weight * X + true_bias + noise\n",
    "\n",
    "# Visualizando os dados gerados\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X.numpy(), y.numpy(), label=\"Dados Sintéticos\")\n",
    "plt.title(\"Dataset Sintético para Regressão Linear\")\n",
    "plt.xlabel(\"Feature (X)\")\n",
    "plt.ylabel(\"Target (y)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Imprimindo as dimensões dos tensores\n",
    "print(f\"Dimensão de X: {X.shape}\")\n",
    "print(f\"Dimensão de y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12a7208",
   "metadata": {},
   "source": [
    "### Função de Custo: Erro Quadrático Médio (MSE)\n",
    "\n",
    "Para encontrar os melhores valores de $w$ e $b$, precisamos de uma forma de medir o quão bem o nosso modelo está se saindo. A função de custo (ou função de perda) quantifica o erro entre os valores previstos ($\\hat{y}$) e os valores reais ($y$).\n",
    "\n",
    "Para a regressão linear, a função de custo mais comum é o **Erro Quadrático Médio** (Mean Squared Error - MSE). Ela calcula a média dos quadrados das diferenças entre as previsões e os valores reais.\n",
    "\n",
    "A fórmula para o MSE, para um conjunto de $m$ exemplos de treinamento, é:\n",
    "\n",
    "$$ J(w, b) = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2 $$\n",
    "\n",
    "Onde:\n",
    "- $\\hat{y}^{(i)} = w x^{(i)} + b$ é a previsão para o $i$-ésimo exemplo.\n",
    "- $y^{(i)}$ é o valor real para o $i$-ésimo exemplo.\n",
    "\n",
    "Nosso objetivo é minimizar essa função de custo $J(w, b)$. Um valor menor para o MSE indica um melhor ajuste do modelo aos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a086ef9",
   "metadata": {},
   "source": [
    "### Solução Analítica: A Equação Normal\n",
    "\n",
    "Para a regressão linear, existe uma solução de forma fechada, ou analítica, que nos permite encontrar os valores ótimos dos parâmetros ($w$ e $b$) de uma só vez, sem a necessidade de um processo iterativo. Esta solução é conhecida como **Equação Normal**.\n",
    "\n",
    "Para usá-la, primeiro representamos nossos dados de entrada $X$ como uma matriz onde cada linha é um exemplo de treinamento e adicionamos uma coluna de 1s para representar o termo de bias ($b$). Se $X$ tem $m$ exemplos e $n$ features, a nova matriz, que chamaremos de $X_b$, terá dimensões $(m, n+1)$. O vetor de parâmetros $\\theta$ incluirá tanto os pesos $w$ quanto o bias $b$.\n",
    "\n",
    "A solução que minimiza a função de custo MSE é dada por:\n",
    "\n",
    "$$ \\hat{\\theta} = (X_b^T X_b)^{-1} X_b^T y $$\n",
    "\n",
    "Onde:\n",
    "- $\\hat{\\theta}$ é o vetor de parâmetros ótimos.\n",
    "- $X_b$ é a matriz de features com a coluna de bias adicionada.\n",
    "- $y$ é o vetor de valores alvo.\n",
    "- $(X_b^T X_b)^{-1}$ é a inversa da matriz $X_b^T X_b$.\n",
    "\n",
    "Embora seja uma solução direta, o cálculo da inversa da matriz $(X_b^T X_b)$ pode ser computacionalmente muito caro, especialmente quando o número de features ($n$) é grande. Por essa razão, métodos iterativos como o Gradiente Descendente são mais comuns na prática, especialmente em deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b25487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solução com a Equação Normal\n",
    "\n",
    "# Preparando a matriz X_b adicionando uma coluna de 1s\n",
    "X_b = torch.cat([torch.ones(X.shape[0], 1), X], dim=1)\n",
    "\n",
    "# Calculando os parâmetros ótimos usando a Equação Normal\n",
    "# theta = (X_b.T @ X_b)^-1 @ X_b.T @ y\n",
    "try:\n",
    "    theta_best = torch.inverse(X_b.T @ X_b) @ X_b.T @ y\n",
    "    b_analytical = theta_best[0].item()\n",
    "    w_analytical = theta_best[1].item()\n",
    "\n",
    "    print(f\"Parâmetros encontrados pela Equação Normal:\")\n",
    "    print(f\"Peso (w): {w_analytical:.4f}\")\n",
    "    print(f\"Bias (b): {b_analytical:.4f}\")\n",
    "\n",
    "    # Plotando o resultado\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X.numpy(), y.numpy(), label=\"Dados Sintéticos\")\n",
    "    plt.plot(X.numpy(), (w_analytical * X + b_analytical).numpy(), color='r', label=\"Regressão Analítica\")\n",
    "    plt.title(\"Solução Analítica da Regressão Linear\")\n",
    "    plt.xlabel(\"Feature (X)\")\n",
    "    plt.ylabel(\"Target (y)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "except torch.linalg.LinAlgError:\n",
    "    print(\"A matriz X_b.T @ X_b é singular e não pode ser invertida.\")\n",
    "\n",
    "print(\"--- Parâmetros verdadeiros (para comparação) ---\")\n",
    "print(f\"Peso real: {true_weight}\")\n",
    "print(f\"Bias real: {true_bias}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c5676e",
   "metadata": {},
   "source": [
    "### Solução Iterativa: Gradiente Descendente Estocástico (SGD)\n",
    "\n",
    "Diferente da solução analítica, o Gradiente Descendente é um algoritmo de otimização iterativo que busca minimizar a função de custo ajustando gradualmente os parâmetros do modelo. A ideia é \"descer a colina\" da função de custo, dando passos na direção do gradiente negativo.\n",
    "\n",
    "O **gradiente** é um vetor que aponta na direção de maior crescimento da função. Portanto, para minimizar a função, ajustamos os parâmetros na direção oposta ao gradiente.\n",
    "\n",
    "A regra de atualização para cada parâmetro (seja $w$ ou $b$) é:\n",
    "\n",
    "$$ \\theta_{novo} = \\theta_{antigo} - \\eta \\nabla J(\\theta) $$\n",
    "\n",
    "Onde:\n",
    "- $\\theta$ representa um parâmetro do modelo ($w$ ou $b$).\n",
    "- $\\eta$ (eta) é a **taxa de aprendizado** (learning rate), um hiperparâmetro que controla o tamanho do passo que damos a cada iteração.\n",
    "- $\\nabla J(\\theta)$ é o gradiente da função de custo em relação ao parâmetro $\\theta$.\n",
    "\n",
    "Existem diferentes variantes do Gradiente Descendente:\n",
    "1.  **Batch Gradient Descent**: Calcula o gradiente usando todo o conjunto de dados de treinamento. É preciso, mas lento para datasets grandes.\n",
    "2.  **Stochastic Gradient Descent (SGD)**: Calcula o gradiente usando apenas um exemplo de treinamento por vez. É muito mais rápido, mas as atualizações são ruidosas.\n",
    "3.  **Mini-batch Gradient Descent**: Um meio-termo, calcula o gradiente usando um pequeno lote (mini-batch) de exemplos. É a abordagem mais comum na prática. No PyTorch, o otimizador `SGD` implementa esta variante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b984011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição do Modelo de Regressão Linear em PyTorch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Definindo a classe do nosso modelo\n",
    "# Herda de nn.Module, a classe base para todos os modelos de redes neurais no PyTorch\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        # Definimos uma camada linear.\n",
    "        # in_features=1: nosso modelo espera 1 feature de entrada.\n",
    "        # out_features=1: nosso modelo produz 1 valor de saída.\n",
    "        self.linear = nn.Linear(in_features=1, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # A passagem para frente (forward pass) define como a entrada é processada.\n",
    "        # Aqui, simplesmente passamos a entrada pela camada linear.\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7988086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando o modelo\n",
    "model = LinearRegressionModel()\n",
    "print(\"Estrutura do modelo:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e225048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Os parâmetros (peso e bias) são inicializados aleatoriamente\n",
    "# Podemos inspecioná-los\n",
    "print(\"Parâmetros iniciais (aleatórios):\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.data.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffa19c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparâmetros\n",
    "learning_rate = 0.001\n",
    "epochs = 200\n",
    "\n",
    "# Função de custo (Loss Function)\n",
    "# Usamos o MSELoss já implementado no PyTorch\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Otimizador (Optimizer)\n",
    "# Usamos o SGD, passando os parâmetros do modelo e a taxa de aprendizado\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5a05ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop de Treinamento com SGD\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Armazenar o histórico de perdas para visualização\n",
    "losses = []\n",
    "\n",
    "# Loop de treinamento\n",
    "for epoch in tqdm(range(epochs), desc=\"Treinando o modelo\"):\n",
    "    # 1. Forward pass: fazer uma previsão\n",
    "    y_pred = model(X)\n",
    "\n",
    "    # 2. Calcular a perda (loss)\n",
    "    loss = loss_function(y_pred, y)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    # 3. Zerar os gradientes da iteração anterior\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Backward pass: calcular os gradientes da perda em relação aos parâmetros\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Atualizar os pesos usando o otimizador\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"\\nTreinamento concluído!\")\n",
    "print(f\"Perda final: {losses[-1]:.4f}\")\n",
    "\n",
    "# Imprimindo os parâmetros aprendidos\n",
    "print(\"\\nParâmetros aprendidos pelo modelo:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.data.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f681fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot da Curva de Perda\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(epochs), losses)\n",
    "plt.title(\"Curva de Aprendizado (Perda vs. Épocas)\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"Erro Quadrático Médio (MSE)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8339c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inferência e Visualização do Modelo Treinado\n",
    "\n",
    "# Colocando o modelo em modo de avaliação\n",
    "model.eval()\n",
    "\n",
    "# Fazendo previsões com o modelo treinado\n",
    "with torch.no_grad(): # Desabilita o cálculo de gradientes para inferência\n",
    "    predicted = model(X).detach().numpy()\n",
    "\n",
    "# Visualizando o resultado final\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X.numpy(), y.numpy(), label=\"Dados Originais\")\n",
    "plt.plot(X.numpy(), predicted, color='r', label=\"Regressão com SGD\")\n",
    "plt.title(\"Resultado Final da Regressão Linear\")\n",
    "plt.xlabel(\"Feature (X)\")\n",
    "plt.ylabel(\"Target (y)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Exemplo de inferência para um novo ponto\n",
    "new_x = torch.tensor([[20.0]]) # Um novo valor de X\n",
    "with torch.no_grad():\n",
    "    new_y = model(new_x)\n",
    "print(f\"Previsão para X = {new_x.item():.1f} -> y = {new_y.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64def949",
   "metadata": {},
   "source": [
    "### Exercícios: Regressão Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7063e14c",
   "metadata": {},
   "source": [
    "#### Exercício 1: Regressão Linear Múltipla com Dados Sintéticos\n",
    "\n",
    "Até agora, nosso modelo usou apenas uma variável de entrada ($x$) para prever a saída ($y$). Na prática, a maioria dos problemas envolve múltiplas variáveis de entrada (features). Sua tarefa é adaptar o código para treinar um modelo de **Regressão Linear Múltipla**.\n",
    "\n",
    "A equação do modelo se expande para:\n",
    "\n",
    "$$\\hat{y} = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b = \\mathbf{w}^T \\mathbf{x} + b$$\n",
    "\n",
    "**Passos:**\n",
    "\n",
    "1.  **Gere um novo dataset sintético**: Crie uma matriz de features $X$ com, por exemplo, 200 amostras e 3 features (ou seja, um tensor de dimensão `(200, 3)`).\n",
    "2.  **Defina os Parâmetros Verdadeiros**: Crie os \"pesos verdadeiros\" que seu modelo tentará aprender. Por exemplo, um tensor `true_weights` de dimensão `(3, 1)` como `torch.tensor([[2.5], [-1.8], [0.5]])` e um `true_bias` escalar.\n",
    "3.  **Calcule o Alvo `y`**: Gere a variável alvo `y` usando a multiplicação de matrizes e adicionando ruído. A equação em PyTorch será: `y = X @ true_weights + true_bias + noise`.\n",
    "4.  **Ajuste a Classe do Modelo**: Modifique a classe `LinearRegressionModel`. O argumento `in_features` da camada `nn.Linear` deve ser alterado para corresponder ao número de features do seu novo dataset $X$.\n",
    "5.  **Treine e Verifique**: Treine o modelo como antes. Ao final, compare os pesos aprendidos (acessíveis via `model.parameters()`) com os `true_weights` que você definiu no passo 2. Os valores devem ser muito próximos, validando a correção da sua implementação."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d885d329",
   "metadata": {},
   "source": [
    "#### Exercício 2: O Impacto da Taxa de Aprendizado ($\\eta$)\n",
    "\n",
    "A taxa de aprendizado ($\\eta$) é um dos hiperparâmetros mais importantes no treinamento de redes neurais. Ela controla o tamanho dos passos dados durante a otimização para minimizar a função de custo, conforme a regra de atualização:\n",
    "\n",
    "$$ \\theta_{novo} = \\theta_{antigo} - \\eta \\nabla J(\\theta) $$\n",
    "\n",
    "**Sua tarefa é investigar o efeito de diferentes taxas de aprendizado:**\n",
    "1.  **Treine o modelo original** (de uma feature, com dados sintéticos) usando três valores diferentes para `learning_rate`:\n",
    "    * Um valor muito pequeno (ex: `1e-6`).\n",
    "    * O valor que usamos (ex: `0.001`).\n",
    "    * Um valor muito grande (ex: `0.1`).\n",
    "2.  **Plote a curva de perda (MSE vs. Épocas)** para cada um dos três experimentos no mesmo gráfico.\n",
    "3.  **Analise e descreva** o que você observa em cada caso:\n",
    "    * O que acontece quando $\\eta$ é muito pequeno? A convergência é rápida ou lenta?\n",
    "    * O que acontece quando $\\eta$ é muito grande? O modelo consegue convergir? O que o comportamento da curva de perda sugere?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec010ff",
   "metadata": {},
   "source": [
    "## 3. Regressão Logística\n",
    "\n",
    "Enquanto a regressão linear é usada para prever valores contínuos, a **Regressão Logística** é usada para problemas de **classificação**, onde o objetivo é prever uma categoria discreta (por exemplo, 0 ou 1, \"sim\" ou \"não\", \"gato\" ou \"cachorro\").\n",
    "\n",
    "Apesar do nome \"regressão\", é um modelo de classificação. Ele adapta o modelo linear para produzir uma probabilidade, que pode ser então usada para determinar a classe.\n",
    "\n",
    "### Função de Ativação Sigmoid\n",
    "\n",
    "Para transformar a saída de um modelo linear (que pode ser qualquer valor real) em uma probabilidade (um valor entre 0 e 1), usamos uma **função de ativação**. Para a Regressão Logística, a função utilizada é a **Sigmoid** (ou função logística).\n",
    "\n",
    "A função Sigmoid é definida como:\n",
    "\n",
    "$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "Onde $z$ é a saída do modelo linear ($z = \\mathbf{w}^T \\mathbf{x} + b$).\n",
    "\n",
    "- Se $z$ for muito grande e positivo, $e^{-z}$ se aproxima de 0, e $\\sigma(z)$ se aproxima de 1.\n",
    "- Se $z$ for muito grande e negativo, $e^{-z}$ se aproxima do infinito, e $\\sigma(z)$ se aproxima de 0.\n",
    "- Se $z = 0$, $\\sigma(z) = 0.5$.\n",
    "\n",
    "A saída $\\sigma(z)$ pode ser interpretada como a probabilidade de a amostra pertencer à classe positiva (classe 1).\n",
    "$$ \\hat{p} = P(y=1 | \\mathbf{x}) = \\sigma(\\mathbf{w}^T \\mathbf{x} + b) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4949dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização da Função Sigmoid\n",
    "\n",
    "# Gerando valores para z\n",
    "z = np.linspace(-10, 10, 100)\n",
    "\n",
    "# Calculando a sigmoid\n",
    "sigmoid = 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Plotando\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(z, sigmoid)\n",
    "plt.title(\"Função de Ativação Sigmoid\")\n",
    "plt.xlabel(\"z (saída linear)\")\n",
    "plt.ylabel(\"σ(z) (probabilidade)\")\n",
    "plt.grid(True)\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', label=\"Limite de 0.5\")\n",
    "plt.axvline(x=0.0, color='k', linestyle='-')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03be88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geração de Dados Sintéticos para Classificação\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Gerando um dataset linearmente separável\n",
    "X_clf, y_clf = make_classification(\n",
    "    n_samples=200,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_clusters_per_class=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Convertendo para tensores do PyTorch\n",
    "X_clf = torch.from_numpy(X_clf).float()\n",
    "y_clf = torch.from_numpy(y_clf).float().view(-1, 1) # Redimensiona para (200, 1)\n",
    "\n",
    "# Dividindo os dados em conjuntos de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_clf, y_clf, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Visualizando os dados\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train[y_train.squeeze() == 0][:, 0], X_train[y_train.squeeze() == 0][:, 1], label=\"Classe 0 (Treino)\")\n",
    "plt.scatter(X_train[y_train.squeeze() == 1][:, 0], X_train[y_train.squeeze() == 1][:, 1], label=\"Classe 1 (Treino)\")\n",
    "plt.title(\"Dataset Sintético para Classificação\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349d44bf",
   "metadata": {},
   "source": [
    "### Função de Custo: Entropia Cruzada Binária (Log-Loss)\n",
    "\n",
    "Para problemas de classificação, o MSE não é uma boa escolha, pois a função de custo pode se tornar não-convexa, dificultando a otimização. Em vez disso, usamos a **Entropia Cruzada Binária** (Binary Cross-Entropy), também conhecida como **Log-Loss**.\n",
    "\n",
    "Essa função penaliza fortemente as previsões que estão confiantes e erradas. A fórmula para um único exemplo é:\n",
    "\n",
    "$$ J(\\theta) = -[y \\log(\\hat{p}) + (1 - y) \\log(1 - \\hat{p})] $$\n",
    "\n",
    "Onde:\n",
    "- $y$ é o rótulo verdadeiro (0 ou 1).\n",
    "- $\\hat{p}$ é a probabilidade prevista para a classe 1 ($\\sigma(z)$).\n",
    "\n",
    "Analisando a fórmula:\n",
    "- Se $y=1$, o custo é $-\\log(\\hat{p})$. O custo é baixo se $\\hat{p}$ está próximo de 1, e muito alto se está próximo de 0.\n",
    "- Se $y=0$, o custo é $-\\log(1 - \\hat{p})$. O custo é baixo se $\\hat{p}$ está próximo de 0, e muito alto se está próximo de 1.\n",
    "\n",
    "Para $m$ exemplos, a função de custo é a média das perdas individuais:\n",
    "\n",
    "$$ J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(\\hat{p}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{p}^{(i)})] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ea1630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo e Treinamento da Regressão Logística\n",
    "\n",
    "# Definindo a classe do modelo\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        # 2 features de entrada, 1 saída (antes da sigmoid)\n",
    "        self.linear = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Passa pela camada linear e depois pela sigmoid\n",
    "        return torch.sigmoid(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202591bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando o modelo\n",
    "clf_model = LogisticRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e855d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparâmetros\n",
    "learning_rate_clf = 0.1\n",
    "epochs_clf = 300\n",
    "\n",
    "# Função de custo e otimizador\n",
    "# BCELoss: Binary Cross Entropy Loss\n",
    "loss_function_clf = nn.BCELoss()\n",
    "optimizer_clf = torch.optim.SGD(clf_model.parameters(), lr=learning_rate_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5acd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas para armazenar métricas\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# Loop de treinamento\n",
    "for epoch in tqdm(range(epochs_clf), desc=\"Treinando o classificador\"):\n",
    "    clf_model.train() # Modo de treinamento\n",
    "\n",
    "    # Treino\n",
    "    y_pred_train = clf_model(X_train)\n",
    "    loss_train = loss_function_clf(y_pred_train, y_train)\n",
    "    train_losses.append(loss_train.item())\n",
    "\n",
    "    optimizer_clf.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer_clf.step()\n",
    "\n",
    "    # Cálculo da acurácia de treino\n",
    "    predicted_labels_train = (y_pred_train >= 0.5).float()\n",
    "    acc_train = (predicted_labels_train == y_train).float().mean()\n",
    "    train_accuracies.append(acc_train.item())\n",
    "\n",
    "    # Validação (teste)\n",
    "    clf_model.eval() # Modo de avaliação\n",
    "    with torch.no_grad():\n",
    "        y_pred_test = clf_model(X_test)\n",
    "        loss_test = loss_function_clf(y_pred_test, y_test)\n",
    "        test_losses.append(loss_test.item())\n",
    "\n",
    "        # Cálculo da acurácia de teste\n",
    "        predicted_labels_test = (y_pred_test >= 0.5).float()\n",
    "        acc_test = (predicted_labels_test == y_test).float().mean()\n",
    "        test_accuracies.append(acc_test.item())\n",
    "\n",
    "print(\"\\nTreinamento concluído!\")\n",
    "print(f\"Acurácia final de teste: {test_accuracies[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4427013c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot das Curvas de Custo e Acurácia\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Plot da Perda (Loss)\n",
    "ax1.plot(train_losses, label=\"Perda de Treino\")\n",
    "ax1.plot(test_losses, label=\"Perda de Teste\")\n",
    "ax1.set_title(\"Curvas de Perda (Loss)\")\n",
    "ax1.set_xlabel(\"Época\")\n",
    "ax1.set_ylabel(\"Entropia Cruzada Binária\")\n",
    "ax1.grid(True)\n",
    "ax1.legend()\n",
    "\n",
    "# Plot da Acurácia\n",
    "ax2.plot(train_accuracies, label=\"Acurácia de Treino\")\n",
    "ax2.plot(test_accuracies, label=\"Acurácia de Teste\")\n",
    "ax2.set_title(\"Curvas de Acurácia\")\n",
    "ax2.set_xlabel(\"Época\")\n",
    "ax2.set_ylabel(\"Acurácia\")\n",
    "ax2.grid(True)\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea02395",
   "metadata": {},
   "source": [
    "### Fronteira de Decisão\n",
    "\n",
    "A fronteira de decisão é a linha (ou superfície, em dimensões maiores) que separa as áreas onde o modelo prevê uma classe daquelas onde prevê a outra. Para a Regressão Logística, a previsão é 1 se $\\sigma(z) \\ge 0.5$ e 0 caso contrário. Isso acontece quando o argumento da sigmoid, $z$, é igual a 0.\n",
    "\n",
    "Portanto, a fronteira de decisão é a linha definida pela equação:\n",
    "\n",
    "$$ \\mathbf{w}^T \\mathbf{x} + b = 0 $$\n",
    "\n",
    "Visualizar essa fronteira nos ajuda a entender como o modelo está separando os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f752c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot da Fronteira de Decisão\n",
    "\n",
    "# Colocar o modelo em modo de avaliação\n",
    "clf_model.eval()\n",
    "\n",
    "# Criar um grid de pontos para plotar a fronteira\n",
    "x_min, x_max = X_clf[:, 0].min() - 1, X_clf[:, 0].max() + 1\n",
    "y_min, y_max = X_clf[:, 1].min() - 1, X_clf[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "\n",
    "# Fazer previsões para cada ponto no grid\n",
    "grid_tensor = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()]).float()\n",
    "with torch.no_grad():\n",
    "    Z = clf_model(grid_tensor)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plotar a fronteira e os dados de teste\n",
    "plt.figure(figsize=(10, 6))\n",
    "# A fronteira de decisão (contorno onde a probabilidade é 0.5)\n",
    "plt.contourf(xx, yy, Z.detach().numpy() > 0.5, cmap=plt.cm.RdYlBu, alpha=0.5)\n",
    "\n",
    "# Plotar os dados de teste\n",
    "plt.scatter(X_test[y_test.squeeze() == 0][:, 0], X_test[y_test.squeeze() == 0][:, 1], label=\"Classe 0 (Teste)\", c='blue', edgecolors='k')\n",
    "plt.scatter(X_test[y_test.squeeze() == 1][:, 0], X_test[y_test.squeeze() == 1][:, 1], label=\"Classe 1 (Teste)\", c='red', edgecolors='k')\n",
    "\n",
    "# Exemplo de inferência em um novo ponto\n",
    "new_point = torch.tensor([[-2.0, 1.0]])\n",
    "with torch.no_grad():\n",
    "    prediction_prob = clf_model(new_point)\n",
    "    prediction_class = (prediction_prob >= 0.5).item()\n",
    "\n",
    "# Plotar o novo ponto\n",
    "plt.scatter(new_point[:, 0], new_point[:, 1], c='lime', marker='*', s=200, edgecolors='k', label=f'Novo Ponto (Previsto: Classe {int(prediction_class)})')\n",
    "print(f\"Probabilidade para o novo ponto: {prediction_prob.item():.4f}\")\n",
    "print(f\"Classe prevista para o novo ponto: {int(prediction_class)}\")\n",
    "\n",
    "\n",
    "plt.title(\"Fronteira de Decisão da Regressão Logística\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f550fba",
   "metadata": {},
   "source": [
    "### Exercícios: Regressão Logística"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eb5f86",
   "metadata": {},
   "source": [
    "#### Exercício 1: Portas Lógicas AND e OR\n",
    "\n",
    "As portas lógicas são os blocos de construção fundamentais da computação digital. Sua tarefa é usar a Regressão Logística para modelar as portas **AND** e **OR**.\n",
    "\n",
    "**Tabelas Verdade:**\n",
    "\n",
    "| $x_1$ | $x_2$ | AND | OR |\n",
    "|:---:|:---:|:---:|:--:|\n",
    "| 0   | 0   | 0   | 0  |\n",
    "| 0   | 1   | 0   | 1  |\n",
    "| 1   | 0   | 0   | 1  |\n",
    "| 1   | 1   | 1   | 1  |\n",
    "\n",
    "**Passos:**\n",
    "1.  **Crie dois datasets**: um para a porta AND e outro para a porta OR. Cada dataset deve conter múltiplos exemplos para cada uma das quatro combinações de entrada (ex: 20 pontos para `(0,0)`, 20 para `(0,1)`, etc.).\n",
    "2.  **Adicione um pouco de ruído gaussiano** aos dados de entrada $X$ para que os pontos não sejam exatamente 0 ou 1. Isso torna o problema de classificação mais realista.\n",
    "3.  **Treine um modelo de Regressão Logística** para cada porta lógica.\n",
    "4.  **Plote a fronteira de decisão** para cada modelo treinado. Você deve observar que o modelo consegue encontrar uma linha reta que separa perfeitamente as classes 0 e 1 para ambas as portas, pois elas são problemas **linearmente separáveis**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b12289",
   "metadata": {},
   "source": [
    "#### Exercício 2: O Desafio da Porta Lógica XOR\n",
    "\n",
    "Agora, vamos tentar o mesmo com a porta **XOR** (OU exclusivo).\n",
    "\n",
    "**Tabela Verdade XOR:**\n",
    "\n",
    "| $x_1$ | $x_2$ | XOR |\n",
    "|:---:|:---:|:---:|\n",
    "| 0   | 0   | 0   |\n",
    "| 0   | 1   | 1   |\n",
    "| 1   | 0   | 1   |\n",
    "| 1   | 1   | 0   |\n",
    "\n",
    "**Passos:**\n",
    "1.  **Crie um dataset para a porta XOR**, seguindo a mesma abordagem do exercício anterior (múltiplos pontos com ruído para cada combinação de entrada).\n",
    "2.  **Treine um modelo de Regressão Logística** para classificar os dados XOR.\n",
    "3.  **Plote a fronteira de decisão** e a curva de acurácia.\n",
    "4.  **Analise e responda**:\n",
    "    * O que você observa no gráfico da fronteira de decisão?\n",
    "    * Qual a acurácia máxima que o seu modelo consegue atingir? Por que você acha que ela fica estagnada em torno desse valor?\n",
    "    * É possível traçar uma **única linha reta** que separe as saídas da classe 0 (pontos `(0,0)` e `(1,1)`) das saídas da classe 1 (pontos `(0,1)` e `(1,0)`)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
