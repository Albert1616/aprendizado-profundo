{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b23187b6",
   "metadata": {},
   "source": [
    "# 09 - Otimizadores\n",
    "\n",
    "Neste notebook, vamos explorar diferentes algoritmos de otimização usados em aprendizado de máquina e deep learning. Começaremos com visualizações de como os otimizadores navegam por superfícies de funções e depois compararemos seu desempenho em um subconjunto do MNIST.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9236a4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5ebd8f",
   "metadata": {},
   "source": [
    "## 1. Fundamentos\n",
    "\n",
    "Os algoritmos de otimização são essenciais para treinar redes neurais. Eles buscam minimizar uma função de custo $L(\\theta)$ ajustando os parâmetros $\\theta$ do modelo.\n",
    "\n",
    "### Gradiente Descendente Básico\n",
    "\n",
    "O algoritmo mais simples atualiza os parâmetros na direção oposta ao gradiente:\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\eta \\nabla L(\\theta_t)$$\n",
    "\n",
    "onde $\\eta$ é a taxa de aprendizado.\n",
    "\n",
    "### Momentum\n",
    "\n",
    "O momentum adiciona uma \"memória\" das atualizações anteriores:\n",
    "\n",
    "$$v_{t+1} = \\beta v_t + \\nabla L(\\theta_t)$$\n",
    "$$\\theta_{t+1} = \\theta_t - \\eta v_{t+1}$$\n",
    "\n",
    "onde $\\beta$ é o coeficiente de momentum (tipicamente 0.9)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51947fb",
   "metadata": {},
   "source": [
    "### Visualizando steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t1ec6gkhcbh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "    return (x**2 + y - 11)**2 + (x + y**2 - 7)**2\n",
    "\n",
    "def f_gradient(x, y):\n",
    "    dx = 4*x*(x**2 + y - 11) + 2*(x + y**2 - 7)\n",
    "    dy = 2*(x**2 + y - 11) + 4*y*(x + y**2 - 7)\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "# Função para simular otimização com PyTorch optimizers\n",
    "def run_pytorch_optimizer(optimizer_class, optimizer_kwargs, start_point, n_steps=50):\n",
    "    # Criando parâmetro como tensor\n",
    "    position = torch.tensor(start_point, dtype=torch.float32, requires_grad=True)\n",
    "    optimizer = optimizer_class([position], **optimizer_kwargs)\n",
    "    \n",
    "    steps = [position.detach().numpy().copy()]\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calculando loss\n",
    "        x, y = position[0], position[1]\n",
    "        loss = (x**2 + y - 11)**2 + (x + y**2 - 7)**2\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        steps.append(position.detach().numpy().copy())\n",
    "        \n",
    "        # Stop se convergiu\n",
    "        if torch.norm(position.grad) < 1e-3:\n",
    "            break\n",
    "    \n",
    "    return steps\n",
    "\n",
    "# Função para plotar contornos e passos do otimizador\n",
    "def plot_contours_and_steps(X, Y, Z, steps_list, labels, title):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot contornos\n",
    "    levels = np.logspace(0, 3, 20)\n",
    "    cp = plt.contour(X, Y, Z, levels=levels, alpha=0.6)\n",
    "    plt.contourf(X, Y, Z, levels=levels, alpha=0.3, cmap='viridis')\n",
    "    \n",
    "    # Plot passos dos otimizadores\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "    for i, (steps, label) in enumerate(zip(steps_list, labels)):\n",
    "        steps = np.array(steps)\n",
    "        plt.plot(steps[:, 0], steps[:, 1], 'o-', \n",
    "                color=colors[i % len(colors)], \n",
    "                label=label, linewidth=2, markersize=6)\n",
    "        plt.plot(steps[0, 0], steps[0, 1], 's', \n",
    "                color=colors[i % len(colors)], markersize=10, alpha=0.7)\n",
    "    \n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel('x', fontsize=12)\n",
    "    plt.ylabel('y', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hg6hcejilfj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a superfície da função\n",
    "x = np.linspace(-3.25, -2.4, 50)\n",
    "y = np.linspace(2.5, 3.5, 50)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = f(X, Y)\n",
    "\n",
    "# Ponto inicial\n",
    "start_point = [-3.0, 3.0]\n",
    "\n",
    "# Testando diferentes valores de momentum usando torch.optim.SGD\n",
    "momentum_values = [0.0, 0.3, 0.6, 0.9]\n",
    "steps_list = []\n",
    "labels = []\n",
    "\n",
    "for momentum in momentum_values:\n",
    "    steps = run_pytorch_optimizer(\n",
    "        optim.SGD, \n",
    "        {'lr': 0.01, 'momentum': momentum}, \n",
    "        start_point, \n",
    "        n_steps=100\n",
    "    )\n",
    "    steps_list.append(steps)\n",
    "    labels.append(f'Momentum = {momentum}')\n",
    "\n",
    "plot_contours_and_steps(X, Y, Z, steps_list, labels, 'Comparação de SGD com Diferentes Valores de Momentum (PyTorch)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98576dcf",
   "metadata": {},
   "source": [
    "## Otimizadores com MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35r1qds3rds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando e preparando o dataset MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Carregando datasets completos\n",
    "full_train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform)\n",
    "full_test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Criando subconjuntos menores para comparação rápida\n",
    "train_indices = torch.randperm(len(full_train_dataset))[:5000]\n",
    "val_indices = torch.randperm(len(full_test_dataset))[:1000]\n",
    "\n",
    "train_subset = torch.utils.data.Subset(full_train_dataset, train_indices)\n",
    "val_subset = torch.utils.data.Subset(full_test_dataset, val_indices)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_subset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Tamanho do conjunto de treino: {len(train_subset)}\\\")\\nprint(f\\\"Tamanho do conjunto de validação: {len(val_subset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y60xfmh3otq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo simples para comparação\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786a1b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de treino\n",
    "def train_model(model, train_loader, val_loader, optimizer, epochs=10):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Treino\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        # Validação\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                output = model(data)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        val_accuracies.append(accuracy)\n",
    "        \n",
    "        if epoch % 2 == 0:\n",
    "            print(f'Epoch {epoch}: Loss = {avg_loss:.4f}, Val Acc = {accuracy:.2f}%')\n",
    "\n",
    "    return train_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8lbarlz3pv",
   "metadata": {},
   "source": [
    "### Otimizadores Avançados\n",
    "\n",
    "Antes de treinar, vamos entender os algoritmos que compararemos:\n",
    "\n",
    "**AdaGrad**: Adapta a taxa de aprendizado baseada no histórico de gradientes\n",
    "$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\nabla L(\\theta_t)$$\n",
    "\n",
    "**RMSprop**: Usa média móvel exponencial dos gradientes ao quadrado\n",
    "$$v_t = \\beta v_{t-1} + (1-\\beta) (\\nabla L(\\theta_t))^2$$\n",
    "$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{v_t + \\epsilon}} \\nabla L(\\theta_t)$$\n",
    "\n",
    "**Adam**: Combina momentum com adaptação de taxa de aprendizado\n",
    "$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) \\nabla L(\\theta_t)$$\n",
    "$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) (\\nabla L(\\theta_t))^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wbymllnw3re",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração dos otimizadores\n",
    "optimizers_config = {\n",
    "    'SGD': {'lr': 0.01},\n",
    "    'SGD+Momentum': {'lr': 0.01, 'momentum': 0.9},\n",
    "    'Adagrad': {'lr': 0.01},\n",
    "    'RMSprop': {'lr': 0.001},\n",
    "    'Adam': {'lr': 0.001}\n",
    "}\n",
    "\n",
    "# Dicionário para armazenar resultados\n",
    "results = {}\n",
    "\n",
    "print(\"Comparando otimizadores no MNIST...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Treinando com cada otimizador\n",
    "for opt_name, config in optimizers_config.items():\n",
    "    print(f\"\\nTreinando com {opt_name}...\")\n",
    "\n",
    "    # Criando novo modelo para cada otimizador\n",
    "    model = SimpleNet()\n",
    "\n",
    "    # Mapeamento de nomes para classes\n",
    "    opt_map = {\n",
    "        'SGD': optim.SGD,\n",
    "        'SGD+Momentum': optim.SGD,\n",
    "        'Adagrad': optim.Adagrad,\n",
    "        'RMSprop': optim.RMSprop,\n",
    "        'Adam': optim.Adam\n",
    "    }\n",
    "    \n",
    "    # Instanciando otimizador\n",
    "    optimizer = opt_map[opt_name](model.parameters(), **config)\n",
    "\n",
    "    # Treinando\n",
    "    train_losses, val_accuracies = train_model(\n",
    "        model, train_loader, val_loader, optimizer, epochs=15\n",
    "    )\n",
    "\n",
    "    # Salvando resultados\n",
    "    results[opt_name] = {\n",
    "        'train_losses': train_losses,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'final_accuracy': val_accuracies[-1]\n",
    "    }\n",
    "\n",
    "    print(f\"Acurácia final: {val_accuracies[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sghm64wjv4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando os resultados\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Curvas de loss de treino\n",
    "for opt_name, result in results.items():\n",
    "    ax1.plot(result['train_losses'], label=opt_name, linewidth=2)\n",
    "\n",
    "ax1.set_xlabel('Época')\n",
    "ax1.set_ylabel('Loss de Treino')\n",
    "ax1.set_title('Comparação das Curvas de Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "# ax1.set_ylim(0, max_loss)  # opcional: fixar limite superior\n",
    "\n",
    "# Plot 2: Curvas de acurácia de validação\n",
    "for opt_name, result in results.items():\n",
    "    ax2.plot(result['val_accuracies'], label=opt_name, linewidth=2)\n",
    "\n",
    "ax2.set_xlabel('Época')\n",
    "ax2.set_ylabel('Acurácia de Validação (%)')\n",
    "ax2.set_title('Comparação das Curvas de Acurácia')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "# ax2.set_ylim(0, 100)  # opcional: manter escala de 0 a 100%\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Resumo dos resultados finais\n",
    "print(\"\\nResumo dos Resultados Finais:\")\n",
    "print(\"=\"*40)\n",
    "for opt_name, result in results.items():\n",
    "    print(f\"{opt_name:<15} -> Acurácia Final: {result['final_accuracy']:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
