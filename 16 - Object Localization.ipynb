{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29053d69",
   "metadata": {},
   "source": [
    "# Introdução à Localização de Objetos\n",
    "\n",
    "A Localização de Objetos (Object Localization) é uma tarefa fundamental em Visão Computacional que estende a tarefa de classificação de imagens. Enquanto a classificação se concentra em determinar a presença de um objeto em uma imagem (o \"o quê\"), a localização visa não apenas identificar o objeto, mas também determinar sua posição espacial, geralmente por meio de uma caixa delimitadora (bounding box) que o envolve (o \"onde\"). Este processo é um passo intermediário para tarefas mais complexas, como detecção de múltiplos objetos e segmentação de instâncias. Neste notebook, exploraremos os conceitos fundamentais da localização de objetos, desde a criação de um dataset sintético até a implementação e o treinamento de uma rede neural convolucional (CNN) com múltiplas saídas para prever simultaneamente a classe do objeto e as coordenadas de sua caixa delimitadora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a02a682",
   "metadata": {},
   "source": [
    "## Geração do Dataset Sintético\n",
    "\n",
    "Para treinar nosso modelo, precisamos de um dataset que contenha imagens e suas respectivas anotações. As anotações devem incluir o rótulo da classe do objeto e as coordenadas da sua *bounding box*. Iremos gerar imagens contendo uma de três formas geométricas (retângulo, círculo ou triângulo) em posições, tamanhos e cores aleatórias.\n",
    "\n",
    "Os arquivos serão salvos em duas pastas: `images/` para os arquivos PNG e `annotations/` para os arquivos JSON correspondentes, que contêm as anotações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7738e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from torch import nn, optim\n",
    "import torchvision.models as models\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07858b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ffd88f",
   "metadata": {},
   "source": [
    "### Formatos da Bounding Box\n",
    "\n",
    "É importante compreender os diferentes formatos de representação de *bounding boxes*. Durante a geração das imagens, utilizaremos o formato `[x0, y0, x1, y1]`, que representa as coordenadas do canto superior esquerdo (top-left) e do canto inferior direito (bottom-right). Este formato é intuitivo para bibliotecas de desenho como a PIL.\n",
    "\n",
    "No entanto, para o treinamento do modelo, é comum e muitas vezes mais eficaz normalizar as coordenadas e utilizar o formato `[cx, cy, w, h]`, onde:\n",
    "- `cx`: coordenada x do centro da caixa\n",
    "- `cy`: coordenada y do centro da caixa\n",
    "- `w`: largura (width) da caixa\n",
    "- `h`: altura (height) da caixa\n",
    "\n",
    "As conversões entre os formatos são dadas por:\n",
    "\n",
    "$$\n",
    "cx = \\frac{x_0 + x_1}{2} \\quad , \\quad cy = \\frac{y_0 + y_1}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "w = x_1 - x_0 \\quad , \\quad h = y_1 - y_0\n",
    "$$\n",
    "\n",
    "Todas as coordenadas `[cx, cy, w, h]` serão normalizadas pela dimensão da imagem para estarem no intervalo [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd419b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_yolo_format(box, img_size):\n",
    "    \"\"\"Converte a bounding box de [x0, y0, x1, y1] para [cx, cy, w, h] normalizado.\"\"\"\n",
    "    x0, y0, x1, y1 = box\n",
    "    img_w, img_h = img_size\n",
    "    \n",
    "    dw = 1. / img_w\n",
    "    dh = 1. / img_h\n",
    "    \n",
    "    cx = (x0 + x1) / 2.0\n",
    "    cy = (y0 + y1) / 2.0\n",
    "    w = x1 - x0\n",
    "    h = y1 - y0\n",
    "    \n",
    "    cx_norm = cx * dw\n",
    "    cy_norm = cy * dh\n",
    "    w_norm = w * dw\n",
    "    h_norm = h * dh\n",
    "    \n",
    "    return [cx_norm, cy_norm, w_norm, h_norm]\n",
    "\n",
    "def convert_from_yolo_format(box, img_size):\n",
    "    \"\"\"Converte a bounding box de [cx, cy, w, h] normalizado para [x0, y0, w, h] em pixels.\"\"\"\n",
    "    cx_norm, cy_norm, w_norm, h_norm = box\n",
    "    img_w, img_h = img_size\n",
    "    \n",
    "    w = w_norm * img_w\n",
    "    h = h_norm * img_h\n",
    "    x0 = (cx_norm * img_w) - (w / 2)\n",
    "    y0 = (cy_norm * img_h) - (h / 2)\n",
    "    \n",
    "    return [x0, y0, w, h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e18b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = 'data/object_location'\n",
    "IMG_SIZE = (224, 224)\n",
    "NUM_IMAGES = 5000\n",
    "SHAPES = ['circle', 'square', 'triangle']\n",
    "SIZES = {'small': 40, 'large': 100}  # tamanhos fixos\n",
    "CLASSES = {shape: i for i, shape in enumerate(SHAPES)}\n",
    "\n",
    "images_path = os.path.join(DATASET_DIR, 'images')\n",
    "annotations_path = os.path.join(DATASET_DIR, 'annotations')\n",
    "\n",
    "os.makedirs(images_path, exist_ok=True)\n",
    "os.makedirs(annotations_path, exist_ok=True)\n",
    "\n",
    "for i in range(NUM_IMAGES):\n",
    "    image = Image.new('RGB', IMG_SIZE, 'black')\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    shape_type = random.choice(SHAPES)\n",
    "    size_label = random.choice(list(SIZES.keys()))\n",
    "    side = SIZES[size_label]\n",
    "\n",
    "    color = (\n",
    "        random.randint(50, 255),\n",
    "        random.randint(50, 255),\n",
    "        random.randint(50, 255)\n",
    "    )\n",
    "\n",
    "    pos_x0 = random.randint(0, IMG_SIZE[0] - side)\n",
    "    pos_y0 = random.randint(0, IMG_SIZE[1] - side)\n",
    "    pos_x1 = pos_x0 + side\n",
    "    pos_y1 = pos_y0 + side\n",
    "    box = [pos_x0, pos_y0, pos_x1, pos_y1]\n",
    "\n",
    "    if shape_type == 'square':\n",
    "        draw.rectangle(box, fill=color)\n",
    "\n",
    "    elif shape_type == 'circle':\n",
    "        draw.ellipse(box, fill=color)\n",
    "\n",
    "    elif shape_type == 'triangle':\n",
    "        cx = (pos_x0 + pos_x1) / 2\n",
    "        cy = (pos_y0 + pos_y1) / 2\n",
    "        h = (math.sqrt(3) / 2) * side\n",
    "        p1 = (cx, cy - h / 2)\n",
    "        p2 = (cx - side / 2, cy + h / 2)\n",
    "        p3 = (cx + side / 2, cy + h / 2)\n",
    "        draw.polygon([p1, p2, p3], fill=color)\n",
    "\n",
    "    img_filename = f'{i:04d}.png'\n",
    "    image.save(os.path.join(images_path, img_filename))\n",
    "\n",
    "    yolo_box = convert_to_yolo_format(box, IMG_SIZE)\n",
    "    annotation = {\n",
    "        'image': img_filename,\n",
    "        'label': CLASSES[shape_type],\n",
    "        'size': size_label,\n",
    "        'bbox': yolo_box\n",
    "    }\n",
    "\n",
    "    ann_filename = f'{i:04d}.json'\n",
    "    with open(os.path.join(annotations_path, ann_filename), 'w') as f:\n",
    "        json.dump(annotation, f)\n",
    "\n",
    "print(f'{NUM_IMAGES} imagens e anotações geradas com sucesso em {DATASET_DIR}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba7b87c",
   "metadata": {},
   "source": [
    "## Dataset e DataLoader em PyTorch\n",
    "\n",
    "Para carregar os dados de forma eficiente no PyTorch, criaremos uma classe `Dataset` customizada. A classe `torch.utils.data.Dataset` é uma classe abstrata que representa um dataset. Uma classe customizada deve implementar três métodos fundamentais:\n",
    "- `__init__(self, ...)`: Executado uma vez na instanciação do objeto, ideal para inicializar o dataset, como carregar os nomes dos arquivos.\n",
    "- `__len__(self)`: Deve retornar o tamanho do dataset.\n",
    "- `__getitem__(self, idx)`: Suporta indexação para obter a i-ésima amostra do dataset. É aqui que carregamos a imagem e sua anotação, aplicamos transformações e retornamos os tensores prontos para o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10574a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(Dataset):\n",
    "    \"\"\"Dataset customizado para as formas geométricas.\"\"\"\n",
    "    def __init__(self, img_dir, ann_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.ann_dir = ann_dir\n",
    "        self.transform = transform\n",
    "        # Lista todos os arquivos de imagem, assumindo que eles correspondem às anotações\n",
    "        self.img_files = sorted([f for f in os.listdir(img_dir) if f.endswith('.png')])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Carrega a imagem\n",
    "        img_name = self.img_files[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Carrega a anotação\n",
    "        ann_name = img_name.replace('.png', '.json')\n",
    "        ann_path = os.path.join(self.ann_dir, ann_name)\n",
    "        with open(ann_path, 'r') as f:\n",
    "            annotation = json.load(f)\n",
    "            \n",
    "        label = torch.tensor(annotation['label'], dtype=torch.long)\n",
    "        bbox = torch.tensor(annotation['bbox'], dtype=torch.float32)\n",
    "\n",
    "        # Aplica transformações na imagem, se houver\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, (label, bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804660e0",
   "metadata": {},
   "source": [
    "### Transformações e Preparação dos DataLoaders\n",
    "\n",
    "Utilizamos `torchvision.transforms` para aplicar transformações comuns em imagens. A transformação `ToTensor()` converte a imagem PIL (intervalo [0, 255]) para um tensor PyTorch (intervalo [0.0, 1.0]) e ajusta a ordem das dimensões (H, W, C) para (C, H, W). A `Normalize()` ajusta os valores dos pixels para terem média e desvio padrão específicos, o que é uma prática padrão ao usar modelos pré-treinados.\n",
    "\n",
    "Após criar a instância do `Dataset`, dividimos os dados em conjuntos de treino e validação usando `torch.utils.data.random_split`. Finalmente, os `DataLoaders` são criados para gerenciar o carregamento de dados em lotes (*batches*), embaralhando os dados de treino a cada época para reduzir o sobreajuste (*overfitting*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45147a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sequência de transformações\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define os caminhos para as pastas\n",
    "images_path = os.path.join(DATASET_DIR, 'images')\n",
    "annotations_path = os.path.join(DATASET_DIR, 'annotations')\n",
    "\n",
    "# Instancia o dataset completo\n",
    "full_dataset = ShapesDataset(img_dir=images_path, ann_dir=annotations_path, transform=data_transforms)\n",
    "\n",
    "# Divide em treino e validação (80% treino, 20% validação)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Cria os DataLoaders\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Tamanho do dataset de treino: {len(train_dataset)}\")\n",
    "print(f\"Tamanho do dataset de validação: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e736fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    img, (label, bbox) = full_dataset[i]\n",
    "\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    cx, cy, w, h = bbox\n",
    "    x0 = (cx - w/2) * IMG_SIZE[0]\n",
    "    y0 = (cy - h/2) * IMG_SIZE[1]\n",
    "    rect = patches.Rectangle((x0, y0), w*IMG_SIZE[0], h*IMG_SIZE[1], linewidth=2, edgecolor='r', facecolor='none')\n",
    "    plt.imshow(img)\n",
    "    plt.gca().add_patch(rect)\n",
    "    plt.title(SHAPES[label])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762405c1",
   "metadata": {},
   "source": [
    "## Arquitetura do Modelo\n",
    "\n",
    "Para a tarefa de localização, o modelo precisa de duas saídas (cabeças):\n",
    "1.  **Cabeça de Classificação**: Uma camada linear que recebe os vetores de características (*features*) extraídos pela rede convolucional e produz *logits* para cada classe de objeto.\n",
    "2.  **Cabeça de Regressão**: Outra camada linear que recebe as mesmas *features* e regride os 4 valores da *bounding box* (`cx`, `cy`, `w`, `h`).\n",
    "\n",
    "Utilizaremos uma arquitetura de *transfer learning*. Um modelo `MobileNetV2` pré-treinado na ImageNet será usado como *backbone* para extração de características. Congelaremos os pesos do *backbone* e treinaremos apenas as duas novas cabeças que adicionaremos ao final da rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a0d755",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalizationModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # Carrega o backbone pré-treinado\n",
    "        self.backbone = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT)\n",
    "        \n",
    "        # Remove a camada de classificação original do MobileNetV2\n",
    "        num_features = self.backbone.classifier[1].in_features\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "\n",
    "        # Congela os pesos do backbone\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Descongela as últimas 3 camadas do backbone para fine-tuning\n",
    "        for param in list(self.backbone.features[-2:].parameters()):\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # Cabeça para classificação\n",
    "        self.classifier_head = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(num_features, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Cabeça para regressão da bounding box\n",
    "        self.regressor_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 4),\n",
    "            nn.Sigmoid() # Garante que as saídas estejam entre 0 e 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        \n",
    "        # Saídas das duas cabeças\n",
    "        class_logits = self.classifier_head(features)\n",
    "        bbox_pred = self.regressor_head(features)\n",
    "        \n",
    "        return class_logits, bbox_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2bb19a",
   "metadata": {},
   "source": [
    "## Função de Custo e Treinamento\n",
    "\n",
    "A função de custo (*loss function*) para este problema de aprendizado multi-tarefa é uma combinação de duas perdas:\n",
    "- **Perda de Classificação**: `CrossEntropyLoss`, adequada para problemas de classificação multi-classe.\n",
    "- **Perda de Regressão**: `L1Loss` (ou Mean Absolute Error), que calcula o erro absoluto médio entre as coordenadas preditas e as verdadeiras. A `L1Loss` é geralmente mais robusta a *outliers* do que a `MSELoss` (L2) para regressão de *bounding boxes*.\n",
    "\n",
    "A perda total é a soma ponderada das duas. Para simplificar, usaremos pesos iguais para ambas.\n",
    "\n",
    "$$L_{total} = L_{classification} + \\lambda L_{regression}$$\n",
    "\n",
    "Aqui, definiremos $\\lambda=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ec5340",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalizationLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        class_logits, bbox_preds = predictions\n",
    "        class_labels, bbox_targets = targets\n",
    "        \n",
    "        # Calcula as perdas separadamente\n",
    "        classification_loss = self.cross_entropy_loss(class_logits, class_labels)\n",
    "        regression_loss = self.l1_loss(bbox_preds, bbox_targets)\n",
    "        \n",
    "        # Soma as perdas\n",
    "        total_loss = classification_loss + regression_loss\n",
    "        \n",
    "        return total_loss, classification_loss, regression_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc756b8e",
   "metadata": {},
   "source": [
    "### Executando o Treinamento\n",
    "\n",
    "Agora, vamos instanciar o modelo, a função de custo e o otimizador. Usaremos o otimizador Adam, que é uma escolha robusta para muitas tarefas de deep learning. Note que passamos para o otimizador apenas os parâmetros das novas cabeças (`classifier_head` e `regressor_head`), já que o *backbone* está congelado e não precisa ter seus gradientes atualizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38b04e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciação do modelo\n",
    "num_classes = len(SHAPES)\n",
    "model = LocalizationModel(num_classes=num_classes).to(device)\n",
    "\n",
    "# Apenas os parâmetros das novas camadas serão otimizados\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Função de custo\n",
    "criterion = LocalizationLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16def52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicionário para guardar o histórico\n",
    "history = {\n",
    "    'train_total_loss': [], 'train_cls_loss': [], 'train_reg_loss': [], 'train_acc': [],\n",
    "    'val_total_loss': [], 'val_cls_loss': [], 'val_reg_loss': [], 'val_acc': []\n",
    "}\n",
    "\n",
    "num_epochs = 15\n",
    "\n",
    "# Inicia o ciclo de treinamento\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\nÉpoca {epoch+1}/{num_epochs}')\n",
    "\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "            loader = train_loader\n",
    "        else:\n",
    "            model.eval()\n",
    "            loader = val_loader\n",
    "\n",
    "        running_total_loss = 0.0\n",
    "        running_cls_loss = 0.0\n",
    "        running_reg_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        size = len(loader.dataset)\n",
    "\n",
    "        loop = tqdm(loader, desc=f'{phase.capitalize()}')\n",
    "        for inputs, (labels, bboxes) in loop:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            bboxes = bboxes.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                class_logits, bbox_preds = model(inputs)\n",
    "                _, preds = torch.max(class_logits, 1)\n",
    "                \n",
    "                loss_total, loss_cls, loss_reg = criterion((class_logits, bbox_preds), (labels, bboxes))\n",
    "                \n",
    "                if phase == 'train':\n",
    "                    loss_total.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            running_total_loss += loss_total.item() * inputs.size(0)\n",
    "            running_cls_loss += loss_cls.item() * inputs.size(0)\n",
    "            running_reg_loss += loss_reg.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels)\n",
    "\n",
    "        epoch_total_loss = running_total_loss / size\n",
    "        epoch_cls_loss = running_cls_loss / size\n",
    "        epoch_reg_loss = running_reg_loss / size\n",
    "        epoch_acc = running_corrects.double() / size\n",
    "        \n",
    "        history[f'{phase}_total_loss'].append(epoch_total_loss)\n",
    "        history[f'{phase}_cls_loss'].append(epoch_cls_loss)\n",
    "        history[f'{phase}_reg_loss'].append(epoch_reg_loss)\n",
    "        history[f'{phase}_acc'].append(epoch_acc.item())\n",
    "\n",
    "        print(f'{phase.capitalize()} -> Total Loss: {epoch_total_loss:.4f} | Cls Loss: {epoch_cls_loss:.4f} | Reg Loss: {epoch_reg_loss:.4f} | Acc: {epoch_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfba513",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(18, 10))\n",
    "fig.suptitle('Histórico de Treinamento', fontsize=16)\n",
    "\n",
    "# Acurácia\n",
    "axes[0, 0].plot(history['train_acc'], label='Train Accuracy')\n",
    "axes[0, 0].plot(history['val_acc'], label='Validation Accuracy')\n",
    "axes[0, 0].set_title('Acurácia do Modelo')\n",
    "axes[0, 0].set_xlabel('Época')\n",
    "axes[0, 0].set_ylabel('Acurácia')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Perda Total\n",
    "axes[0, 1].plot(history['train_total_loss'], label='Train Total Loss')\n",
    "axes[0, 1].plot(history['val_total_loss'], label='Validation Total Loss')\n",
    "axes[0, 1].set_title('Perda Total')\n",
    "axes[0, 1].set_xlabel('Época')\n",
    "axes[0, 1].set_ylabel('Perda')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Perda de Classificação\n",
    "axes[1, 0].plot(history['train_cls_loss'], label='Train Classification Loss')\n",
    "axes[1, 0].plot(history['val_cls_loss'], label='Validation Classification Loss')\n",
    "axes[1, 0].set_title('Perda de Classificação (CLS)')\n",
    "axes[1, 0].set_xlabel('Época')\n",
    "axes[1, 0].set_ylabel('Perda')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Perda de Regressão\n",
    "axes[1, 1].plot(history['train_reg_loss'], label='Train Regression Loss')\n",
    "axes[1, 1].plot(history['val_reg_loss'], label='Validation Regression Loss')\n",
    "axes[1, 1].set_title('Perda de Regressão (REG)')\n",
    "axes[1, 1].set_xlabel('Época')\n",
    "axes[1, 1].set_ylabel('Perda')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4310a3",
   "metadata": {},
   "source": [
    "## Inferência e Visualização dos Resultados\n",
    "\n",
    "Após o treinamento, o passo final é avaliar o desempenho do modelo visualmente. Para quantificar a acurácia da localização, além da inspeção visual, utilizaremos a métrica *Intersection over Union* (IoU)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85337402",
   "metadata": {},
   "source": [
    "### Intersection over Union (IoU)\n",
    "\n",
    "A *Intersection over Union* (IoU), também conhecida como índice de Jaccard, é uma métrica utilizada para avaliar a sobreposição entre duas caixas delimitadoras: a caixa predita ($B_{p}$) e a caixa verdadeira (*ground truth*, $B_{gt}$). O valor do IoU varia de 0 (nenhuma sobreposição) a 1 (sobreposição perfeita).\n",
    "\n",
    "A fórmula é definida pela razão entre a área da interseção e a área da união das duas caixas:\n",
    "\n",
    "$$\n",
    "\\text{IoU}(B_p, B_{gt}) = \\frac{\\text{Área}(B_p \\cap B_{gt})}{\\text{Área}(B_p \\cup B_{gt})}\n",
    "$$\n",
    "\n",
    "Onde a área da união pode ser calculada como:\n",
    "\n",
    "$$\n",
    "\\text{Área}(B_p \\cup B_{gt}) = \\text{Área}(B_p) + \\text{Área}(B_{gt}) - \\text{Área}(B_p \\cap B_{gt})\n",
    "$$\n",
    "\n",
    "Para calcular a área da interseção, determinamos as coordenadas do retângulo de sobreposição. Assumindo que as caixas são definidas por seus cantos superior esquerdo $(x_1, y_1)$ e inferior direito $(x_2, y_2)$, o retângulo de interseção é dado por:\n",
    "\n",
    "-   $x_{inter\\_1} = \\max(x_{p1}, x_{gt1})$\n",
    "-   $y_{inter\\_1} = \\max(y_{p1}, y_{gt1})$\n",
    "-   $x_{inter\\_2} = \\min(x_{p2}, x_{gt2})$\n",
    "-   $y_{inter\\_2} = \\min(y_{p2}, y_{gt2})$\n",
    "\n",
    "A área da interseção é então o produto de sua largura e altura, garantindo que não sejam negativas: $\\max(0, x_{inter\\_2} - x_{inter\\_1}) \\times \\max(0, y_{inter\\_2} - y_{inter\\_1})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38809cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calcula a Intersection over Union (IoU) entre duas bounding boxes.\n",
    "    As boxes devem estar no formato [x1, y1, x2, y2] (canto superior esquerdo, canto inferior direito).\n",
    "    \"\"\"\n",
    "    # Determina as coordenadas (x, y) do retângulo de interseção\n",
    "    x1_inter = max(box1[0], box2[0])\n",
    "    y1_inter = max(box1[1], box2[1])\n",
    "    x2_inter = min(box1[2], box2[2])\n",
    "    y2_inter = min(box1[3], box2[3])\n",
    "\n",
    "    # Calcula a área da interseção\n",
    "    # max(0, ...) garante que a área não seja negativa se as caixas não se sobrepuserem\n",
    "    inter_width = max(0, x2_inter - x1_inter)\n",
    "    inter_height = max(0, y2_inter - y1_inter)\n",
    "    intersection_area = inter_width * inter_height\n",
    "\n",
    "    # Calcula a área de ambas as bounding boxes\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "    # Calcula a área da união\n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "\n",
    "    # Calcula a IoU\n",
    "    # Adiciona um pequeno epsilon para evitar divisão por zero\n",
    "    iou = intersection_area / (union_area + 1e-6)\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8e2bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_plot(model, dataset, device, num_images=5):\n",
    "    inv_classes = {v: k for k, v in CLASSES.items()}\n",
    "    model.eval()\n",
    "    \n",
    "    indices = random.sample(range(len(dataset)), num_images)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in indices:\n",
    "            image_tensor, (true_label, true_bbox) = dataset[i]\n",
    "            input_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "            \n",
    "            pred_logits, pred_bbox = model(input_tensor)\n",
    "            \n",
    "            pred_label_idx = torch.argmax(pred_logits, dim=1).item()\n",
    "            pred_bbox = pred_bbox[0].cpu().numpy()\n",
    "            true_bbox = true_bbox.numpy()\n",
    "            \n",
    "            # Desnormaliza a imagem para plotagem\n",
    "            mean = np.array([0.485, 0.456, 0.406])\n",
    "            std = np.array([0.229, 0.224, 0.225])\n",
    "            img = image_tensor.permute(1, 2, 0).numpy()\n",
    "            img = std * img + mean\n",
    "            img = np.clip(img, 0, 1)\n",
    "\n",
    "            # Converte as bounding boxes para formato [x, y, width, height]\n",
    "            true_bbox_px = convert_from_yolo_format(true_bbox, IMG_SIZE)\n",
    "            pred_bbox_px = convert_from_yolo_format(pred_bbox, IMG_SIZE)\n",
    "            \n",
    "            # Calcula o IoU\n",
    "            # Converte para [x1, y1, x2, y2] para a função de IoU\n",
    "            true_box_corners = [true_bbox_px[0], true_bbox_px[1], true_bbox_px[0] + true_bbox_px[2], true_bbox_px[1] + true_bbox_px[3]]\n",
    "            pred_box_corners = [pred_bbox_px[0], pred_bbox_px[1], pred_bbox_px[0] + pred_bbox_px[2], pred_bbox_px[1] + pred_bbox_px[3]]\n",
    "            iou_score = calculate_iou(true_box_corners, pred_box_corners)\n",
    "            \n",
    "            fig, ax = plt.subplots(1)\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "\n",
    "            # Cria e adiciona o retângulo da BBox verdadeira (verde)\n",
    "            rect_true = patches.Rectangle((true_bbox_px[0], true_bbox_px[1]), true_bbox_px[2], true_bbox_px[3], linewidth=2, edgecolor='g', facecolor='none')\n",
    "            ax.add_patch(rect_true)\n",
    "            \n",
    "            # Cria e adiciona o retângulo da BBox predita (vermelho)\n",
    "            rect_pred = patches.Rectangle((pred_bbox_px[0], pred_bbox_px[1]), pred_bbox_px[2], pred_bbox_px[3], linewidth=2, edgecolor='r', facecolor='none')\n",
    "            ax.add_patch(rect_pred)\n",
    "            \n",
    "            true_label_name = inv_classes[true_label.item()]\n",
    "            pred_label_name = inv_classes[pred_label_idx]\n",
    "            \n",
    "            plt.title(f'Verdadeiro: {true_label_name} (Verde)\\nPredito: {pred_label_name} (Vermelho)\\nIoU: {iou_score:.4f}')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98310265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executa a função de visualização nas imagens do conjunto de validação\n",
    "predict_and_plot(model, val_dataset.dataset, device, num_images=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
