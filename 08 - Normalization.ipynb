{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Técnicas de Normalização\n",
    "\n",
    "Este notebook explora diferentes técnicas de normalização utilizadas em redes neurais:\n",
    "\n",
    "1. **Normalização de Entrada (Input Normalization)**\n",
    "2. **Normalização em Lote (Batch Normalization)**\n",
    "3. **Normalização de Camada (Layer Normalization)**\n",
    "\n",
    "Cada técnica será apresentada com sua fundamentação matemática e implementação direta, mostrando como os dados são transformados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuração para reprodutibilidade\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuração de visualização\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Normalização de Entrada (Input Normalization)\n",
    "\n",
    "### Fundamentação Matemática\n",
    "\n",
    "A normalização de entrada padroniza os dados de entrada para ter média zero e desvio padrão unitário:\n",
    "\n",
    "$$\\hat{x}_i = \\frac{x_i - \\mu}{\\sigma}$$\n",
    "\n",
    "Onde:\n",
    "- $x_i$ é o valor original da feature $i$\n",
    "- $\\mu$ é a média: $\\mu = \\frac{1}{N}\\sum_{i=1}^{N} x_i$\n",
    "- $\\sigma$ é o desvio padrão: $\\sigma = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(x_i - \\mu)^2}$\n",
    "- $\\hat{x}_i$ é o valor normalizado\n",
    "\n",
    "### Benefícios:\n",
    "1. **Acelera a convergência** durante o treinamento\n",
    "2. **Previne saturação** das funções de ativação\n",
    "3. **Equaliza a importância** das diferentes features\n",
    "4. **Melhora a estabilidade numérica**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando dados sintéticos com escalas diferentes\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "n_features = 3\n",
    "\n",
    "# Features com diferentes escalas e distribuições\n",
    "feature1 = np.random.normal(100, 20, n_samples)  # Média alta, baixa variância\n",
    "feature2 = np.random.normal(5, 50, n_samples)    # Média baixa, alta variância\n",
    "feature3 = np.random.uniform(0, 1000, n_samples) # Distribuição uniforme\n",
    "\n",
    "data = np.column_stack([feature1, feature2, feature3])\n",
    "\n",
    "print(\"Dados originais:\")\n",
    "print(f\"Feature 1 - Média: {np.mean(data[:, 0]):.2f}, Desvio: {np.std(data[:, 0]):.2f}\")\n",
    "print(f\"Feature 2 - Média: {np.mean(data[:, 1]):.2f}, Desvio: {np.std(data[:, 1]):.2f}\")\n",
    "print(f\"Feature 3 - Média: {np.mean(data[:, 2]):.2f}, Desvio: {np.std(data[:, 2]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementação direta da normalização de entrada (Z-score)\n",
    "# Calcular média e desvio padrão para cada feature\n",
    "data_mean = np.mean(data, axis=0)  # Média ao longo das amostras\n",
    "data_std = np.std(data, axis=0)    # Desvio padrão ao longo das amostras\n",
    "\n",
    "print(\"Estatísticas calculadas:\")\n",
    "print(f\"Médias: {data_mean}\")\n",
    "print(f\"Desvios padrão: {data_std}\")\n",
    "\n",
    "# Aplicar normalização: (x - média) / desvio_padrão\n",
    "data_normalized = (data - data_mean) / data_std\n",
    "\n",
    "print(\"\\nDados normalizados:\")\n",
    "print(f\"Feature 1 - Média: {np.mean(data_normalized[:, 0]):.6f}, Desvio: {np.std(data_normalized[:, 0]):.6f}\")\n",
    "print(f\"Feature 2 - Média: {np.mean(data_normalized[:, 1]):.6f}, Desvio: {np.std(data_normalized[:, 1]):.6f}\")\n",
    "print(f\"Feature 3 - Média: {np.mean(data_normalized[:, 2]):.6f}, Desvio: {np.std(data_normalized[:, 2]):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização da normalização de entrada\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Dados originais\n",
    "for i in range(3):\n",
    "    axes[0, i].hist(data[:, i], bins=50, alpha=0.7, color=f'C{i}')\n",
    "    axes[0, i].set_title(f'Feature {i+1} - Original\\nMédia: {np.mean(data[:, i]):.1f}, Std: {np.std(data[:, i]):.1f}')\n",
    "    axes[0, i].set_ylabel('Frequência')\n",
    "\n",
    "# Dados normalizados\n",
    "for i in range(3):\n",
    "    axes[1, i].hist(data_normalized[:, i], bins=50, alpha=0.7, color=f'C{i}')\n",
    "    axes[1, i].set_title(f'Feature {i+1} - Normalizada\\nMédia: {np.mean(data_normalized[:, i]):.3f}, Std: {np.std(data_normalized[:, i]):.3f}')\n",
    "    axes[1, i].set_xlabel('Valor')\n",
    "    axes[1, i].set_ylabel('Frequência')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Comparação: Dados Originais vs Normalizados', y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Normalização em Lote (Batch Normalization)\n",
    "\n",
    "### Fundamentação Matemática\n",
    "\n",
    "A Batch Normalization normaliza as ativações de uma camada ao longo da dimensão do batch:\n",
    "\n",
    "**Passo 1:** Calcular estatísticas do batch\n",
    "$$\\mu_B = \\frac{1}{m}\\sum_{i=1}^{m} x_i$$\n",
    "$$\\sigma_B^2 = \\frac{1}{m}\\sum_{i=1}^{m}(x_i - \\mu_B)^2$$\n",
    "\n",
    "**Passo 2:** Normalizar\n",
    "$$\\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$$\n",
    "\n",
    "**Passo 3:** Escalar e deslocar (parâmetros aprendíveis)\n",
    "$$y_i = \\gamma \\hat{x}_i + \\beta$$\n",
    "\n",
    "Onde:\n",
    "- $m$ é o tamanho do batch\n",
    "- $\\epsilon$ é uma constante pequena para estabilidade numérica (tipicamente $10^{-5}$)\n",
    "- $\\gamma$ e $\\beta$ são parâmetros aprendíveis (scale e shift)\n",
    "\n",
    "### Benefícios:\n",
    "1. **Reduz o Internal Covariate Shift**\n",
    "2. **Permite taxas de aprendizado maiores**\n",
    "3. **Atua como regularizador**\n",
    "4. **Reduz dependência da inicialização**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulando ativações de uma camada neural\n",
    "batch_size = 32\n",
    "n_features = 10\n",
    "\n",
    "# Ativações não normalizadas (simulando saída de uma camada linear)\n",
    "x = np.random.normal(5, 2, (batch_size, n_features))  # Média 5, std 2\n",
    "\n",
    "print(\"Ativações antes da Batch Norm:\")\n",
    "print(f\"Shape: {x.shape} (batch_size, features)\")\n",
    "print(f\"Média geral: {np.mean(x):.3f}\")\n",
    "print(f\"Desvio padrão geral: {np.std(x):.3f}\")\n",
    "print(f\"Média por feature (primeiras 5): {np.mean(x, axis=0)[:5]}\")\n",
    "print(f\"Desvio padrão por feature (primeiras 5): {np.std(x, axis=0)[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementação direta da Batch Normalization\n",
    "\n",
    "# Passo 1: Calcular estatísticas do batch\n",
    "batch_mean = np.mean(x, axis=0)  # Média ao longo do batch (para cada feature)\n",
    "batch_var = np.var(x, axis=0)    # Variância ao longo do batch (para cada feature)\n",
    "\n",
    "print(\"Passo 1 - Estatísticas do batch:\")\n",
    "print(f\"Média do batch por feature: {batch_mean[:5]}...\")\n",
    "print(f\"Variância do batch por feature: {batch_var[:5]}...\")\n",
    "\n",
    "# Passo 2: Normalizar\n",
    "eps = 1e-5  # Epsilon para estabilidade numérica\n",
    "x_normalized = (x - batch_mean) / np.sqrt(batch_var + eps)\n",
    "\n",
    "print(\"\\nPasso 2 - Após normalização:\")\n",
    "print(f\"Média por feature: {np.mean(x_normalized, axis=0)[:5]}...\")\n",
    "print(f\"Desvio padrão por feature: {np.std(x_normalized, axis=0)[:5]}...\")\n",
    "\n",
    "# Passo 3: Escalar e deslocar (inicialização padrão)\n",
    "gamma = np.ones(n_features)    # Parâmetro de escala (inicializado com 1)\n",
    "beta = np.zeros(n_features)    # Parâmetro de deslocamento (inicializado com 0)\n",
    "\n",
    "y = gamma * x_normalized + beta\n",
    "\n",
    "print(\"\\nPasso 3 - Após escalar e deslocar:\")\n",
    "print(f\"Gamma (primeiros 5): {gamma[:5]}\")\n",
    "print(f\"Beta (primeiros 5): {beta[:5]}\")\n",
    "print(f\"Saída final - média por feature: {np.mean(y, axis=0)[:5]}...\")\n",
    "print(f\"Saída final - desvio padrão por feature: {np.std(y, axis=0)[:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização dos efeitos da Batch Normalization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Histograma das ativações antes da normalização\n",
    "axes[0, 0].hist(x.flatten(), bins=50, alpha=0.7, color='red')\n",
    "axes[0, 0].set_title('Distribuição das Ativações - Antes da Batch Norm')\n",
    "axes[0, 0].set_xlabel('Valor da Ativação')\n",
    "axes[0, 0].set_ylabel('Frequência')\n",
    "axes[0, 0].axvline(np.mean(x), color='darkred', linestyle='--', label=f'Média: {np.mean(x):.2f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Histograma das ativações depois da normalização\n",
    "axes[0, 1].hist(y.flatten(), bins=50, alpha=0.7, color='blue')\n",
    "axes[0, 1].set_title('Distribuição das Ativações - Depois da Batch Norm')\n",
    "axes[0, 1].set_xlabel('Valor da Ativação')\n",
    "axes[0, 1].set_ylabel('Frequência')\n",
    "axes[0, 1].axvline(np.mean(y), color='darkblue', linestyle='--', label=f'Média: {np.mean(y):.2f}')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Comparação das médias por feature\n",
    "features = range(n_features)\n",
    "axes[1, 0].plot(features, np.mean(x, axis=0), 'ro-', label='Antes BN', markersize=4)\n",
    "axes[1, 0].plot(features, np.mean(y, axis=0), 'bo-', label='Depois BN', markersize=4)\n",
    "axes[1, 0].set_title('Média por Feature')\n",
    "axes[1, 0].set_xlabel('Feature')\n",
    "axes[1, 0].set_ylabel('Média')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Comparação dos desvios padrão por feature\n",
    "axes[1, 1].plot(features, np.std(x, axis=0), 'ro-', label='Antes BN', markersize=4)\n",
    "axes[1, 1].plot(features, np.std(y, axis=0), 'bo-', label='Depois BN', markersize=4)\n",
    "axes[1, 1].set_title('Desvio Padrão por Feature')\n",
    "axes[1, 1].set_xlabel('Feature')\n",
    "axes[1, 1].set_ylabel('Desvio Padrão')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalização de Camada (Layer Normalization)\n",
    "\n",
    "### Fundamentação Matemática\n",
    "\n",
    "A Layer Normalization normaliza as ativações ao longo da dimensão das features (em vez do batch):\n",
    "\n",
    "**Passo 1:** Calcular estatísticas da camada para cada amostra\n",
    "$$\\mu^{(i)} = \\frac{1}{H}\\sum_{j=1}^{H} x_j^{(i)}$$\n",
    "$$\\sigma^{(i)2} = \\frac{1}{H}\\sum_{j=1}^{H}(x_j^{(i)} - \\mu^{(i)})^2$$\n",
    "\n",
    "**Passo 2:** Normalizar\n",
    "$$\\hat{x}_j^{(i)} = \\frac{x_j^{(i)} - \\mu^{(i)}}{\\sqrt{\\sigma^{(i)2} + \\epsilon}}$$\n",
    "\n",
    "**Passo 3:** Escalar e deslocar\n",
    "$$y_j^{(i)} = \\gamma_j \\hat{x}_j^{(i)} + \\beta_j$$\n",
    "\n",
    "Onde:\n",
    "- $i$ indexa as amostras no batch\n",
    "- $j$ indexa as features\n",
    "- $H$ é o número de features\n",
    "\n",
    "### Diferenças da Batch Normalization:\n",
    "1. **Independente do batch size**\n",
    "2. **Funciona bem em RNNs**\n",
    "3. **Comportamento idêntico em treino e teste**\n",
    "4. **Normaliza ao longo das features, não das amostras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar as mesmas ativações do exemplo anterior\n",
    "print(\"Ativações antes da Layer Norm:\")\n",
    "print(f\"Shape: {x.shape} (batch_size, features)\")\n",
    "print(f\"Média por amostra (primeiras 5): {np.mean(x, axis=1)[:5]}\")\n",
    "print(f\"Desvio padrão por amostra (primeiras 5): {np.std(x, axis=1)[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementação direta da Layer Normalization\n",
    "\n",
    "# Passo 1: Calcular estatísticas da camada para cada amostra\n",
    "layer_mean = np.mean(x, axis=1, keepdims=True)  # Média ao longo das features (para cada amostra)\n",
    "layer_var = np.var(x, axis=1, keepdims=True)    # Variância ao longo das features (para cada amostra)\n",
    "\n",
    "print(\"Passo 1 - Estatísticas da camada:\")\n",
    "print(f\"Shape das médias: {layer_mean.shape}\")\n",
    "print(f\"Média por amostra (primeiras 5): {layer_mean[:5, 0]}\")\n",
    "print(f\"Variância por amostra (primeiras 5): {layer_var[:5, 0]}\")\n",
    "\n",
    "# Passo 2: Normalizar\n",
    "eps = 1e-5\n",
    "x_ln_normalized = (x - layer_mean) / np.sqrt(layer_var + eps)\n",
    "\n",
    "print(\"\\nPasso 2 - Após normalização:\")\n",
    "print(f\"Média por amostra: {np.mean(x_ln_normalized, axis=1)[:5]}\")\n",
    "print(f\"Desvio padrão por amostra: {np.std(x_ln_normalized, axis=1)[:5]}\")\n",
    "\n",
    "# Passo 3: Escalar e deslocar\n",
    "gamma_ln = np.ones(n_features)    # Parâmetro de escala\n",
    "beta_ln = np.zeros(n_features)    # Parâmetro de deslocamento\n",
    "\n",
    "y_ln = gamma_ln * x_ln_normalized + beta_ln\n",
    "\n",
    "print(\"\\nPasso 3 - Após escalar e deslocar:\")\n",
    "print(f\"Saída final - média por amostra: {np.mean(y_ln, axis=1)[:5]}\")\n",
    "print(f\"Saída final - desvio padrão por amostra: {np.std(y_ln, axis=1)[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparação visual entre Batch Norm e Layer Norm\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Dados originais como heatmap\n",
    "im1 = axes[0, 0].imshow(x.T, cmap='viridis', aspect='auto')\n",
    "axes[0, 0].set_title('Ativações Originais')\n",
    "axes[0, 0].set_xlabel('Amostras')\n",
    "axes[0, 0].set_ylabel('Features')\n",
    "plt.colorbar(im1, ax=axes[0, 0])\n",
    "\n",
    "# Batch Normalization\n",
    "im2 = axes[0, 1].imshow(y.T, cmap='viridis', aspect='auto')\n",
    "axes[0, 1].set_title('Batch Normalization')\n",
    "axes[0, 1].set_xlabel('Amostras')\n",
    "axes[0, 1].set_ylabel('Features')\n",
    "plt.colorbar(im2, ax=axes[0, 1])\n",
    "\n",
    "# Layer Normalization\n",
    "im3 = axes[0, 2].imshow(y_ln.T, cmap='viridis', aspect='auto')\n",
    "axes[0, 2].set_title('Layer Normalization')\n",
    "axes[0, 2].set_xlabel('Amostras')\n",
    "axes[0, 2].set_ylabel('Features')\n",
    "plt.colorbar(im3, ax=axes[0, 2])\n",
    "\n",
    "# Estatísticas por dimensão\n",
    "samples = range(batch_size)\n",
    "features = range(n_features)\n",
    "\n",
    "# Média ao longo das amostras (Batch Norm normaliza isso)\n",
    "axes[1, 0].plot(features, np.mean(x, axis=0), 'ro-', label='Original', markersize=4)\n",
    "axes[1, 0].plot(features, np.mean(y, axis=0), 'bo-', label='Batch Norm', markersize=4)\n",
    "axes[1, 0].plot(features, np.mean(y_ln, axis=0), 'go-', label='Layer Norm', markersize=4)\n",
    "axes[1, 0].set_title('Média ao longo das Amostras')\n",
    "axes[1, 0].set_xlabel('Features')\n",
    "axes[1, 0].set_ylabel('Média')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Média ao longo das features (Layer Norm normaliza isso)\n",
    "axes[1, 1].plot(samples, np.mean(x, axis=1), 'ro-', label='Original', markersize=3)\n",
    "axes[1, 1].plot(samples, np.mean(y, axis=1), 'bo-', label='Batch Norm', markersize=3)\n",
    "axes[1, 1].plot(samples, np.mean(y_ln, axis=1), 'go-', label='Layer Norm', markersize=3)\n",
    "axes[1, 1].set_title('Média ao longo das Features')\n",
    "axes[1, 1].set_xlabel('Amostras')\n",
    "axes[1, 1].set_ylabel('Média')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "# Distribuição geral\n",
    "axes[1, 2].hist(x.flatten(), bins=30, alpha=0.5, label='Original', color='red')\n",
    "axes[1, 2].hist(y.flatten(), bins=30, alpha=0.5, label='Batch Norm', color='blue')\n",
    "axes[1, 2].hist(y_ln.flatten(), bins=30, alpha=0.5, label='Layer Norm', color='green')\n",
    "axes[1, 2].set_title('Distribuição das Ativações')\n",
    "axes[1, 2].set_xlabel('Valor')\n",
    "axes[1, 2].set_ylabel('Frequência')\n",
    "axes[1, 2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparação das Técnicas\n",
    "\n",
    "### Resumo das Diferenças:\n",
    "\n",
    "| Técnica | Normaliza ao longo de | Parâmetros Aprendíveis | Uso Principal |\n",
    "|---------|----------------------|------------------------|---------------|\n",
    "| **Input Norm** | Amostras (features) | Não | Pré-processamento |\n",
    "| **Batch Norm** | Batch (amostras) | Sim (γ, β) | CNNs, MLPs |\n",
    "| **Layer Norm** | Features (camada) | Sim (γ, β) | RNNs, Transformers |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstração do comportamento com diferentes batch sizes\n",
    "print(\"Efeito do batch size na Batch Normalization:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "batch_sizes = [4, 16, 64]\n",
    "n_features = 10\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    # Gerar dados com distribuição fixa\n",
    "    x_test = np.random.normal(10, 3, (bs, n_features))\n",
    "    \n",
    "    # Aplicar Batch Normalization\n",
    "    batch_mean = np.mean(x_test, axis=0)\n",
    "    batch_var = np.var(x_test, axis=0)\n",
    "    x_bn = (x_test - batch_mean) / np.sqrt(batch_var + 1e-5)\n",
    "    \n",
    "    # Aplicar Layer Normalization\n",
    "    layer_mean = np.mean(x_test, axis=1, keepdims=True)\n",
    "    layer_var = np.var(x_test, axis=1, keepdims=True)\n",
    "    x_ln = (x_test - layer_mean) / np.sqrt(layer_var + 1e-5)\n",
    "    \n",
    "    print(f\"\\nBatch Size: {bs}\")\n",
    "    print(f\"Batch Norm - Consistência da normalização: {np.std(np.var(x_bn, axis=0)):.4f}\")\n",
    "    print(f\"Layer Norm - Consistência da normalização: {np.std(np.var(x_ln, axis=1)):.4f}\")\n",
    "\n",
    "print(\"\\nObservação: Batch Norm é mais instável com batches pequenos\")\n",
    "print(\"Layer Norm mantém comportamento consistente independente do batch size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementações no PyTorch\n",
    "\n",
    "Agora vamos ver como usar as implementações otimizadas do PyTorch para cada técnica de normalização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Input Normalization com PyTorch\n",
    "print(\"1. Input Normalization com PyTorch:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Converter dados para tensor PyTorch\n",
    "data_tensor = torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "# Normalização manual\n",
    "mean_torch = torch.mean(data_tensor, dim=0)\n",
    "std_torch = torch.std(data_tensor, dim=0)\n",
    "normalized_torch = (data_tensor - mean_torch) / std_torch\n",
    "\n",
    "print(f\"Original shape: {data_tensor.shape}\")\n",
    "print(f\"Média após normalização: {torch.mean(normalized_torch, dim=0)[:3]}\")\n",
    "print(f\"Std após normalização: {torch.std(normalized_torch, dim=0)[:3]}\")\n",
    "\n",
    "# Comparação com sklearn StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "normalized_sklearn = scaler.fit_transform(data)\n",
    "\n",
    "print(f\"\\nComparação com sklearn - diferença máxima: {np.max(np.abs(normalized_torch.numpy() - normalized_sklearn)):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Batch Normalization com PyTorch\n",
    "print(\"2. Batch Normalization com PyTorch:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Criar uma camada de Batch Normalization\n",
    "batch_norm = nn.BatchNorm1d(n_features)\n",
    "\n",
    "# Dados de teste\n",
    "test_input = torch.randn(32, n_features)  # Batch de 32 amostras, n_features\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Input - mean: {test_input.mean().item():.4f}, std: {test_input.std().item():.4f}\")\n",
    "\n",
    "# Aplicar Batch Normalization (modo treino)\n",
    "batch_norm.train()\n",
    "output_bn = batch_norm(test_input)\n",
    "\n",
    "print(f\"Output BN - mean: {output_bn.mean().item():.6f}, std: {output_bn.std().item():.4f}\")\n",
    "\n",
    "# Mostrar parâmetros aprendíveis\n",
    "print(f\"\\nParâmetros do BatchNorm1d:\")\n",
    "print(f\"Weight (gamma) shape: {batch_norm.weight.shape}\")\n",
    "print(f\"Bias (beta) shape: {batch_norm.bias.shape}\")\n",
    "print(f\"Running mean shape: {batch_norm.running_mean.shape}\")\n",
    "print(f\"Running var shape: {batch_norm.running_var.shape}\")\n",
    "\n",
    "# Valores iniciais\n",
    "print(f\"\\nValores iniciais:\")\n",
    "print(f\"Weight (primeiros 5): {batch_norm.weight.data[:5]}\")\n",
    "print(f\"Bias (primeiros 5): {batch_norm.bias.data[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Layer Normalization com PyTorch\n",
    "print(\"3. Layer Normalization com PyTorch:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Criar uma camada de Layer Normalization\n",
    "layer_norm = nn.LayerNorm(n_features)\n",
    "\n",
    "# Usar os mesmos dados de teste\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Input - mean: {test_input.mean().item():.4f}, std: {test_input.std().item():.4f}\")\n",
    "\n",
    "# Aplicar Layer Normalization\n",
    "output_ln = layer_norm(test_input)\n",
    "\n",
    "print(f\"Output LN - mean: {output_ln.mean().item():.6f}, std: {output_ln.std().item():.4f}\")\n",
    "\n",
    "# Analisar normalização por amostra\n",
    "sample_means_ln = output_ln.mean(dim=-1)  # Média ao longo da dimensão das features\n",
    "sample_stds_ln = output_ln.std(dim=-1)    # Std ao longo da dimensão das features\n",
    "\n",
    "print(f\"\\nLayer Norm - Médias por amostra (primeiras 5): {sample_means_ln[:5]}\")\n",
    "print(f\"Layer Norm - Stds por amostra (primeiras 5): {sample_stds_ln[:5]}\")\n",
    "\n",
    "# Mostrar parâmetros\n",
    "print(f\"\\nParâmetros do LayerNorm:\")\n",
    "print(f\"Weight (gamma) shape: {layer_norm.weight.shape}\")\n",
    "print(f\"Bias (beta) shape: {layer_norm.bias.shape}\")\n",
    "print(f\"Weight (primeiros 5): {layer_norm.weight.data[:5]}\")\n",
    "print(f\"Bias (primeiros 5): {layer_norm.bias.data[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Comparação lado a lado das implementações PyTorch\n",
    "print(\"Comparação das Normalizações PyTorch:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Dados de entrada com características específicas\n",
    "torch.manual_seed(42)\n",
    "test_data = torch.randn(16, 8) * 5 + 10  # Média ~10, std ~5\n",
    "\n",
    "print(f\"Dados originais:\")\n",
    "print(f\"  Shape: {test_data.shape}\")\n",
    "print(f\"  Mean: {test_data.mean().item():.3f}\")\n",
    "print(f\"  Std: {test_data.std().item():.3f}\")\n",
    "\n",
    "# Input normalization manual\n",
    "input_mean = test_data.mean(dim=0)\n",
    "input_std = test_data.std(dim=0)\n",
    "input_normalized = (test_data - input_mean) / input_std\n",
    "\n",
    "# Batch normalization\n",
    "bn = nn.BatchNorm1d(8)\n",
    "bn.train()\n",
    "batch_normalized = bn(test_data)\n",
    "\n",
    "# Layer normalization\n",
    "ln = nn.LayerNorm(8)\n",
    "layer_normalized = ln(test_data)\n",
    "\n",
    "print(f\"\\nApós Input Normalization:\")\n",
    "print(f\"  Mean: {input_normalized.mean().item():.6f}\")\n",
    "print(f\"  Std: {input_normalized.std().item():.6f}\")\n",
    "\n",
    "print(f\"\\nApós Batch Normalization:\")\n",
    "print(f\"  Mean: {batch_normalized.mean().item():.6f}\")\n",
    "print(f\"  Std: {batch_normalized.std().item():.6f}\")\n",
    "\n",
    "print(f\"\\nApós Layer Normalization:\")\n",
    "print(f\"  Mean: {layer_normalized.mean().item():.6f}\")\n",
    "print(f\"  Std: {layer_normalized.std().item():.6f}\")\n",
    "\n",
    "# Verificar propriedades específicas\n",
    "print(f\"\\nPropriedades específicas:\")\n",
    "print(f\"Input Norm - std por feature: {input_normalized.std(dim=0)[:3]}...\")\n",
    "print(f\"Batch Norm - std por feature: {batch_normalized.std(dim=0)[:3]}...\")\n",
    "print(f\"Layer Norm - std por amostra: {layer_normalized.std(dim=1)[:3]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Resumo e Recomendações de Uso\n",
    "\n",
    "### Quando Usar Cada Técnica:\n",
    "\n",
    "1. **Input Normalization (Normalização de Entrada)**\n",
    "   - **Sempre** aplicar nos dados de entrada\n",
    "   - Especialmente importante quando features têm escalas muito diferentes\n",
    "   - Usar StandardScaler ou normalização Z-score\n",
    "\n",
    "2. **Batch Normalization**\n",
    "   - **CNNs**: Quase sempre benéfico\n",
    "   - **MLPs**: Muito útil para redes profundas\n",
    "   - **Atenção**: Cuidado com batch sizes pequenos\n",
    "   - **Posição**: Geralmente entre camada linear e ativação\n",
    "\n",
    "3. **Layer Normalization**\n",
    "   - **RNNs/LSTMs**: Preferível à Batch Norm\n",
    "   - **Transformers**: Padrão da arquitetura\n",
    "   - **Batch sizes variáveis**: Mais estável que Batch Norm\n",
    "   - **Inferência**: Comportamento idêntico ao treino\n",
    "\n",
    "### Implementação no PyTorch:\n",
    "\n",
    "```python\n",
    "# Input Normalization\n",
    "mean = X.mean(dim=0)\n",
    "std = X.std(dim=0)\n",
    "X_norm = (X - mean) / std\n",
    "\n",
    "# Batch Normalization\n",
    "self.bn = nn.BatchNorm1d(num_features)\n",
    "\n",
    "# Layer Normalization  \n",
    "self.ln = nn.LayerNorm(normalized_shape)\n",
    "```\n",
    "\n",
    "### Considerações Importantes:\n",
    "\n",
    "- **Batch Norm** requer diferentes comportamentos entre treino e avaliação (`model.train()` vs `model.eval()`)\n",
    "- **Layer Norm** é determinística e não depende do batch\n",
    "- **Ambas** introduzem parâmetros aprendíveis (γ e β)\n",
    "- **Normalização de entrada** é determinística e não tem parâmetros aprendíveis\n",
    "\n",
    "### Próximos Passos:\n",
    "- Experimente com diferentes posições das normalizações (antes vs depois da ativação)\n",
    "- Teste o impacto em diferentes arquiteturas (CNNs, RNNs, Transformers)\n",
    "- Analise o efeito em diferentes tamanhos de batch\n",
    "- Compare com outras técnicas como Group Normalization e Instance Normalization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
