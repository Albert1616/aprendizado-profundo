{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feb5ed1b",
   "metadata": {},
   "source": [
    "# Imagens e Filtros\n",
    "\n",
    "Neste notebook, exploraremos os fundamentos da Visão Computacional, focando em como as imagens são representadas computacionalmente e como a operação de convolução é utilizada para extrair características por meio de filtros (kernels).\n",
    "\n",
    "### Conteúdo Abordado\n",
    "\n",
    "1.  **Representação de Imagens**:\n",
    "    * Imagens como tensores (arrays NumPy).\n",
    "    * Criação e manipulação de imagens em escala de cinza e RGB.\n",
    "    * Carregamento de imagens externas.\n",
    "2.  **A Operação de Convolução 2D**:\n",
    "    * Definição matemática da convolução discreta.\n",
    "    * Implementação da convolução a partir do zero.\n",
    "3.  **Filtros (Kernels) e suas Aplicações**:\n",
    "    * Filtro de suavização (Blur).\n",
    "    * Filtro de realce de detalhes (Sharpen).\n",
    "    * Filtros de detecção de bordas (Sobel)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a17c2c5",
   "metadata": {},
   "source": [
    "## 1. Representação de Imagens\n",
    "\n",
    "Uma imagem digital é, fundamentalmente, uma estrutura de dados numérica. Para o computador, uma imagem é um tensor (ou um array multidimensional) de valores de intensidade de pixel. Em uma imagem em escala de cinza (*grayscale*), temos uma matriz 2D onde cada elemento representa a intensidade de um pixel, geralmente em um intervalo de [0, 255]. Para imagens coloridas, o modelo mais comum é o RGB, que utiliza um tensor 3D, onde a terceira dimensão representa os canais de cor: Vermelho (Red), Verde (Green) e Azul (Blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5c9fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03faae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define as dimensões da imagem\n",
    "height, width = 100, 100\n",
    "\n",
    "# Cria uma imagem preta (todos os pixels com valor 0)\n",
    "grayscale_image = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "print(f\"Shape da imagem em escala de cinza: {grayscale_image.shape}\")\n",
    "\n",
    "# Vamos adicionar um retângulo branco no centro\n",
    "# O valor 255 corresponde à cor branca\n",
    "start_row, end_row = height // 4, 3 * height // 4\n",
    "start_col, end_col = width // 4, 3 * width // 4\n",
    "grayscale_image[start_row:end_row, start_col:end_col] = 255\n",
    "\n",
    "# Exibe a imagem\n",
    "plt.imshow(grayscale_image, cmap='gray', vmin=0, vmax=255)\n",
    "plt.title(\"Imagem Grayscale Gerada com NumPy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b39ff9e",
   "metadata": {},
   "source": [
    "### Indexação e Modificação de Imagens\n",
    "\n",
    "Como as imagens são representadas por arrays NumPy, podemos usar as operações de indexação e *slicing* para acessar e modificar regiões específicas da imagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27f79ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz uma cópia da imagem anterior para modificação\n",
    "modified_image = grayscale_image.copy()\n",
    "\n",
    "# Altera um quadrante da imagem para um tom de cinza médio (valor 128)\n",
    "modified_image[0:height//2, 0:width//2] = 128\n",
    "\n",
    "# Exibe a imagem modificada\n",
    "plt.imshow(modified_image, cmap='gray', vmin=0, vmax=255)\n",
    "plt.title(\"Imagem Modificada\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854f0b67",
   "metadata": {},
   "source": [
    "### Imagens Coloridas (RGB)\n",
    "\n",
    "Imagens coloridas no padrão RGB são representadas como tensores 3D com shape `(altura, largura, 3)`. A terceira dimensão, de tamanho 3, corresponde aos canais de Vermelho, Verde e Azul. Cada pixel `(i, j)` é, portanto, um vetor de três valores `[R, G, B]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e195e636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define as dimensões\n",
    "height, width = 100, 100\n",
    "\n",
    "# Cria uma imagem preta\n",
    "rgb_image = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "print(f\"Shape da imagem RGB: {rgb_image.shape}\")\n",
    "\n",
    "# Adiciona uma faixa vermelha na imagem\n",
    "# O canal 0 é o Vermelho (Red)\n",
    "rgb_image[:, 0:width//3, 0] = 255\n",
    "\n",
    "# Adiciona uma faixa verde na imagem\n",
    "# O canal 1 é o Verde (Green)\n",
    "rgb_image[:, width//3:2*width//3, 1] = 255\n",
    "\n",
    "# Adiciona uma faixa azul na imagem\n",
    "# O canal 2 é o Azul (Blue)\n",
    "rgb_image[:, 2*width//3:width, 2] = 255\n",
    "\n",
    "# Exibe a imagem\n",
    "plt.imshow(rgb_image)\n",
    "plt.title(\"Imagem RGB Gerada com NumPy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ff969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_circle(image, center, radius, value=255):\n",
    "    cy, cx = center\n",
    "    y = np.arange(image.shape[0])[:, None]  # coluna\n",
    "    x = np.arange(image.shape[1])[None, :]  # linha\n",
    "    mask = (x - cx) ** 2 + (y - cy) ** 2 <= radius ** 2\n",
    "    image[mask] = value\n",
    "\n",
    "# Define dimensões da imagem\n",
    "height, width = 300, 400\n",
    "\n",
    "# Cria canais R, G e B com zeros\n",
    "red_channel = np.zeros((height, width), dtype=np.uint8)\n",
    "green_channel = np.zeros((height, width), dtype=np.uint8)\n",
    "blue_channel = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "# Parâmetros dos círculos\n",
    "radius = 80\n",
    "center_offset = 60\n",
    "\n",
    "center_red = (height // 2 - center_offset // 2, width // 2 - center_offset)\n",
    "center_green = (height // 2 - center_offset // 2, width // 2 + center_offset)\n",
    "center_blue = (height // 2 + int(center_offset * 0.8), width // 2)\n",
    "\n",
    "# Desenha cada círculo no seu canal\n",
    "draw_circle(red_channel, center_red, radius)\n",
    "draw_circle(green_channel, center_green, radius)\n",
    "draw_circle(blue_channel, center_blue, radius)\n",
    "\n",
    "# Junta os canais em imagem RGB\n",
    "color_circles_image = np.stack([red_channel, green_channel, blue_channel], axis=-1)\n",
    "\n",
    "# Exibe o resultado\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(color_circles_image)\n",
    "plt.title(\"Círculos nos Canais R, G e B\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1820177c",
   "metadata": {},
   "source": [
    "### Carregando Imagens Reais\n",
    "\n",
    "Na prática, raramente criamos imagens do zero. O fluxo de trabalho comum envolve carregar imagens de arquivos. Bibliotecas como `Pillow` (PIL) e `OpenCV` são usadas para essa finalidade. `Matplotlib` também pode carregar e exibir imagens.\n",
    "\n",
    "A biblioteca **OpenCV** (`cv2`) é o padrão de fato para a maioria das tarefas de Visão Computacional. Ela é altamente otimizada, implementada em C/C++ com wrappers para Python, e oferece uma vasta gama de funcionalidades prontas para processamento de imagem e vídeo.\n",
    "\n",
    "A função principal para carregar uma imagem é `cv2.imread()`. É importante notar duas características:\n",
    "1.  **Formato de Dados**: O OpenCV carrega imagens diretamente como arrays NumPy, o que facilita a integração com o ecossistema de computação científica do Python.\n",
    "2.  **Ordem dos Canais de Cor**: Por razões históricas, o OpenCV carrega imagens coloridas no formato **BGR** (Blue, Green, Red), e não no tradicional RGB. Bibliotecas de visualização como o Matplotlib esperam imagens em RGB. Portanto, ao carregar uma imagem colorida com `cv2` para exibi-la com `plt`, é quase sempre necessário converter o espaço de cores de BGR para RGB.\n",
    "\n",
    "A função `cv2.imread()` aceita um segundo argumento, uma *flag* que define como a imagem deve ser carregada:\n",
    "* `cv2.IMREAD_COLOR` ou `1`: Carrega a imagem em BGR (padrão).\n",
    "* `cv2.IMREAD_GRAYSCALE` ou `0`: Converte a imagem para escala de cinza.\n",
    "* `cv2.IMREAD_UNCHANGED` ou `-1`: Carrega a imagem como está, incluindo o canal alfa (transparência), se houver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baba9676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# URL da imagem de exemplo\n",
    "image_url = \"https://lirp.cdn-website.com/7ece8951/dms3rep/multi/opt/972752326-640w.jpg\"\n",
    "image_filename = \"dog.jpg\"\n",
    "\n",
    "# Baixa a imagem\n",
    "response = requests.get(image_url)\n",
    "with open(image_filename, \"wb\") as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74742583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Carrega a imagem em escala de cinza usando OpenCV\n",
    "gray_image_loaded = cv2.imread(image_filename, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "print(f\"Shape da imagem grayscale (OpenCV): {gray_image_loaded.shape}\")\n",
    "print(f\"Tipo de dados (OpenCV): {gray_image_loaded.dtype}\")\n",
    "\n",
    "plt.imshow(gray_image_loaded, cmap='gray')\n",
    "plt.title(\"Imagem Grayscale Carregada com OpenCV\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec43651",
   "metadata": {},
   "source": [
    "### A Conversão de BGR para RGB\n",
    "\n",
    "Agora, vamos carregar a mesma imagem em modo colorido para observar a inversão de canais. Ao exibi-la diretamente com `matplotlib`, as cores parecerão incorretas. A correção é feita com a função `cv2.cvtColor()`, especificando a conversão `cv2.COLOR_BGR2RGB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae144738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega a imagem colorida (padrão BGR)\n",
    "bgr_image_loaded = cv2.imread(image_filename, cv2.IMREAD_COLOR)\n",
    "\n",
    "# Converte de BGR para RGB para a visualização correta\n",
    "rgb_image_loaded = cv2.cvtColor(bgr_image_loaded, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print(f\"Shape da imagem colorida: {bgr_image_loaded.shape}\")\n",
    "\n",
    "# Exibe as imagens para comparação\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axes[0].imshow(bgr_image_loaded)\n",
    "axes[0].set_title(\"Imagem BGR (Visualização Incorreta)\")\n",
    "\n",
    "axes[1].imshow(rgb_image_loaded)\n",
    "axes[1].set_title(\"Imagem RGB (Após Conversão)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79656201",
   "metadata": {},
   "source": [
    "### Manipulação de Canais de Cor\n",
    "\n",
    "Como vimos, uma imagem RGB é um tensor de shape `(altura, largura, 3)`. Essa terceira dimensão contém os canais de cor Vermelho, Verde e Azul. Podemos pensar nela como três matrizes 2D (imagens em escala de cinza) empilhadas, onde cada matriz representa a intensidade de uma cor primária em cada pixel.\n",
    "\n",
    "Ao isolar esses canais, podemos analisá-los e modificá-los individualmente. Usando o fatiamento (slicing) do NumPy, podemos acessar cada canal facilmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed88bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separa os canais de cor da imagem RGB\n",
    "# Lembre-se: 0 = Red, 1 = Green, 2 = Blue\n",
    "red_channel = rgb_image_loaded[:, :, 0]\n",
    "green_channel = rgb_image_loaded[:, :, 1]\n",
    "blue_channel = rgb_image_loaded[:, :, 2]\n",
    "\n",
    "# Para visualizar cada canal, criamos uma imagem colorida onde os outros canais são zerados.\n",
    "# Isso mostra a contribuição de cada cor primária para a imagem final.\n",
    "zeros = np.zeros_like(red_channel) # Matriz de zeros com as mesmas dimensões de um canal\n",
    "\n",
    "# Cria uma imagem contendo apenas o canal vermelho\n",
    "image_only_red = np.stack([red_channel, zeros, zeros], axis=2)\n",
    "\n",
    "# Cria uma imagem contendo apenas o canal verde\n",
    "image_only_green = np.stack([zeros, green_channel, zeros], axis=2)\n",
    "\n",
    "# Cria uma imagem contendo apenas o canal azul\n",
    "image_only_blue = np.stack([zeros, zeros, blue_channel], axis=2)\n",
    "\n",
    "\n",
    "# Exibe os canais separados\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "axes[0].imshow(rgb_image_loaded)\n",
    "axes[0].set_title(\"Imagem Original RGB\")\n",
    "axes[1].imshow(image_only_red)\n",
    "axes[1].set_title(\"Canal Vermelho (Red)\")\n",
    "axes[2].imshow(image_only_green)\n",
    "axes[2].set_title(\"Canal Verde (Green)\")\n",
    "axes[3].imshow(image_only_blue)\n",
    "axes[3].set_title(\"Canal Azul (Blue)\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3746d5",
   "metadata": {},
   "source": [
    "### Modificando e Unindo Canais\n",
    "\n",
    "A verdadeira flexibilidade surge quando modificamos os canais de forma independente e depois os unimos novamente para criar uma nova imagem. Operações como alterar o brilho de um canal, aplicar filtros seletivamente ou adicionar elementos gráficos a apenas um canal são comuns em processamento de imagem.\n",
    "\n",
    "Após a manipulação, podemos usar a função `cv2.merge()` ou `np.stack()` para recombinar as matrizes 2D em um único tensor 3D, formando a imagem colorida final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a469ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos fazer uma cópia dos canais para não alterar os originais\n",
    "red_mod = red_channel.copy()\n",
    "green_mod = green_channel.copy()\n",
    "blue_mod = blue_channel.copy()\n",
    "\n",
    "# Modificação 1: Reduzir a intensidade do canal verde em 50%\n",
    "# Isso dará à imagem um tom mais roxo/magenta\n",
    "green_mod = (green_mod * 0.5).astype(np.uint8)\n",
    "\n",
    "# Modificação 2: Adicionar uma sobreposição (overlay) apenas no canal azul\n",
    "# Vamos adicionar um círculo branco no centro do canal azul.\n",
    "h, w = blue_mod.shape\n",
    "center_x, center_y = w // 2, h // 2\n",
    "radius = min(h, w) // 4\n",
    "cv2.circle(blue_mod, (center_x, center_y), radius, 255, -1) # 255=branco, -1=preenchido\n",
    "\n",
    "# Une os canais modificados para formar uma nova imagem\n",
    "# A ordem deve ser [R, G, B]\n",
    "merged_image = cv2.merge([red_mod, green_mod, blue_mod])\n",
    "\n",
    "\n",
    "# Exibe a imagem original e a modificada\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axes[0].imshow(rgb_image_loaded)\n",
    "axes[0].set_title(\"Imagem Original\")\n",
    "axes[1].imshow(merged_image)\n",
    "axes[1].set_title(\"Imagem com Canais Modificados\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e48306",
   "metadata": {},
   "source": [
    "## 2. A Operação de Convolução 2D\n",
    "\n",
    "A convolução é uma operação matemática fundamental em processamento de imagens e a base das Redes Neurais Convolucionais (CNNs). Ela consiste em aplicar um filtro (ou kernel), que é uma pequena matriz de pesos, sobre a imagem de entrada. O kernel desliza sobre a imagem, e para cada posição, é calculada a soma ponderada dos pixels da vizinhança, onde os pesos são os próprios valores do kernel.\n",
    "\n",
    "A convolução discreta 2D é definida como:\n",
    "\n",
    "$$(I * K)(i, j) = \\sum_{m} \\sum_{n} I(i-m, j-n) K(m, n)$$\n",
    "\n",
    "Onde $I$ é a imagem de entrada e $K$ é o kernel. O resultado é um novo mapa de características (*feature map*), que representa a ativação do filtro em cada posição da imagem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca462ad",
   "metadata": {},
   "source": [
    "### Implementando a Convolução\n",
    "\n",
    "Para entender o processo, vamos implementar uma função de convolução 2D. Nossa implementação utilizará um *padding* do tipo 'valid', o que significa que não adicionaremos preenchimento nas bordas da imagem. Como resultado, a imagem de saída será menor que a de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc98fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution2d(image, kernel):\n",
    "    # Dimensões da imagem e do kernel\n",
    "    image_height, image_width = image.shape\n",
    "    kernel_height, kernel_width = kernel.shape\n",
    "\n",
    "    # Dimensões da imagem de saída\n",
    "    output_height = image_height - kernel_height + 1\n",
    "    output_width = image_width - kernel_width + 1\n",
    "\n",
    "    # Inicializa a matriz de saída com zeros\n",
    "    output = np.zeros((output_height, output_width))\n",
    "\n",
    "    # Itera sobre cada pixel da imagem para aplicar o kernel\n",
    "    for y in range(output_height):\n",
    "        for x in range(output_width):\n",
    "            # Extrai a região da imagem correspondente ao kernel\n",
    "            region = image[y:y + kernel_height, x:x + kernel_width]\n",
    "            \n",
    "            # Aplica a operação de convolução (produto escalar elemento a elemento e soma)\n",
    "            output[y, x] = np.sum(region * kernel)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9deed66",
   "metadata": {},
   "source": [
    "## 3. Aplicação de Filtros (Kernels)\n",
    "\n",
    "Diferentes kernels podem ser usados para extrair diferentes tipos de características de uma imagem. Vamos explorar alguns dos filtros mais comuns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4769b5",
   "metadata": {},
   "source": [
    "### Filtro de Suavização (Blur)\n",
    "\n",
    "Um filtro de suavização, também conhecido como filtro de média ou *blur*, tem como objetivo principal atenuar ruídos e detalhes de alta frequência em uma imagem. A intuição é simples: cada pixel da imagem de saída é o resultado da média dos valores dos pixels em sua vizinhança na imagem de entrada. Esse processo \"espalha\" as intensidades dos pixels, resultando em transições mais suaves e uma aparência geral mais \"borrada\".\n",
    "\n",
    "O kernel para um *blur* 3x3 é uma matriz onde todos os elementos são iguais. Para preservar o brilho geral da imagem, a soma de todos os elementos do kernel deve ser 1. Portanto, para um kernel de dimensão $3 \\times 3$, cada elemento terá o valor de $1/9$.\n",
    "\n",
    "$$\n",
    "K_{blur} = \\frac{1}{9} \\begin{bmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "1 & 1 & 1 \\\\\n",
    "1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Ao aplicar este kernel, o valor do pixel central é efetivamente substituído pela média de si mesmo e de seus oito vizinhos imediatos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdca0492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel de Blur 3x3\n",
    "# Cada pixel será substituído pela média de seus vizinhos\n",
    "blur_kernel = np.array([\n",
    "    [1/9, 1/9, 1/9],\n",
    "    [1/9, 1/9, 1/9],\n",
    "    [1/9, 1/9, 1/9]\n",
    "])\n",
    "\n",
    "# Aplica o filtro de blur na imagem\n",
    "blurred_image = convolution2d(gray_image_loaded, blur_kernel)\n",
    "\n",
    "# Exibe os resultados\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axes[0].imshow(gray_image_loaded, cmap='gray')\n",
    "axes[0].set_title(\"Imagem Original\")\n",
    "axes[1].imshow(blurred_image, cmap='gray')\n",
    "axes[1].set_title(\"Imagem com Filtro de Blur\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de67b90",
   "metadata": {},
   "source": [
    "### Filtro de Realce (Sharpen)\n",
    "\n",
    "O filtro de *sharpening* (nitidez ou realce) opera de forma oposta ao de suavização. Seu objetivo é realçar as bordas e os detalhes finos, aumentando o contraste entre os pixels adjacentes. A ideia por trás deste filtro é acentuar a diferença entre um pixel e a média de sua vizinhança.\n",
    "\n",
    "Um kernel de *sharpen* comum consegue isso atribuindo um peso positivo alto ao pixel central e pesos negativos aos seus vizinhos.\n",
    "\n",
    "$$\n",
    "K_{sharpen} = \\begin{bmatrix}\n",
    "0 & -1 & 0 \\\\\n",
    "-1 & 5 & -1 \\\\\n",
    "0 & -1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Analisando este kernel:\n",
    "* O valor central (`5`) multiplica a intensidade do pixel original.\n",
    "* Os valores vizinhos (`-1`) subtraem a intensidade dos pixels ao redor.\n",
    "* Se uma região é \"plana\" (todos os pixels têm valores semelhantes), a soma ponderada resultará em um valor próximo ao original, pois a soma dos pesos do kernel é $5 - 1 - 1 - 1 - 1 = 1$. Isso garante que áreas sem bordas não sejam alteradas drasticamente.\n",
    "* Em uma borda, onde o valor do pixel central difere significativamente de seus vizinhos, a operação amplificará essa diferença, tornando a borda mais nítida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf71495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel de Sharpen\n",
    "sharpen_kernel = np.array([\n",
    "    [0, -1, 0],\n",
    "    [-1, 5, -1],\n",
    "    [0, -1, 0]\n",
    "])\n",
    "\n",
    "# Aplica o filtro de sharpen\n",
    "sharpened_image = convolution2d(gray_image_loaded, sharpen_kernel)\n",
    "\n",
    "# Exibe os resultados\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axes[0].imshow(gray_image_loaded, cmap='gray', vmin=0, vmax=255)\n",
    "axes[0].set_title(\"Imagem Original\")\n",
    "axes[1].imshow(sharpened_image, cmap='gray', vmin=0, vmax=255)\n",
    "axes[1].set_title(\"Imagem com Filtro Sharpen\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31525595",
   "metadata": {},
   "source": [
    "### Filtros de Detecção de Bordas (Sobel)\n",
    "\n",
    "A detecção de bordas é uma tarefa importante em visão computacional. O operador de Sobel é um dos algoritmos clássicos para isso. Ele utiliza dois kernels, um para detectar bordas horizontais ($K_x$) e outro para bordas verticais ($K_y$).\n",
    "\n",
    "$$\n",
    "K_x = \\begin{bmatrix}\n",
    "-1 & 0 & +1 \\\\\n",
    "-2 & 0 & +2 \\\\\n",
    "-1 & 0 & +1\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "K_y = \\begin{bmatrix}\n",
    "-1 & -2 & -1 \\\\\n",
    " 0 &  0 &  0 \\\\\n",
    "+1 & +2 & +1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Aplicamos ambos os kernels à imagem para obter os gradientes $G_x$ e $G_y$. A magnitude do gradiente, $G = \\sqrt{G_x^2 + G_y^2}$, nos dá a intensidade da borda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1a3f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernels de Sobel\n",
    "sobel_x_kernel = np.array([\n",
    "    [-1, 0, 1],\n",
    "    [-2, 0, 2],\n",
    "    [-1, 0, 1]\n",
    "])\n",
    "\n",
    "sobel_y_kernel = np.array([\n",
    "    [-1, -2, -1],\n",
    "    [0, 0, 0],\n",
    "    [1, 2, 1]\n",
    "])\n",
    "\n",
    "# Aplica os filtros para obter os gradientes\n",
    "gradient_x = convolution2d(gray_image_loaded, sobel_x_kernel)\n",
    "gradient_y = convolution2d(gray_image_loaded, sobel_y_kernel)\n",
    "\n",
    "# Exibe os gradientes horizontal e vertical\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axes[0].imshow(gradient_x, cmap='gray')\n",
    "axes[0].set_title(\"Gradiente Horizontal (Gx - Bordas Verticais)\")\n",
    "axes[1].imshow(gradient_y, cmap='gray')\n",
    "axes[1].set_title(\"Gradiente Vertical (Gy - Bordas Horizontais)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae300a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula a magnitude do gradiente\n",
    "# Para evitar que a imagem de saída seja menor, vamos aplicar o filtro em uma imagem já cortada\n",
    "h, w = gradient_x.shape\n",
    "gradient_magnitude = np.sqrt(gradient_x**2 + gradient_y**2)\n",
    "\n",
    "# Normaliza a imagem para o intervalo [0, 255] para melhor visualização\n",
    "gradient_magnitude = (gradient_magnitude / np.max(gradient_magnitude)) * 255\n",
    "\n",
    "# Exibe o resultado final\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axes[0].imshow(gray_image_loaded, cmap='gray')\n",
    "axes[0].set_title(\"Imagem Original\")\n",
    "axes[1].imshow(gradient_magnitude, cmap='gray')\n",
    "axes[1].set_title(\"Detecção de Bordas (Magnitude de Sobel)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c63ba0",
   "metadata": {},
   "source": [
    "### Visualizando a Direção dos Gradientes\n",
    "\n",
    "Os gradientes $G_x$ e $G_y$ não nos dão apenas a informação sobre a presença de uma borda, mas também a sua **orientação**. Em cada pixel, o par $(G_x, G_y)$ forma um vetor que aponta na direção da maior variação de intensidade (do escuro para o claro). A magnitude desse vetor, que calculamos em seguida, nos dá a \"força\" da borda, enquanto sua direção nos informa sobre a orientação da borda.\n",
    "\n",
    "Uma excelente forma de visualizar um campo de vetores é através de um *quiver plot*. Este tipo de gráfico desenha setas sobre uma grade para representar a direção e a magnitude dos vetores em cada ponto.\n",
    "\n",
    "No código a seguir, faremos o seguinte:\n",
    "1.  **Subamostragem (Subsampling)**: Desenhar uma seta para cada pixel da imagem resultaria em um gráfico excessivamente denso e ilegível. Por isso, selecionamos apenas um subconjunto de pixels (a cada `step` pixels) para visualizar os vetores de gradiente.\n",
    "2.  **Desenho das Setas**: Para cada ponto selecionado na grade, desenhamos uma seta (vetor) onde os componentes `(U, V)` correspondem aos valores de `(Gx, Gy)` naquele ponto. A direção da seta indica a direção do gradiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc989730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A imagem de gradiente (Gx, Gy) nos dá a magnitude e direção da mudança de intensidade.\n",
    "# Podemos visualizar isso usando um quiver plot.\n",
    "# Para evitar um plot muito denso, vamos amostrar os gradientes\n",
    "step = 5  # Amostra a cada 'step' pixels\n",
    "y_coords, x_coords = np.mgrid[0:h:step, 0:w:step]\n",
    "U = gradient_x[0:h:step, 0:w:step]\n",
    "V = gradient_y[0:h:step, 0:w:step]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.imshow(gray_image_loaded, cmap='gray') # Exibe a imagem subamostrada\n",
    "ax.quiver(x_coords, y_coords, U, V, color='red', scale=np.max(np.sqrt(U**2 + V**2)) * 20)\n",
    "ax.set_title(\"Direção dos Gradientes (Quiver Plot)\")\n",
    "ax.axis('off') # Remove os eixos para uma visualização mais limpa\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249567fe",
   "metadata": {},
   "source": [
    "### As Limitações dos Filtros Manuais\n",
    "\n",
    "O principal obstáculo dos filtros manuais é que eles são **fixos e específicos para uma tarefa de baixo nível**.\n",
    "\n",
    "1.  **Incapacidade de Capturar Complexidade**: Um filtro de Sobel é excelente para detectar bordas, mas como projetaríamos manualmente um filtro para detectar características mais complexas, como um \"olho de gato\", a \"textura de uma roda de carro\" ou o \"padrão de uma folha\"? A complexidade visual do mundo real torna impossível a criação manual de filtros para cada característica relevante.\n",
    "\n",
    "2.  **Falta de Generalização e Robustez**: Filtros manuais são frágeis. Um detector de bordas pode funcionar bem em uma imagem com iluminação e contraste ideais, mas falhar completamente se a iluminação mudar, o objeto rotacionar ou a perspectiva for diferente. Eles não se adaptam às infinitas variações presentes em dados do mundo real.\n",
    "\n",
    "3.  **Escalabilidade Inviável**: Para reconhecer um objeto complexo como um \"cachorro\", precisaríamos de uma combinação de centenas ou milhares de características: contornos, texturas de pelos, formas das orelhas, focinho, etc. Tentar orquestrar manualmente a aplicação e combinação de filtros para essa tarefa seria computacionalmente e logicamente inviável.\n",
    "\n",
    "Isso nos leva a uma questão fundamental: e se, em vez de nós projetarmos os filtros, pudéssemos criar um sistema que **aprende os valores ótimos dos filtros diretamente a partir dos dados**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f37b2b",
   "metadata": {},
   "source": [
    "## Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7c62c8",
   "metadata": {},
   "source": [
    "### Exercício 1\n",
    "\n",
    "Implemente o filtro *Bevel* manualmente e aplique-o em uma imagem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4ae961",
   "metadata": {},
   "source": [
    "### Exercício 2\n",
    "\n",
    "Faça upload de uma imagem RGB e aplique um filtro diferente para cada canal de cor. Em seguida, forma uma nova imagem colorida."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
