{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3af18e2a",
   "metadata": {},
   "source": [
    "# Redes Neurais Recorrentes\n",
    "\n",
    "As Redes Neurais Recorrentes (RNNs) representam uma classe de redes neurais artificiais especializadas no processamento de dados sequenciais $x^{(1)}, x^{(2)}, \\dots, x^{(T)}$. Diferentemente das redes *feedforward* tradicionais, as RNNs possuem conexões cíclicas que permitem a persistência de informações através de um estado oculto (*hidden state*), denotado por $h_t$. Este mecanismo confere à rede uma \"memória\" de curto prazo, essencial para tarefas onde o contexto temporal é crítico, como processamento de linguagem natural, análise de séries temporais e reconhecimento de fala.\n",
    "\n",
    "Neste notebook, exploraremos a implementação de uma RNN fundamental (Elman RNN) utilizando PyTorch. Abordaremos desde a geração de um conjunto de dados sintético para classificação de séries temporais, passando pela estruturação correta dos dados em tensores, até a definição da arquitetura e o ciclo de treinamento. O objetivo é classificar sequências temporais ruidosas baseando-se em suas frequências fundamentais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6448067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1313d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d86b74",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "926834b3",
   "metadata": {},
   "source": [
    "## Geração de Dados Sintéticos\n",
    "\n",
    "Para avaliar a capacidade da rede em discernir padrões temporais distintos, construiremos um dataset de classificação multiclasse. O problema consiste em categorizar séries temporais ruidosas com base em suas características espectrais predominantes.\n",
    "\n",
    "Definimos um conjunto de classes $C$, onde cada classe $c \\in \\{0, \\dots, K-1\\}$ está associada a uma frequência fundamental específica $\\omega_c$. O sinal observado $x_t$ para uma instância da classe $c$ é modelado como uma função senoidal corrompida por ruído gaussiano:\n",
    "\n",
    "$$x_t^{(c)} = \\sin(\\omega_c t) + \\epsilon_t, \\quad \\text{onde } \\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
    "\n",
    "Esta formulação exige que o modelo aprenda a mapear a sequência de observações ruidosas para a frequência subjacente correta, ignorando a variabilidade estocástica introduzida por $\\epsilon_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d281f3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(n_samples, seq_len, frequencies):\n",
    "    \"\"\"\n",
    "    Gera dataset sintético para classificação multiclasse de séries temporais.\n",
    "    \n",
    "    Args:\n",
    "        n_samples (int): Número total de amostras.\n",
    "        seq_len (int): Comprimento de cada série temporal.\n",
    "        frequencies (list): Lista de frequências, onde o índice é a classe.\n",
    "    \n",
    "    Returns:\n",
    "        data (np.array): Dados gerados (N, Seq_Len, 1).\n",
    "        labels (np.array): Rótulos das classes (N,).\n",
    "    \"\"\"\n",
    "    time_steps = np.linspace(0, 4 * np.pi, seq_len)\n",
    "    data = []\n",
    "    labels = []\n",
    "    n_classes = len(frequencies)\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Amostragem uniforme da classe\n",
    "        label = np.random.randint(0, n_classes)\n",
    "        \n",
    "        # Recuperação da frequência correspondente à classe sorteada\n",
    "        freq = frequencies[label]\n",
    "        \n",
    "        # Geração do sinal com ruído\n",
    "        noise = np.random.normal(0, 0.4, seq_len) # Aumentei levemente o ruído\n",
    "        sequence = np.sin(freq * time_steps) + noise\n",
    "        \n",
    "        data.append(sequence)\n",
    "        labels.append(label)\n",
    "        \n",
    "    data = np.array(data, dtype=np.float32)[..., np.newaxis]\n",
    "    labels = np.array(labels, dtype=np.longlong)\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28fdf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_freqs = [0.5, 1.5, 3.0, 5.0] # 4 Classes distintas\n",
    "raw_data, raw_labels = generate_synthetic_data(n_samples=3000, seq_len=60, frequencies=class_freqs)\n",
    "\n",
    "print(f\"Dataset gerado com {len(class_freqs)} classes. Shape: {raw_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b962c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_classes = np.unique(raw_labels)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for cls in unique_classes:\n",
    "    idx = np.where(raw_labels == cls)[0][0]\n",
    "    plt.plot(raw_data[idx], label=f\"Classe {cls} (Freq: {class_freqs[cls]})\")\n",
    "\n",
    "plt.title(f\"Amostras Representativas das {len(class_freqs)} Classes\")\n",
    "plt.xlabel(\"Tempo (steps)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ebaf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = torch.from_numpy(sequences)\n",
    "        self.labels = torch.from_numpy(labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dc3699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisão treino/teste\n",
    "train_size = int(0.8 * len(raw_data))\n",
    "train_data, test_data = raw_data[:train_size], raw_data[train_size:]\n",
    "train_labels, test_labels = raw_labels[:train_size], raw_labels[train_size:]\n",
    "\n",
    "# Instanciação dos Datasets\n",
    "train_dataset = TimeSeriesDataset(train_data, train_labels)\n",
    "test_dataset = TimeSeriesDataset(test_data, test_labels)\n",
    "\n",
    "# Configuração dos hiperparâmetros de carregamento\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Dimensão do batch de entrada: {next(iter(train_loader))[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c177a02",
   "metadata": {},
   "source": [
    "## Implementação da Rede Recorrente\n",
    "\n",
    "Formalmente, para cada passo de tempo $t$, a RNN computa um novo estado oculto $h_t$ baseado no estado anterior $h_{t-1}$ e na entrada atual $x_t$. A equação de recorrência para uma RNN de Elman com função de ativação tangente hiperbólica é dada por:\n",
    "\n",
    "$$h_t = \\tanh(W_{ih} x_t + b_{ih} + W_{hh} h_{t-1} + b_{hh})$$\n",
    "\n",
    "Onde:\n",
    "* $W_{ih}$ são os pesos da entrada para o estado oculto.\n",
    "* $W_{hh}$ são os pesos do estado oculto para o estado oculto (recorrência).\n",
    "* $b_{ih}, b_{hh}$ são os termos de viés."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbce91b",
   "metadata": {},
   "source": [
    "### Implementação de Baixo Nível: A Célula Recorrente\n",
    "\n",
    "Para compreender a mecânica interna de uma RNN, é instrutivo implementar manualmente a operação que ocorre em um único passo de tempo $t$. A \"célula\" de uma RNN é responsável por receber a entrada atual $x_t$ e o estado oculto anterior $h_{t-1}$, e computar o novo estado oculto $h_t$.\n",
    "\n",
    "Na implementação abaixo, utilizamos dois módulos `nn.Linear` distintos para representar as transformações lineares da entrada ($W_{ih}$) e do estado oculto ($W_{hh}$). A função de ativação tangente hiperbólica ($\\tanh$) é utilizada para manter os valores do estado oculto no intervalo $[-1, 1]$, mitigando problemas de explosão de gradiente em comparação com funções não limitadas como ReLU, embora não resolva o problema de desvanecimento de gradiente em sequências longas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db5b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Projeção linear da entrada x_t\n",
    "        self.input2hidden = nn.Linear(input_size, hidden_size)\n",
    "        # Projeção linear do estado anterior h_{t-1}\n",
    "        # O viés é opcional aqui pois já existe um em input2hidden\n",
    "        self.hidden2hidden = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x_t, h_prev):\n",
    "        # Cálculo do novo estado oculto\n",
    "        # h_t = tanh(W_xh * x_t + W_hh * h_prev + b)\n",
    "        h_t = self.activation(\n",
    "            self.input2hidden(x_t) + self.hidden2hidden(h_prev)\n",
    "        )\n",
    "        return h_t\n",
    "\n",
    "# Demonstração de um passo de tempo isolado\n",
    "batch_size = 5\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "\n",
    "cell = SimpleRNNCell(input_size, hidden_size)\n",
    "\n",
    "# Tensores para t=0\n",
    "x_t = torch.randn(batch_size, input_size)\n",
    "h_prev = torch.randn(batch_size, hidden_size)\n",
    "\n",
    "h_next = cell(x_t, h_prev)\n",
    "\n",
    "print(f\"Entrada (x_t): {x_t.shape}\")\n",
    "print(f\"Estado anterior (h_prev): {h_prev.shape}\")\n",
    "print(f\"Novo estado (h_next): {h_next.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315de0d6",
   "metadata": {},
   "source": [
    "### Processamento Sequencial e Unrolling\n",
    "\n",
    "Uma rede neural recorrente nada mais é do que a aplicação repetida da célula definida anteriormente sobre uma sequência de dados. Este processo é conhecido como *unrolling* (desenrolamento) da rede ao longo do tempo.\n",
    "\n",
    "A classe `SimpleRNN` abaixo encapsula este comportamento. Ela recebe um tensor de entrada de ordem 3 com dimensões $(N, L, H_{in})$, onde $N$ é o tamanho do lote e $L$ é o comprimento da sequência. O método `forward` itera explicitamente sobre a dimensão temporal, alimentando a saída de um passo como entrada do estado oculto do próximo.\n",
    "\n",
    "Observe a inicialização do estado oculto $h_0$. É convenção padrão iniciar $h_0$ como um tensor de zeros, o que implica que a rede não possui memória prévia antes do início da sequência, embora existam técnicas para aprender este estado inicial como um parâmetro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe668fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell = SimpleRNNCell(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, x_sequence):\n",
    "        # x_sequence shape: [batch_size, sequence_length, input_size]\n",
    "        batch_size = x_sequence.size(0)\n",
    "        seq_len = x_sequence.size(1)\n",
    "        \n",
    "        # Inicializa o estado oculto h_0 com zeros\n",
    "        # Deve estar no mesmo dispositivo (CPU/GPU) que a entrada\n",
    "        h_t = torch.zeros(batch_size, self.hidden_size).to(x_sequence.device)\n",
    "        \n",
    "        # Lista para armazenar o histórico dos estados ocultos\n",
    "        outputs = []\n",
    "        \n",
    "        # Loop temporal (Unrolling)\n",
    "        for t in range(seq_len):\n",
    "            # Seleciona a fatia de tempo t: (batch_size, input_size)\n",
    "            x_t = x_sequence[:, t, :] \n",
    "            \n",
    "            # Atualiza o estado oculto usando a célula\n",
    "            h_t = self.cell(x_t, h_t)\n",
    "            \n",
    "            # Armazena o estado atual\n",
    "            outputs.append(h_t)\n",
    "            \n",
    "        # Empilha a lista de tensores na dimensão temporal (dim=1)\n",
    "        # Resultado final: [batch_size, sequence_length, hidden_size]\n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "# Demonstração do processamento da sequência completa\n",
    "seq_length = 7\n",
    "rnn_from_scratch = SimpleRNN(input_size, hidden_size)\n",
    "\n",
    "# Tensor simulando uma sequência temporal completa\n",
    "x_sequence = torch.randn(batch_size, seq_length, input_size)\n",
    "\n",
    "output_sequence = rnn_from_scratch(x_sequence)\n",
    "\n",
    "print(f\"Sequência de Entrada: {x_sequence.shape}\")\n",
    "print(f\"Sequência de Saída (Estados Ocultos): {output_sequence.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44cadeb",
   "metadata": {},
   "source": [
    "### O Módulo nn.RNN do PyTorch\n",
    "\n",
    "Em ambientes de produção e pesquisa utilizamos a classe `torch.nn.RNN`. Esta implementação é altamente otimizada, suportando aceleração via cuDNN (CUDA Deep Neural Network library) e execução paralela onde possível.\n",
    "\n",
    "Ao instanciar `nn.RNN`, os principais hiperparâmetros são:\n",
    "* `input_size`: O número de features esperadas na entrada $x_t$.\n",
    "* `hidden_size`: O número de features no estado oculto $h_t$.\n",
    "* `num_layers`: Número de camadas recorrentes empilhadas (stacked RNN).\n",
    "* `batch_first`: Se `True`, a entrada e saída são fornecidas como $(Batch, Seq, Feature)$. O padrão do PyTorch é `False` $(Seq, Batch, Feature)$.\n",
    "* `nonlinearity`: Pode ser `'tanh'` (padrão) ou `'relu'`.\n",
    "\n",
    "A chamada `forward` deste módulo espera uma entrada $X$ e um estado oculto inicial $h_0$ (opcional, assume-se zero se omitido).\n",
    "\n",
    "Para a tarefa de classificação, utilizaremos o vetor do estado oculto do último passo de tempo, $h_T$, como uma representação comprimida de toda a sequência. Este vetor será alimentado em uma camada totalmente conectada (linear) para projetar as pontuações das classes (logits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67470a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_layers=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Camada Recorrente\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Camada Linear para classificação baseada no último estado oculto\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Inicialização do estado oculto (h0)\n",
    "        # Dimensões: (num_layers, batch_size, hidden_size)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward pass na RNN\n",
    "        # out: contém os estados ocultos de todos os timesteps\n",
    "        # _ : contém o estado oculto do último timestep (h_n)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        \n",
    "        # Selecionamos o output do último passo de tempo (many-to-one architecture)\n",
    "        # out[:, -1, :] pega todas as amostras do batch, último tempo, todas as features ocultas\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Decodificação para classes\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130d5254",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1                       # Univariado\n",
    "hidden_dim = 32                     # Capacidade de memória da RNN\n",
    "output_dim = len(class_freqs)       # Número de classes\n",
    "n_layers = 1                        # Profundidade da recorrência\n",
    "\n",
    "model = RNNClassifier(input_dim, hidden_dim, output_dim, n_layers).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4373eaef",
   "metadata": {},
   "source": [
    "## Treinamento\n",
    "\n",
    "A otimização dos parâmetros $\\theta$ da rede é realizada através da minimização da função de custo Cross-Entropy, adequada para problemas de classificação multiclasse (ou binária quando não se usa sigmoide na saída final).\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = -\\frac{1}{N} \\sum_{i=1}^{N} \\log \\left( \\frac{e^{s_{y_i}}}{\\sum_{j} e^{s_j}} \\right)$$\n",
    "\n",
    "Onde $s$ são os *logits* produzidos pela rede e $y_i$ é a classe verdadeira. Utilizaremos o otimizador Adam, que adapta a taxa de aprendizado para cada parâmetro individualmente baseando-se nos momentos de primeira e segunda ordem dos gradientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5bae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a6aad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for sequences, labels in train_loader:\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward e Otimização\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Métricas\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b90f5e",
   "metadata": {},
   "source": [
    "## Inferência e Análise de Resultados\n",
    "\n",
    "Após a convergência do modelo, é crucial avaliar qualitativamente o desempenho da rede em dados não vistos. A visualização abaixo apresenta amostras do conjunto de teste juntamente com a predição do modelo e a probabilidade associada (obtida via função Softmax). Isso permite inspecionar visualmente se a rede está capturando corretamente as características de frequência que definem as classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038a92c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Pegar um batch do conjunto de teste\n",
    "    inputs, labels = next(iter(test_loader))\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    # Inferência\n",
    "    outputs = model(inputs)\n",
    "    probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    \n",
    "    # Plotar resultados\n",
    "    inputs = inputs.cpu().numpy()\n",
    "    preds = preds.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    probs = probs.cpu().numpy()\n",
    "    \n",
    "    fig, axes = plt.subplots(4, 4, figsize=(12, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(16):\n",
    "        ax = axes[i]\n",
    "        ax.plot(inputs[i], color='blue' if preds[i] == labels[i] else 'red')\n",
    "        ax.set_title(f\"Real: {labels[i]} | Pred: {preds[i]}\\nConfiança: {probs[i][preds[i]]:.2f}\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
