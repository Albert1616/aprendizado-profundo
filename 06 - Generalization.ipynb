{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Generalização em Machine Learning\n",
    "\n",
    "Neste notebook, vamos explorar um dos conceitos mais importantes em machine learning: **generalização**. Veremos como os modelos se comportam quando expostos a dados que não viram durante o treinamento e como evitar problemas como overfitting e underfitting.\n",
    "\n",
    "## Objetivos de Aprendizagem\n",
    "\n",
    "Ao final deste notebook, você será capaz de:\n",
    "- Compreender os conceitos de overfitting, underfitting e bias-variance tradeoff\n",
    "- Implementar modelos polinomiais usando PyTorch para demonstrar estes conceitos\n",
    "- Analisar curvas de treinamento para identificar problemas de generalização\n",
    "- Aplicar técnicas de validação em datasets reais (Fashion-MNIST)\n",
    "\n",
    "## Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuração para gráficos\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Configurar dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Dispositivo utilizado: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Conceitos Fundamentais de Generalização\n",
    "\n",
    "### 1.1 Definições\n",
    "\n",
    "**Generalização** é a capacidade de um modelo de fazer predições precisas em dados que não foram vistos durante o treinamento.\n",
    "\n",
    "**Overfitting (Sobreajuste)**: O modelo aprende os dados de treinamento muito bem, incluindo ruído e detalhes específicos, mas falha em generalizar para novos dados.\n",
    "\n",
    "**Underfitting (Subajuste)**: O modelo é muito simples para capturar os padrões subjacentes nos dados, resultando em performance ruim tanto no treinamento quanto na validação.\n",
    "\n",
    "### 1.2 Bias-Variance Tradeoff\n",
    "\n",
    "O erro total de um modelo pode ser decomposto em três componentes:\n",
    "\n",
    "$$\\text{Erro Total} = \\text{Bias}^2 + \\text{Variância} + \\text{Ruído}$$\n",
    "\n",
    "- **Bias**: Erro devido a suposições simplificadas no algoritmo de aprendizagem\n",
    "- **Variância**: Erro devido à sensibilidade a pequenas flutuações no conjunto de treinamento\n",
    "- **Ruído**: Erro irredutível nos dados\n",
    "\n",
    "![Bias-Variance Tradeoff](https://upload.wikimedia.org/wikipedia/commons/9/9f/Bias_and_variance_contributing_to_total_error.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Demonstração com Regressão Polinomial\n",
    "\n",
    "Vamos criar um exemplo prático usando regressão polinomial para demonstrar overfitting e underfitting.\n",
    "\n",
    "### 2.1 Classe Polinomial Customizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialRegressor(nn.Module):\n",
    "    \"\"\"Modelo de regressão polinomial usando PyTorch.\n",
    "    \n",
    "    Implementa um polinômio de grau n:\n",
    "    y = a_0 + a_1*x + a_2*x^2 + ... + a_n*x^n\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, degree):\n",
    "        super(PolynomialRegressor, self).__init__()\n",
    "        self.degree = degree\n",
    "        # Coeficientes do polinômio (parâmetros aprendíveis)\n",
    "        self.coefficients = nn.Parameter(torch.randn(degree + 1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass: calcula o polinômio para os valores de entrada.\"\"\"\n",
    "        # Criar matriz de potências [1, x, x^2, x^3, ..., x^n]\n",
    "        powers = torch.stack([x.squeeze()**i for i in range(self.degree + 1)], dim=1)\n",
    "        # Calcular y = sum(a_i * x^i)\n",
    "        return torch.sum(powers * self.coefficients, dim=1, keepdim=True)\n",
    "    \n",
    "    def get_coefficients(self):\n",
    "        \"\"\"Retorna os coeficientes aprendidos.\"\"\"\n",
    "        return self.coefficients.detach().numpy()\n",
    "\n",
    "# Testar a classe\n",
    "model = PolynomialRegressor(degree=2)\n",
    "x_test = torch.linspace(-2, 2, 10).unsqueeze(1)\n",
    "y_test = model(x_test)\n",
    "print(f'Forma da entrada: {x_test.shape}')\n",
    "print(f'Forma da saída: {y_test.shape}')\n",
    "print(f'Coeficientes iniciais: {model.get_coefficients()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Geração de Dados Sintéticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(true_func, n_samples=100, noise_std=0.3, x_range=(-2, 2), seed=42, offset=0.0):\n",
    "    \"\"\"Gera dados com base em uma função verdadeira e adiciona ruído.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    x = np.random.uniform(x_range[0]-offset, x_range[1]+offset, n_samples)\n",
    "    y_true = true_func(x)\n",
    "    noise = np.random.normal(0, noise_std, n_samples)\n",
    "    y_noisy = y_true + noise\n",
    "    \n",
    "    return x, y_true, y_noisy\n",
    "\n",
    "# Definir função verdadeira (parábola)\n",
    "def true_polynomial(x):\n",
    "    return 1.5 - 2.0 * x + 0.5 * x**2 + 3.5 * x**3 \n",
    "\n",
    "x_range = (-1, 1)\n",
    "noise_std = 0.2\n",
    "test_offset = 0.5\n",
    "\n",
    "# Gerar dados\n",
    "x_train, y_train_true, y_train_noisy = generate_data(true_polynomial, n_samples=10, noise_std=noise_std, x_range=x_range)\n",
    "x_test, y_test_true, y_test_noisy = generate_data(true_polynomial, n_samples=10, noise_std=noise_std, x_range=x_range, offset=test_offset)\n",
    "\n",
    "# Converter para tensores\n",
    "x_train_tensor = torch.FloatTensor(x_train).unsqueeze(1)\n",
    "y_train_tensor = torch.FloatTensor(y_train_noisy).unsqueeze(1)\n",
    "x_test_tensor = torch.FloatTensor(x_test).unsqueeze(1)\n",
    "y_test_tensor = torch.FloatTensor(y_test_noisy).unsqueeze(1)\n",
    "\n",
    "# Visualizar os dados\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "x_smooth = np.linspace(x_range[0], x_range[1], 100)\n",
    "y_smooth = true_polynomial(x_smooth)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x_train, y_train_noisy, alpha=0.6, label='Dados de Treinamento (com ruído)')\n",
    "plt.plot(x_smooth, y_smooth, 'r-', label='Função Verdadeira', linewidth=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Dados de Treinamento')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(x_test, y_test_noisy, alpha=0.6, color='orange', label='Dados de Teste (com ruído)')\n",
    "plt.plot(x_smooth, y_smooth, 'r-', label='Função Verdadeira', linewidth=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Dados de Teste')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Tamanho do conjunto de treinamento: {len(x_train)}')\n",
    "print(f'Tamanho do conjunto de teste: {len(x_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Função de Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_polynomial_model(degree, epochs=1000, lr=0.01, verbose=False):\n",
    "    \"\"\"Treina um modelo polinomial e retorna histórico de losses.\"\"\"\n",
    "    model = PolynomialRegressor(degree)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Treinamento\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        train_pred = model(x_train_tensor)\n",
    "        train_loss = criterion(train_pred, y_train_tensor)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Avaliação\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_pred = model(x_test_tensor)\n",
    "            test_loss = criterion(test_pred, y_test_tensor)\n",
    "        \n",
    "        train_losses.append(train_loss.item())\n",
    "        test_losses.append(test_loss.item())\n",
    "        \n",
    "        if verbose and (epoch + 1) % 200 == 0:\n",
    "            print(f'Época {epoch+1}/{epochs}, '\n",
    "                  f'Loss Treino: {train_loss:.4f}, '\n",
    "                  f'Loss Teste: {test_loss:.4f}')\n",
    "    \n",
    "    return model, train_losses, test_losses\n",
    "\n",
    "# Testar a função de treinamento\n",
    "model_test, train_losses_test, test_losses_test = train_polynomial_model(degree=3, epochs=1000, lr=0.1, verbose=True)\n",
    "print(f'\\nCoeficientes finais: {model_test.get_coefficients()}')\n",
    "print(f'Coeficientes verdadeiros: [1.5, -2.0, 0.5]')\n",
    "\n",
    "# Visualizar ajuste\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_test, y_test_noisy, alpha=0.6, label='Dados de Treinamento')\n",
    "x_plot = torch.linspace(x_range[0]-test_offset, x_range[1]+test_offset, 100).unsqueeze(1)\n",
    "with torch.no_grad():\n",
    "    y_plot = model_test(x_plot)\n",
    "plt.plot(x_plot.squeeze(), y_plot.squeeze(), 'g--', linewidth=2, label='Modelo Ajustado')\n",
    "plt.plot(x_plot.squeeze(), true_polynomial(x_plot.squeeze()), 'r-', linewidth=2, label='Função Verdadeira')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Teste do Ajuste Polinomial')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Comparação de Diferentes Graus de Complexidade\n",
    "\n",
    "Agora vamos treinar modelos com diferentes graus polinomiais para demonstrar underfitting, bom ajuste e overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar modelos com diferentes graus\n",
    "degrees = [1, 2, 3, 5, 8]\n",
    "models = {}\n",
    "histories = {}\n",
    "\n",
    "print(\"Treinando modelos com diferentes graus...\")\n",
    "for degree in tqdm(degrees):\n",
    "    model, train_losses, test_losses = train_polynomial_model(degree, epochs=5000, lr=0.01)\n",
    "    models[degree] = model\n",
    "    histories[degree] = {'train': train_losses, 'test': test_losses}\n",
    "\n",
    "print(\"Treinamento concluído!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = []\n",
    "for degree in degrees:\n",
    "    train_mse = histories[degree]['train'][-1]\n",
    "    test_mse = histories[degree]['test'][-1]\n",
    "    gap = test_mse - train_mse\n",
    "    \n",
    "    results.append({\n",
    "        'Grau': degree,\n",
    "        'MSE Treino': f'{train_mse:.4f}',\n",
    "        'MSE Teste': f'{test_mse:.4f}',\n",
    "        'Gap': f'{gap:.4f}'\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"=== ANÁLISE COMPARATIVA DOS MODELOS ===\")\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "# Visualizar coeficientes aprendidos\n",
    "print(\"\\n=== COEFICIENTES APRENDIDOS ===\")\n",
    "for degree in degrees:\n",
    "    coeffs = models[degree].get_coefficients()\n",
    "    print(f\"Grau {degree}: {coeffs[:min(len(coeffs), 6)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar curvas de loss\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    \n",
    "    train_losses = histories[degree]['train']\n",
    "    test_losses = histories[degree]['test']\n",
    "    \n",
    "    plt.plot(train_losses, label='Treinamento', linewidth=2)\n",
    "    plt.plot(test_losses, label='Teste', linewidth=2)\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.title(f'Grau {degree}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ylim(-1, 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar as predições dos modelos\n",
    "x_plot = torch.linspace(-2.5, 2.5, 200).unsqueeze(1)\n",
    "y_true_plot = true_polynomial(x_plot)\n",
    "\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    \n",
    "    # Dados de treinamento\n",
    "    plt.scatter(x_train, y_train_noisy, alpha=0.6, color='blue', s=30, label='Dados Treino')\n",
    "    plt.scatter(x_test, y_test_noisy, alpha=0.6, color='orange', s=30, label='Dados Teste')\n",
    "    \n",
    "    # Função verdadeira\n",
    "    plt.plot(x_plot.squeeze(), y_true_plot, 'r-', linewidth=2, label='Função Verdadeira')\n",
    "    \n",
    "    # Predição do modelo\n",
    "    model = models[degree]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_plot = model(x_plot)\n",
    "    \n",
    "    plt.plot(x_plot.squeeze(), y_pred_plot.squeeze(), 'g--', linewidth=3, \n",
    "             label=f'Modelo Grau {degree}', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(f'Polinômio Grau {degree}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ylim(-2, 6)\n",
    "    \n",
    "    # Calcular erros finais\n",
    "    train_mse = histories[degree]['train'][-1]\n",
    "    test_mse = histories[degree]['test'][-1]\n",
    "    \n",
    "    plt.text(0.05, 0.95, f'MSE Treino: {train_mse:.3f}\\nMSE Teste: {test_mse:.3f}', \n",
    "             transform=plt.gca().transAxes, \n",
    "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "             verticalalignment='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bias-Variance Tradeoff\n",
    "\n",
    "### 3.1 Fundamentos Matemáticos\n",
    "\n",
    "O **Bias-Variance Tradeoff** é um conceito fundamental para entender por que modelos generalizam bem ou mal. Matematicamente, podemos decompor o erro esperado de um modelo:\n",
    "\n",
    "$$E[(y - \\hat{f}(x))^2] = \\text{Bias}^2[\\hat{f}(x)] + \\text{Var}[\\hat{f}(x)] + \\sigma^2$$\n",
    "\n",
    "Onde:\n",
    "- **$y$**: valor verdadeiro\n",
    "- **$\\hat{f}(x)$**: predição do modelo\n",
    "- **$\\sigma^2$**: ruído irredutível nos dados\n",
    "\n",
    "#### Definições Matemáticas:\n",
    "\n",
    "1. **Bias (Viés)**: Mede o quanto a predição média do modelo se afasta do valor verdadeiro\n",
    "   $$\\text{Bias}[\\hat{f}(x)] = E[\\hat{f}(x)] - f(x)$$\n",
    "\n",
    "2. **Variância**: Mede o quanto as predições variam entre diferentes conjuntos de treinamento\n",
    "   $$\\text{Var}[\\hat{f}(x)] = E[(\\hat{f}(x) - E[\\hat{f}(x)])^2]$$\n",
    "\n",
    "#### Interpretação Prática:\n",
    "\n",
    "- **Alto Bias, Baixa Variância**: Modelo consistente, mas sistematicamente errado (underfitting)\n",
    "- **Baixo Bias, Alta Variância**: Modelo correto em média, mas muito sensível aos dados (overfitting)\n",
    "- **Objetivo**: Encontrar o equilíbrio que minimiza o erro total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Análise de Overfitting com MNIST\n",
    "\n",
    "### 4.1 Efeito do Tamanho do Dataset de Treinamento\n",
    "\n",
    "Vamos investigar como o tamanho do conjunto de treinamento afeta o overfitting usando o dataset MNIST. Iremos treinar redes neurais com diferentes quantidades de dados de treinamento e observar como isso impacta a generalização.\n",
    "\n",
    "#### Hipóteses:\n",
    "- **Poucos dados**: Alto overfitting (grande gap entre treino e validação)\n",
    "- **Mais dados**: Menor overfitting e melhor generalização\n",
    "- **Muitos dados**: Convergência para o limite teórico de performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o dataset MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Normalização específica do MNIST\n",
    "])\n",
    "\n",
    "# Carregar datasets completos\n",
    "train_dataset_full = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "# Criar dataloader para teste (fixo para todas as experiências)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "print(f'Dataset MNIST carregado!')\n",
    "print(f'Tamanho do conjunto de treinamento completo: {len(train_dataset_full)}')\n",
    "print(f'Tamanho do conjunto de teste: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Arquitetura da Rede Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier(nn.Module):\n",
    "    \"\"\"Rede neural simples para classificação MNIST.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size=128):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28 * 28, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Testar a arquitetura\n",
    "model_test = MNISTClassifier(hidden_size=128)\n",
    "print(f'Arquitetura do modelo:')\n",
    "print(model_test)\n",
    "\n",
    "# Contar parâmetros\n",
    "total_params = sum(p.numel() for p in model_test.parameters())\n",
    "trainable_params = sum(p.numel() for p in model_test.parameters() if p.requires_grad)\n",
    "print(f'\\nTotal de parâmetros: {total_params:,}')\n",
    "print(f'Parâmetros treináveis: {trainable_params:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Função de Treinamento e Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist_model(train_loader, val_loader, epochs=20, lr=0.001, hidden_size=128):\n",
    "    \"\"\"Treina modelo MNIST e retorna histórico de métricas.\"\"\"\n",
    "    \n",
    "    model = MNISTClassifier(hidden_size=hidden_size).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Treinamento\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_total += target.size(0)\n",
    "            train_correct += (predicted == target).sum().item()\n",
    "        \n",
    "        # Avaliação no conjunto de validação\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                val_total += target.size(0)\n",
    "                val_correct += (predicted == target).sum().item()\n",
    "        \n",
    "        # Calcular métricas da época\n",
    "        train_loss_avg = train_loss / len(train_loader)\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        val_loss_avg = val_loss / len(val_loader)\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        train_losses.append(train_loss_avg)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss_avg)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'Época {epoch+1}/{epochs}: '\n",
    "                  f'Train Loss: {train_loss_avg:.4f}, Train Acc: {train_acc:.2f}% | '\n",
    "                  f'Val Loss: {val_loss_avg:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "    \n",
    "    return model, {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accuracies': val_accuracies\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Experimento: Variando o Tamanho do Dataset de Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir diferentes tamanhos de dataset para o experimento\n",
    "dataset_sizes = [100, 500, 10000]  # Poucos, médio e muitos dados\n",
    "batch_size = 64\n",
    "val_size = 1000  # Conjunto de validação fixo para comparação justa\n",
    "\n",
    "# Separar dados de validação (primeiros 5000 exemplos do conjunto de teste)\n",
    "val_indices = list(range(val_size))\n",
    "val_dataset = torch.utils.data.Subset(train_dataset_full, val_indices)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "epochs = 20\n",
    "\n",
    "# Inicializar dicionários para armazenar resultados\n",
    "results_by_size = {}\n",
    "models_by_size = {}\n",
    "\n",
    "print(\"=== EXPERIMENTO: EFEITO DO TAMANHO DO DATASET ===\")\n",
    "print(f\"Conjunto de validação fixo: {val_size} exemplos\")\n",
    "print(f\"Tamanhos de treinamento a testar: {dataset_sizes}\")\n",
    "print(f\"Épocas por experimento: {epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar experimentos para cada tamanho de dataset\n",
    "for size in dataset_sizes:\n",
    "    print(f\"Treinando com {size} exemplos...\")\n",
    "    \n",
    "    # Criar subset do dataset de treinamento\n",
    "    # Usar índices após os de validação para evitar overlap\n",
    "    train_indices = list(range(val_size, val_size + size))\n",
    "    train_subset = torch.utils.data.Subset(train_dataset_full, train_indices)\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Treinar modelo\n",
    "    model, history = train_mnist_model(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        epochs=epochs,\n",
    "        lr=0.001,\n",
    "        hidden_size=128\n",
    "    )\n",
    "    \n",
    "    # Armazenar resultados\n",
    "    results_by_size[size] = history\n",
    "    models_by_size[size] = model\n",
    "    \n",
    "    print(f\"Concluído para {size} exemplos\")\n",
    "\n",
    "print(\"Todos os experimentos concluídos!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar curvas de treinamento\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "colors = ['r', 'b', 'g']\n",
    "\n",
    "for i, size in enumerate(dataset_sizes):\n",
    "    history = results_by_size[size]\n",
    "    color = colors[i]\n",
    "    \n",
    "    # Curvas de loss\n",
    "    ax_loss = axes[0, i]\n",
    "    epochs_range = range(1, len(history['train_losses']) + 1)\n",
    "    ax_loss.plot(epochs_range, history['train_losses'], f'{color}-', linewidth=2, label='Treinamento')\n",
    "    ax_loss.plot(epochs_range, history['val_losses'], f'{color}--', linewidth=2, label='Validação')\n",
    "    ax_loss.set_title(f'Loss - Dataset: {size} exemplos')\n",
    "    ax_loss.set_xlabel('Épocas')\n",
    "    ax_loss.set_ylabel('CrossEntropy Loss')\n",
    "    ax_loss.legend()\n",
    "    ax_loss.grid(True, alpha=0.3)\n",
    "    ax_loss.set_ylim([0, 2])\n",
    "    \n",
    "    # Curvas de acurácia\n",
    "    ax_acc = axes[1, i]\n",
    "    ax_acc.plot(epochs_range, history['train_accuracies'], f'{color}-', linewidth=2, label='Treinamento')\n",
    "    ax_acc.plot(epochs_range, history['val_accuracies'], f'{color}--', linewidth=2, label='Validação')\n",
    "    ax_acc.set_title(f'Acurácia - Dataset: {size} exemplos')\n",
    "    ax_acc.set_xlabel('Épocas')\n",
    "    ax_acc.set_ylabel('Acurácia (%)')\n",
    "    ax_acc.legend()\n",
    "    ax_acc.grid(True, alpha=0.3)\n",
    "    ax_acc.set_ylim([0, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Análise dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisar resultados finais\n",
    "print(\"=== ANÁLISE DE OVERFITTING POR TAMANHO DO DATASET ===\\\\n\")\n",
    "\n",
    "analysis_results = []\n",
    "for size in dataset_sizes:\n",
    "    history = results_by_size[size]\n",
    "    \n",
    "    # Métricas finais (última época)\n",
    "    final_train_loss = history['train_losses'][-1]\n",
    "    final_val_loss = history['val_losses'][-1]\n",
    "    final_train_acc = history['train_accuracies'][-1]\n",
    "    final_val_acc = history['val_accuracies'][-1]\n",
    "    \n",
    "    # Gap entre treino e validação (indicador de overfitting)\n",
    "    loss_gap = final_val_loss - final_train_loss\n",
    "    acc_gap = final_train_acc - final_val_acc\n",
    "    \n",
    "    analysis_results.append({\n",
    "        'Dataset Size': size,\n",
    "        'Train Loss': f'{final_train_loss:.4f}',\n",
    "        'Val Loss': f'{final_val_loss:.4f}',\n",
    "        'Loss Gap': f'{loss_gap:.4f}',\n",
    "        'Train Acc': f'{final_train_acc:.2f}%',\n",
    "        'Val Acc': f'{final_val_acc:.2f}%',\n",
    "        'Acc Gap': f'{acc_gap:.2f}%'\n",
    "    })\n",
    "    \n",
    "    print(f\"📈 Dataset com {size} exemplos:\")\n",
    "    print(f\"   Loss: Treino {final_train_loss:.4f} | Validação {final_val_loss:.4f} | Gap {loss_gap:.4f}\")\n",
    "    print(f\"   Acc:  Treino {final_train_acc:.2f}% | Validação {final_val_acc:.2f}% | Gap {acc_gap:.2f}%\")\n",
    "    print()\n",
    "\n",
    "# Criar tabela resumo\n",
    "df_analysis = pd.DataFrame(analysis_results)\n",
    "print(\"📋 TABELA RESUMO:\")\n",
    "print(df_analysis.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Visualizações Comparativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de barras comparativo do gap de overfitting\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Extrair dados para os gráficos\n",
    "dataset_sizes_str = [str(size) for size in dataset_sizes]\n",
    "loss_gaps = []\n",
    "acc_gaps = []\n",
    "\n",
    "for size in dataset_sizes:\n",
    "    history = results_by_size[size]\n",
    "    loss_gap = history['val_losses'][-1] - history['train_losses'][-1]\n",
    "    acc_gap = history['train_accuracies'][-1] - history['val_accuracies'][-1]\n",
    "    loss_gaps.append(loss_gap)\n",
    "    acc_gaps.append(acc_gap)\n",
    "\n",
    "# Gráfico de gap de loss\n",
    "bars1 = ax1.bar(dataset_sizes_str, loss_gaps, color=['lightcoral', 'lightblue', 'lightgreen'], \n",
    "                edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "ax1.set_title('Gap de Loss (Val - Train)\\n(Maior = Mais Overfitting)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Tamanho do Dataset de Treinamento')\n",
    "ax1.set_ylabel('Gap de Loss')\n",
    "ax1.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for bar, value in zip(bars1, loss_gaps):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Gráfico de gap de acurácia\n",
    "bars2 = ax2.bar(dataset_sizes_str, acc_gaps, color=['lightcoral', 'lightblue', 'lightgreen'], \n",
    "                edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "ax2.set_title('Gap de Acurácia (Train - Val)\\n(Maior = Mais Overfitting)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Tamanho do Dataset de Treinamento')\n",
    "ax2.set_ylabel('Gap de Acurácia (%)')\n",
    "ax2.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for bar, value in zip(bars2, acc_gaps):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.2,\n",
    "             f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Conclusões do Experimento\n",
    "\n",
    "#### Implicações Práticas:\n",
    "\n",
    "- **Mais dados = Melhor generalização**: O aumento do tamanho do dataset reduziu consistentemente o overfitting\n",
    "- **Bias-Variance Tradeoff**: Datasets maiores reduzem a variância do modelo\n",
    "- **Capacidade do Modelo**: A mesma arquitetura se beneficia diferentemente dependendo da quantidade de dados\n",
    "\n",
    "#### Insights Teóricos:\n",
    "\n",
    "- O **ruído nos dados** tem menos impacto quando há mais exemplos diversos\n",
    "- A **complexidade do modelo** deve ser ajustada de acordo com a quantidade de dados disponíveis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
