{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2c351e8",
   "metadata": {},
   "source": [
    "# Classificação de Texto com LSTM\n",
    "\n",
    "Neste notebook, exploraremos a aplicação de Redes Neurais Recorrentes (RNNs) do tipo Long Short-Term Memory (LSTM) para a classificação de textos em 10 categorias distintas. Utilizaremos o dataset *20 Newsgroups*, um padrão para benchmarks em Processamento de Linguagem Natural (PLN). O foco principal será a construção de um pipeline de processamento que transforma texto bruto em representações vetoriais densas (embeddings), que são então processadas sequencialmente pela LSTM para capturar dependências temporais e semânticas, superando limitações de modelos tradicionais baseados em frequência de palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb2295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1271b424",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Dispositivo selecionado: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9e9882",
   "metadata": {},
   "source": [
    "## Preparação dos Dados\n",
    "\n",
    "Para o processamento de texto em redes neurais profundas, devemos converter as strings em tensores numéricos. Diferente de abordagens complexas de tokenização como BPE ou WordPiece, utilizaremos uma abordagem determinística baseada na separação por espaços e conversão para minúsculas. Isso simplifica o vocabulário, embora possa gerar um número maior de tokens desconhecidos (`<unk>`).\n",
    "\n",
    "Selecionaremos um subconjunto de 10 categorias para o treinamento. O processo envolve a criação de um vocabulário que mapeia cada palavra única para um índice inteiro. Palavras raras ou fora do vocabulário de treino serão mapeadas para um token especial `<unk>`, e as sequências serão preenchidas com `<pad>` para garantir uniformidade dimensional nos lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845bf139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorias selecionadas\n",
    "categories = [\n",
    "    'sci.med', 'talk.politics.mideast', 'comp.graphics'\n",
    "]\n",
    "\n",
    "# Carregamento do dataset original\n",
    "train_source = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "test_source = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "train_data = train_source.data\n",
    "train_labels = train_source.target\n",
    "test_data = test_source.data\n",
    "test_labels = test_source.target\n",
    "\n",
    "print(f\"Amostras de treino: {len(train_data)}\")\n",
    "print(f\"Amostras de teste: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08680db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_examples(data, labels, target_names, n=3, max_chars=200):\n",
    "    classes = np.unique(labels)\n",
    "    for c in classes:\n",
    "        print(\"=\"*80)\n",
    "        print(f\"CLASSE: {target_names[c]}\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        idxs = np.where(labels == c)[0]\n",
    "        chosen = np.random.choice(idxs, size=min(n, len(idxs)), replace=False)\n",
    "\n",
    "        for i, idx in enumerate(chosen, 1):\n",
    "            text = data[idx].strip().replace(\"\\n\", \" \")\n",
    "            if len(text) > max_chars:\n",
    "                text = text[:max_chars] + \"...\"\n",
    "            \n",
    "            print(f\"\\n--- Exemplo {i} ---\")\n",
    "            print(text)\n",
    "        print(\"\\n\")\n",
    "\n",
    "show_examples(train_data, train_labels, train_source.target_names, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7afe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_pattern = re.compile(r\"\\b\\w+\\b\", flags=re.UNICODE)\n",
    "\n",
    "def simple_tokenizer(text, max_len=300):\n",
    "    tokens = token_pattern.findall(text.lower())\n",
    "    return tokens[:max_len]\n",
    "\n",
    "# Construção do Vocabulário\n",
    "token_counts = Counter()\n",
    "for text in train_data:\n",
    "    token_counts.update(simple_tokenizer(text))\n",
    "\n",
    "# Definição de índices especiais\n",
    "vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
    "min_freq = 2\n",
    "\n",
    "for word, count in token_counts.items():\n",
    "    if count >= min_freq:\n",
    "        vocab[word] = len(vocab)\n",
    "\n",
    "print(f\"Tamanho final do vocabulário: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac060ec",
   "metadata": {},
   "source": [
    "### Dataset e DataLoaders\n",
    "\n",
    "No PyTorch, a classe `Dataset` deve implementar os métodos `__len__` e `__getitem__`. No método `__getitem__`, realizamos a conversão imediata do texto cru para uma lista de índices baseada no vocabulário construído anteriormente.\n",
    "\n",
    "A função `collate_fn` é importante em modelos de sequência. Como as frases têm comprimentos variados, não podemos simplesmente empilhá-las em um tensor retangular. Esta função preenche (*pads*) as sequências mais curtas com o índice 0 (`<pad>`) até atingirem o comprimento da maior sequência presente no lote atual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f047939",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsGroupDataset(Dataset):\n",
    "    def __init__(self, data, targets, vocab, tokenizer):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        tokens = self.tokenizer(text)\n",
    "        # Mapeia token para índice, usa <unk> se não existir\n",
    "        indices = [self.vocab.get(token, self.vocab[\"<unk>\"]) for token in tokens]\n",
    "        \n",
    "        # Garante que não haja sequências vazias\n",
    "        if len(indices) == 0:\n",
    "            indices = [self.vocab[\"<unk>\"]]\n",
    "            \n",
    "        return torch.tensor(indices, dtype=torch.long), torch.tensor(self.targets[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9907da0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparâmetros de carregamento\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = NewsGroupDataset(train_data, train_labels, vocab, simple_tokenizer)\n",
    "test_dataset = NewsGroupDataset(test_data, test_labels, vocab, simple_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82af6b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    labels = []\n",
    "    text_list = []\n",
    "    \n",
    "    for (_text_indices, _label) in batch:\n",
    "        labels.append(_label)\n",
    "        text_list.append(_text_indices)\n",
    "    \n",
    "    # Pad sequences para que todos tenham o mesmo tamanho no batch\n",
    "    # padding_value=0 corresponde ao nosso <pad>\n",
    "    text_padded = pad_sequence(text_list, batch_first=True, padding_value=0)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return text_padded.to(device), labels.to(device)\n",
    "    \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a05c324",
   "metadata": {},
   "source": [
    "## Implementação do Modelo\n",
    "\n",
    "A arquitetura inicia-se com uma camada `nn.Embedding`. Matematicamente, ela funciona como uma tabela de consulta $E \\in \\mathbb{R}^{|V| \\times d}$, onde $|V|$ é o tamanho do vocabulário e $d$ a dimensão do embedding. Dado um índice de palavra $w$, a saída é o vetor $v_w$. Isso permite que a rede aprenda representações semânticas densas, onde palavras com significados similares estão geometricamente próximas.\n",
    "\n",
    "Em seguida, temos a camada LSTM (Long Short-Term Memory). A LSTM resolve o problema do desvanecimento do gradiente através de um estado de célula $C_t$ que carrega informação por longos períodos, controlado por portões (*gates*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39be81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, n_layers, dropout_prob):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Camada de Embedding: padding_idx=0 garante que o vetor do <pad> seja sempre zero e não treinado\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # Camada LSTM\n",
    "        self.lstm = nn.LSTM(input_size=embed_dim, \n",
    "                            hidden_size=hidden_dim, \n",
    "                            num_layers=n_layers, \n",
    "                            batch_first=True, \n",
    "                            dropout=dropout_prob if n_layers > 1 else 0)\n",
    "        \n",
    "        # Camada Linear de Saída\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "\n",
    "        lengths = (text != 0).sum(dim=1).cpu()\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        _, (hidden, _) = self.lstm(packed)\n",
    "        last_hidden = hidden[-1]\n",
    "\n",
    "        output = self.fc(self.dropout(last_hidden))\n",
    "        return output\n",
    "\n",
    "\n",
    "    # def forward(self, text):\n",
    "    #     # text: [batch_size, seq_len]\n",
    "    #     embedded = self.dropout(self.embedding(text))\n",
    "    #     # outputs: [batch_size, seq_len, hidden_dim]\n",
    "    #     outputs, _ = self.lstm(embedded)\n",
    "\n",
    "    #     # máscara de não-padding\n",
    "    #     mask = (text != 0)  # [batch_size, seq_len]\n",
    "    #     lengths = mask.sum(dim=1) - 1  # índice do último token não-pad\n",
    "\n",
    "    #     batch_indices = torch.arange(text.size(0), device=text.device)\n",
    "    #     last_outputs = outputs[batch_indices, lengths]  # [batch_size, hidden_dim]\n",
    "\n",
    "    #     output = self.fc(self.dropout(last_outputs))\n",
    "    #     return output\n",
    "        \n",
    "    # def forward(self, text):\n",
    "    #     # text: [batch_size, seq_len]\n",
    "        \n",
    "    #     embedded = self.dropout(self.embedding(text))\n",
    "    #     # embedded: [batch_size, seq_len, embed_dim]\n",
    "        \n",
    "    #     # LSTM output: output, (hidden, cell)\n",
    "    #     # Usamos apenas o último estado oculto da última camada\n",
    "    #     _, (hidden, _) = self.lstm(embedded)\n",
    "        \n",
    "    #     # hidden: [num_layers, batch_size, hidden_dim]\n",
    "    #     # Pegamos a última camada: hidden[-1]\n",
    "    #     last_hidden = hidden[-1]\n",
    "        \n",
    "    #     # last_hidden: [batch_size, hidden_dim]\n",
    "    #     output = self.fc(self.dropout(last_hidden))\n",
    "        \n",
    "    #     return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e786292",
   "metadata": {},
   "source": [
    "## Treinamento\n",
    "\n",
    "Nesta etapa, instanciamos o modelo e definimos a função de custo (`CrossEntropyLoss`) e o otimizador (`Adam`). A função de custo `CrossEntropyLoss` no PyTorch combina `LogSoftmax` e `NLLLoss`, sendo ideal para classificação multiclasse onde a saída da rede são *logits* (valores não normalizados).\n",
    "\n",
    "O loop de treinamento itera sobre o dataset, calcula o gradiente da perda em relação aos parâmetros e atualiza os pesos. O loop de avaliação monitora a generalização do modelo nos dados de teste sem atualizar os gradientes (`torch.no_grad()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6a4952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição de Hiperparâmetros\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 64\n",
    "hidden_dim = 128\n",
    "output_dim = len(categories)\n",
    "n_layers = 1\n",
    "dropout_prob = 0.5\n",
    "\n",
    "# Inicialização\n",
    "model = TextClassifier(vocab_size, embed_dim, hidden_dim, output_dim, n_layers, dropout_prob).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b1a295",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c98c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for text, labels in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text)\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = torch.max(predictions.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), correct / total\n",
    "\n",
    "def evaluate_epoch(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text, labels in iterator:\n",
    "            predictions = model(text)\n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            _, predicted = torch.max(predictions.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    return epoch_loss / len(iterator), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dce758",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "epochs = 25\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = evaluate_epoch(model, test_loader, criterion)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    print(f\"Época {epoch+1:02} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}% | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}%\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_accs, label='Train Acc')\n",
    "plt.plot(val_accs, label='Val Acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24941529",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for text, labels in test_loader:\n",
    "        outputs = model(text)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Matriz\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=categories,\n",
    "            yticklabels=categories)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f374cfaa",
   "metadata": {},
   "source": [
    "## Inferência\n",
    "\n",
    "Na inferência, é mandatório aplicar a mesma função `clean_tokenizer` utilizada no treino. Caracteres não alfabéticos devem ser removidos antes da consulta ao vocabulário para garantir a consistência da entrada do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f296959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, model, vocab, categories):\n",
    "    model.eval()\n",
    "    tokens = clean_tokenizer(text)\n",
    "    indices = [vocab.get(t, vocab[\"<unk>\"]) for t in tokens]\n",
    "    \n",
    "    if not indices:\n",
    "        indices = [vocab[\"<unk>\"]]\n",
    "        \n",
    "    tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(tensor)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        pred_idx = probs.argmax().item()\n",
    "        \n",
    "    return categories[pred_idx], probs.max().item()\n",
    "\n",
    "# Exemplo\n",
    "sample = \"Here is a press release from Medical Science Communications.   Results of GUSTO Heart Attack Study to be Released Friday  To: Assignment Desk, Medical Writer  Contact: Jim Augustine of Medical Science\"\n",
    "pred_class, confidence = predict(sample, model, vocab, categories)\n",
    "\n",
    "print(f\"Entrada: {sample}\")\n",
    "print(f\"Predição: {pred_class} ({confidence:.2%})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
