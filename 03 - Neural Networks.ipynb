{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c33b7875",
   "metadata": {},
   "source": [
    "# Redes Neurais\n",
    "\n",
    "Este notebook serve como um guia introdutório ao desenvolvimento de redes neurais utilizando a biblioteca PyTorch. Abordaremos os componentes fundamentais da biblioteca, desde a manipulação de tensores até a construção e o treinamento de um modelo de rede neural para classificação de imagens.\n",
    "\n",
    "## Conteúdos Abordados\n",
    "\n",
    "1.  **Tensores e Grafos Computacionais**: A base do PyTorch.\n",
    "2.  **O Módulo `torch.nn`**: Construindo camadas da rede.\n",
    "3.  **Funções de Ativação**: Introduzindo não linearidade.\n",
    "4.  **Construindo Modelos**: `nn.Sequential` e classes customizadas com `nn.Module`.\n",
    "5.  **Datasets e DataLoaders**: Gerenciando e preparando dados com o `torchvision`.\n",
    "6.  **Funções de Custo (Loss Functions)**: Quantificando o erro do modelo.\n",
    "7.  **Otimizadores**: Atualizando os pesos do modelo.\n",
    "8.  **Treinamento**: O ciclo completo de forward, backward e otimização."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f64f89",
   "metadata": {},
   "source": [
    "## 1. Tensores e Grafos Computacionais\n",
    "\n",
    "O `Tensor` é a estrutura de dados central do PyTorch. Trata-se de uma matriz multidimensional otimizada para operações em hardware especializado como GPUs. Os tensores são a base para a construção de grafos computacionais dinâmicos, que são essenciais para o cálculo automático de gradientes através do mecanismo de diferenciação automática, conhecido como `autograd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a888fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Criando um tensor a partir de uma lista Python\n",
    "data_list = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data_list, dtype=torch.float32)\n",
    "print(f\"Tensor a partir de lista:\\n {x_data}\\n\")\n",
    "\n",
    "# Funções de criação de tensores\n",
    "x_ones = torch.ones_like(x_data) # Cria um tensor de 'uns' com o mesmo formato de x_data\n",
    "print(f\"Tensor de 'uns':\\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # Cria um tensor com valores aleatórios\n",
    "print(f\"Tensor aleatório:\\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8909834a",
   "metadata": {},
   "source": [
    "### O Grafo Computacional e o `autograd`\n",
    "\n",
    "O coração do PyTorch é o `autograd`, seu mecanismo de diferenciação automática. Para que o `autograd` funcione, ele constrói um **grafo computacional** dinamicamente. Este grafo é uma representação de todas as operações executadas nos tensores.\n",
    "\n",
    "Nesta seção, vamos decompor passo a passo como esse grafo é construído e utilizado para calcular gradientes, usando como exemplo a função:\n",
    "$$\n",
    "L = (ab - c)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f90b23a",
   "metadata": {},
   "source": [
    "#### Passo 1: Definindo os Tensores Folha com `requires_grad`\n",
    "\n",
    "O processo começa com a definição dos tensores de entrada. Estes são as \"folhas\" do nosso grafo. Para que o PyTorch rastreie as operações e calcule os gradientes em relação a eles, devemos definir seu atributo `requires_grad` como `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e60d3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Definindo os tensores de entrada (folhas do grafo)\n",
    "# Estes são os parâmetros em relação aos quais queremos os gradientes\n",
    "a = torch.tensor(4.0, requires_grad=True)\n",
    "b = torch.tensor(5.0, requires_grad=True)\n",
    "c = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "print(f\"Tensor 'a': {a}\")\n",
    "print(f\"Tensor 'b': {b}\")\n",
    "print(f\"Tensor 'c': {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a5a71c",
   "metadata": {},
   "source": [
    "#### Passo 2: Construindo o Grafo no Forward Pass\n",
    "\n",
    "À medida que executamos as operações (o *forward pass*), o PyTorch constrói o grafo dinamicamente. Cada novo tensor resultante de uma operação armazena uma referência à função que o criou através do atributo `grad_fn`. Isso cria uma \"história\" computacional que permite a retropropagação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7ef489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executando o forward pass para construir o grafo\n",
    "z = a * b - c\n",
    "L = z**2\n",
    "\n",
    "print(f\"Resultado intermediário 'z': {z}\")\n",
    "print(f\"  - z.grad_fn: {z.grad_fn}\\n\") # z foi criado por uma subtração (SubBackward0)\n",
    "\n",
    "print(f\"Resultado final 'L': {L}\")\n",
    "print(f\"  - L.grad_fn: {L.grad_fn}\") # L foi criado por uma potenciação (PowBackward0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a9031a",
   "metadata": {},
   "source": [
    "#### Passo 3: Calculando os Gradientes com `.backward()`\n",
    "\n",
    "Com o grafo construído, podemos calcular os gradientes usando o método `.backward()` no tensor de saída final (que deve ser um escalar). Esta chamada inicia o processo de retropropagação a partir de `L`, aplicando a regra da cadeia para calcular as derivadas parciais de `L` em relação a cada tensor folha.\n",
    "\n",
    "Para o nosso exemplo, o `autograd` calcula:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a} = \\frac{\\partial L}{\\partial z}\\frac{\\partial z}{\\partial a} = 2(ab - c)b\n",
    "$$\n",
    "E de forma análoga para `b` e `c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7776a3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicia a retropropagação a partir de L\n",
    "L.backward()\n",
    "print(\"O método L.backward() foi executado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75abb038",
   "metadata": {},
   "source": [
    "#### Passo 4: Acessando os Gradientes Calculados\n",
    "\n",
    "Após a execução de `.backward()`, os gradientes calculados são acumulados no atributo `.grad` dos tensores folha (aqueles que foram inicializados com `requires_grad=True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3080aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Os gradientes agora estão disponíveis no atributo .grad de cada tensor folha\n",
    "print(f\"Gradiente dL/da: {a.grad}\")\n",
    "print(f\"  - Cálculo manual: 2 * (4*5 - 2) * 5 = {2 * (4*5 - 2) * 5.0}\\n\")\n",
    "\n",
    "print(f\"Gradiente dL/db: {b.grad}\")\n",
    "print(f\"  - Cálculo manual: 2 * (4*5 - 2) * 4 = {2 * (4*5 - 2) * 4.0}\\n\")\n",
    "\n",
    "print(f\"Gradiente dL/dc: {c.grad}\")\n",
    "print(f\"  - Cálculo manual: 2 * (4*5 - 2) * (-1) = {2 * (4*5 - 2) * (-1.0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc463e3",
   "metadata": {},
   "source": [
    "#### Desligando o Rastreamento com `torch.no_grad()`\n",
    "\n",
    "Para fases de inferência ou avaliação, onde não precisamos de gradientes, é crucial desativar o rastreamento para economizar memória e acelerar a computação. O gerenciador de contexto `with torch.no_grad():` instrui o PyTorch a não construir o grafo computacional para nenhuma operação dentro de seu escopo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798f133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Rastreamento de gradientes está ativado por padrão para 'a': {a.requires_grad}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"--- Entrando no bloco no_grad ---\")\n",
    "    \n",
    "    # A mesma operação é executada aqui\n",
    "    L_no_grad = (a * b - c)**2\n",
    "    \n",
    "    print(f\"Resultado L_no_grad: {L_no_grad}\")\n",
    "    # O tensor resultante não possui grad_fn, pois a operação não foi rastreada\n",
    "    print(f\"L_no_grad.grad_fn: {L_no_grad.grad_fn}\")\n",
    "    print(f\"L_no_grad.requires_grad: {L_no_grad.requires_grad}\")\n",
    "    \n",
    "    print(\"--- Saindo do bloco no_grad ---\")\n",
    "\n",
    "print(f\"O rastreamento de gradientes para 'a' continua ativado fora do bloco: {a.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ec41d9",
   "metadata": {},
   "source": [
    "## 2. O Módulo `torch.nn`\n",
    "\n",
    "O `torch.nn` é o módulo do PyTorch para a construção de redes neurais. Ele fornece um conjunto de blocos de construção, como camadas (`Layers`), funções de ativação (`Activation Functions`), funções de custo (`Loss Functions`) e contêineres (`Containers`). Uma \"camada\" no `torch.nn` é um objeto que encapsula tanto os pesos (parâmetros) quanto as operações a serem aplicadas nos dados de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2222abd9",
   "metadata": {},
   "source": [
    "### Camada Linear: `nn.Linear`\n",
    "\n",
    "A camada mais fundamental é a `nn.Linear`, que aplica uma transformação afim aos dados de entrada: $y = xW^T + b$.\n",
    "\n",
    "-   `in_features`: a dimensionalidade do espaço de entrada.\n",
    "-   `out_features`: a dimensionalidade do espaço de saída.\n",
    "\n",
    "Os tensores de peso (`weight`, $W$) e de viés (`bias`, $b$) são encapsulados como `nn.Parameter`, uma subclasse de `torch.Tensor` que os registra automaticamente como parâmetros de um `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ee0e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Definindo uma camada linear\n",
    "# Entrada: 10 features\n",
    "# Saída: 5 features\n",
    "linear_layer = nn.Linear(in_features=10, out_features=5)\n",
    "\n",
    "# Criando um tensor de entrada de exemplo (um \"batch\" com 3 amostras)\n",
    "# O formato é (batch_size, in_features)\n",
    "input_tensor = torch.randn(3, 10)\n",
    "\n",
    "# Passando os dados pela camada\n",
    "output_tensor = linear_layer(input_tensor)\n",
    "\n",
    "print(f\"Formato do tensor de entrada: {input_tensor.shape}\")\n",
    "print(f\"Formato do tensor de saída: {output_tensor.shape}\")\n",
    "print(f\"\\nPesos (weights) da camada:\\n {linear_layer.weight.shape}\")\n",
    "print(f\"Viés (bias) da camada:\\n {linear_layer.bias.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db32237",
   "metadata": {},
   "source": [
    "## 3. Funções de Ativação\n",
    "\n",
    "Funções de ativação introduzem não linearidade no modelo, capacitando-o a aprender fronteiras de decisão complexas. Elas são aplicadas elemento a elemento na saída de uma camada.\n",
    "\n",
    "### Funções Comuns\n",
    "\n",
    "-   **Sigmoid**: Comprime os valores de entrada no intervalo $(0, 1)$. Utilizada historicamente em camadas ocultas e atualmente em camadas de saída para classificação binária.\n",
    "    $$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "-   **Tanh (Tangente Hiperbólica)**: Comprime os valores de entrada no intervalo $(-1, 1)$. Geralmente converge mais rápido que a Sigmoid por ser centrada em zero.\n",
    "    $$ \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $$\n",
    "-   **ReLU (Rectified Linear Unit)**: A função de ativação mais utilizada em redes profundas. É computacionalmente eficiente e ajuda a mitigar o problema do desaparecimento do gradiente (vanishing gradient).\n",
    "    $$ \\text{ReLU}(x) = \\max(0, x) $$\n",
    "-   **Leaky ReLU**: Uma variação da ReLU que permite a passagem de um pequeno gradiente negativo, prevenindo o problema dos \"neurônios mortos\".\n",
    "    $$ \\text{LeakyReLU}(x) = \\begin{cases} x, & \\text{if } x > 0 \\\\ \\alpha x, & \\text{otherwise} \\end{cases} $$\n",
    "-   **Softmax**: Transforma um vetor de números reais (logits) em uma distribuição de probabilidade sobre múltiplas classes. Utilizada na camada de saída para classificação multiclasse.\n",
    "    $$ \\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As funções de ativação também estão no módulo nn\n",
    "relu_activation = nn.ReLU()\n",
    "leaky_relu_activation = nn.LeakyReLU()\n",
    "\n",
    "# Aplicando a ativação na saída da camada linear anterior\n",
    "output_with_relu = relu_activation(output_tensor)\n",
    "output_with_leaky_relu = leaky_relu_activation(output_tensor)\n",
    "\n",
    "print(f\"Saída da camada linear:\\n {output_tensor}\\n\")\n",
    "print(f\"Saída após ativação ReLU:\\n {output_with_relu}\\n\")\n",
    "print(f\"Saída após ativação Leaky ReLU:\\n {output_with_leaky_relu}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c7291d",
   "metadata": {},
   "source": [
    "## 4. Construindo Modelos\n",
    "\n",
    "O PyTorch oferece duas maneiras principais de agrupar camadas para formar um modelo completo.\n",
    "\n",
    "### `nn.Sequential`\n",
    "\n",
    "`nn.Sequential` é um contêiner que recebe uma sequência de módulos (camadas, funções de ativação, etc.) e os executa na ordem em que são passados. É uma forma rápida e simples de criar modelos onde os dados fluem sequencialmente através das camadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965dde91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construindo um modelo simples com nn.Sequential\n",
    "# Input (784) -> Linear (128) -> ReLU -> Linear (10) -> Output\n",
    "model_sequential = nn.Sequential(\n",
    "    nn.Linear(in_features=784, out_features=128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=128, out_features=10)\n",
    ")\n",
    "\n",
    "# Criando um tensor de entrada de exemplo (batch com 64 imagens achatadas de 28x28)\n",
    "# 28 * 28 = 784\n",
    "input_images = torch.randn(64, 784)\n",
    "\n",
    "# Forward pass através do modelo sequencial\n",
    "logits = model_sequential(input_images) # 'logits' são as saídas brutas antes da probabilidade\n",
    "\n",
    "print(f\"Formato da saída do modelo: {logits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c230087",
   "metadata": {},
   "source": [
    "### Classes customizadas com `nn.Module`\n",
    "\n",
    "Para modelos mais complexos, como aqueles com múltiplos caminhos de entrada/saída ou lógicas de *forward pass* não sequenciais (e.g., redes residuais), a abordagem recomendada é criar uma classe que herda de `nn.Module`.\n",
    "\n",
    "Toda classe de modelo customizada deve:\n",
    "1.  Herdar de `torch.nn.Module`.\n",
    "2.  Definir as camadas no construtor `__init__(self)`.\n",
    "3.  Implementar a lógica do *forward pass* no método `forward(self, x)`.\n",
    "\n",
    "O método `backward()` é gerenciado automaticamente pelo `autograd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2492bef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo o mesmo modelo anterior, mas agora como uma classe\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer2(out)\n",
    "        return out\n",
    "\n",
    "# Instanciando o modelo\n",
    "model_class = NeuralNetwork(input_size=784, hidden_size=128, num_classes=10)\n",
    "\n",
    "# O uso é idêntico\n",
    "logits_class = model_class(input_images)\n",
    "\n",
    "print(f\"Formato da saída do modelo (classe): {logits_class.shape}\")\n",
    "print(f\"\\nEstrutura do modelo:\\n{model_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e0aed0",
   "metadata": {},
   "source": [
    "## 5. Datasets e DataLoaders\n",
    "\n",
    "Para treinar um modelo, precisamos de um pipeline de dados eficiente. O PyTorch oferece duas primitivas de dados fundamentais para isso: `torch.utils.data.Dataset` e `torch.utils.data.DataLoader`.\n",
    "\n",
    "### A Estrutura de um `Dataset` Customizado\n",
    "\n",
    "A classe `Dataset` é uma classe abstrata que representa uma fonte de dados. Para criar seu próprio dataset, você precisa herdar desta classe e sobrescrever três métodos especiais (métodos mágicos):\n",
    "\n",
    "1.  `__init__(self, ...)`: O construtor da classe. É executado uma única vez ao instanciar o dataset. É aqui que você normalmente faria o carregamento inicial dos dados (ex: ler um arquivo CSV, encontrar os caminhos das imagens em um diretório).\n",
    "\n",
    "2.  `__len__(self)`: Este método deve retornar o número total de amostras no seu dataset. O `DataLoader` utiliza essa informação para saber o tamanho do dataset e definir os índices.\n",
    "\n",
    "3.  `__getitem__(self, idx)`: Este método é responsável por carregar e retornar **uma única amostra** do dataset, dado um índice `idx`. É aqui que transformações nos dados (como data augmentation ou normalização) são frequentemente aplicadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dcbac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Exemplo prático de um Dataset customizado com tensores\n",
    "class SimpleTensorDataset(Dataset):\n",
    "    def __init__(self, features_tensor, labels_tensor):\n",
    "        self.features = features_tensor\n",
    "        self.labels = labels_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        # Retorna o número total de amostras\n",
    "        return self.features.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retorna uma tupla (feature, label) para um dado índice\n",
    "        feature_sample = self.features[idx]\n",
    "        label_sample = self.labels[idx]\n",
    "        return feature_sample, label_sample\n",
    "\n",
    "# Criando dados de exemplo\n",
    "features = torch.randn(500, 64) # 500 amostras, 64 features cada\n",
    "labels = torch.randint(0, 2, (500,)) # 500 rótulos (0 ou 1)\n",
    "\n",
    "# Instanciando o nosso dataset customizado\n",
    "custom_dataset = SimpleTensorDataset(features, labels)\n",
    "\n",
    "# Verificando a implementação dos métodos\n",
    "print(f\"Tamanho total do dataset: {len(custom_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2062638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pegando a primeira amostra (chamando __getitem__ com idx=0)\n",
    "first_sample_features, first_sample_label = custom_dataset[0]\n",
    "print(f\"Primeira amostra (features): {first_sample_features.shape}\")\n",
    "print(f\"Primeira amostra (label): {first_sample_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f3e2cc",
   "metadata": {},
   "source": [
    "### O Papel do `DataLoader`\n",
    "\n",
    "Uma vez que temos um objeto `Dataset`, que sabe como acessar amostras individuais, precisamos de uma forma eficiente de iterar sobre ele durante o treinamento. É aqui que entra o `DataLoader`.\n",
    "\n",
    "O `DataLoader` é um iterador que envolve um `Dataset` e automatiza o processo de criação de mini-lotes (*mini-batches*). Suas principais funcionalidades são:\n",
    "\n",
    "-   **Agrupamento em Lotes (Batching)**: Agrupa múltiplas amostras retornadas pelo `__getitem__` do `Dataset` para formar um lote (batch) de dados.\n",
    "-   **Embaralhamento (Shuffling)**: Permite embaralhar os dados a cada época (`shuffle=True`) para evitar que o modelo aprenda a ordem dos dados e melhore a generalização.\n",
    "-   **Carregamento Paralelo (Parallel Loading)**: Pode usar múltiplos subprocessos (`num_workers`) para carregar os dados em paralelo, evitando que o carregamento de dados se torne um gargalo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248ca3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Agora, usamos o DataLoader com o 'custom_dataset' que criamos anteriormente\n",
    "data_loader = DataLoader(dataset=custom_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# O DataLoader é um iterável. Podemos usá-lo em um laço 'for' ou com 'next(iter())'\n",
    "# para obter o próximo lote de dados.\n",
    "first_batch_features, first_batch_labels = next(iter(data_loader))\n",
    "\n",
    "print(f\"Formato do batch de features: {first_batch_features.shape}\")\n",
    "print(f\"Formato do batch de rótulos: {first_batch_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bb3e8e",
   "metadata": {},
   "source": [
    "### Datasets Disponibilizados: `torchvision`\n",
    "\n",
    "Compreendida a estrutura de um `Dataset` e o papel do `DataLoader`, podemos apreciar a conveniência de bibliotecas como a `torchvision`. Ela já fornece implementações prontas da classe `Dataset` para datasets populares como o MNIST, que seguem a mesma estrutura que acabamos de ver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40c9ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Definindo transformações para os dados\n",
    "# ToTensor() converte a imagem PIL (H x W x C) no intervalo [0, 255]\n",
    "# para um FloatTensor (C x H x W) no intervalo [0.0, 1.0].\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # Média e desvio padrão do MNIST\n",
    "])\n",
    "\n",
    "# Baixando o dataset de treino\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# Baixando o dataset de teste\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad79391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando os DataLoaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Iterando sobre o DataLoader para ver o formato de um batch\n",
    "images, labels = next(iter(train_loader))\n",
    "print(f\"Formato do batch de imagens: {images.shape}\") # (batch_size, channels, height, width)\n",
    "print(f\"Formato do batch de rótulos: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26da9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Função para exibir uma imagem\n",
    "def imshow(img):\n",
    "    # A normalização precisa ser revertida para a visualização correta\n",
    "    # Média = 0.1307, Desvio Padrão = 0.3081\n",
    "    img = img * 0.3081 + 0.1307 \n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off') # Remove os eixos\n",
    "    plt.show()\n",
    "\n",
    "# Pega um lote (batch) de imagens de treino\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Mostra as imagens em uma grade\n",
    "# O make_grid organiza o lote de imagens em uma única imagem-grade\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "# Imprime os rótulos correspondentes\n",
    "print('Rótulos: ', ' '.join(f'{labels[j].item()}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9b6e42",
   "metadata": {},
   "source": [
    "## 6. Funções de Custo (Loss Functions)\n",
    "\n",
    "A função de custo $J(\\theta)$ mede a discrepância entre a saída prevista pelo modelo $\\hat{y}$ e o valor real $y$. O objetivo do treinamento é encontrar os parâmetros $\\theta$ que minimizam $J(\\theta)$.\n",
    "\n",
    "### Funções Comuns\n",
    "\n",
    "-   **Mean Squared Error (MSE)**: Utilizada principalmente para tarefas de regressão. Calcula a média dos erros quadráticos entre a previsão e o valor real.\n",
    "    $$ J_{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 $$\n",
    "-   **Binary Cross-Entropy (BCE)**: Utilizada para classificação binária. Geralmente é combinada com uma camada de saída Sigmoid.\n",
    "    $$ J_{BCE} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] $$\n",
    "-   **Cross-Entropy Loss**: Utilizada para classificação multiclasse. No PyTorch, `nn.CrossEntropyLoss` combina `nn.LogSoftmax` e `nn.NLLLoss`, sendo numericamente mais estável. Ela espera como entrada os *logits* brutos do modelo e os rótulos de classe como inteiros.\n",
    "    $$ J_{CE} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{p}_{i,c}) $$\n",
    "    Onde $y_{i,c}$ é 1 se a amostra $i$ pertence à classe $c$ (0 caso contrário), e $\\hat{p}_{i,c}$ é a probabilidade prevista pelo modelo para a amostra $i$ pertencer à classe $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc54866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando a função de custo para classificação multiclasse\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Exemplo de uso:\n",
    "# Saída do modelo (logits) para um batch de 3 amostras e 10 classes\n",
    "output_logits = torch.randn(3, 10)\n",
    "# Rótulos verdadeiros\n",
    "target_labels = torch.tensor([1, 4, 9]) # Classe 1, Classe 4, Classe 9\n",
    "\n",
    "# Calculando a perda\n",
    "loss = loss_function(output_logits, target_labels)\n",
    "print(f\"Valor da perda (loss): {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7e0143",
   "metadata": {},
   "source": [
    "## 7. Otimizadores\n",
    "\n",
    "O otimizador implementa o algoritmo de atualização dos parâmetros do modelo, $\\theta$, com base nos gradientes da função de custo, $\\nabla_{\\theta} J(\\theta)$. O objetivo é convergir para um mínimo (local ou global) da função de custo.\n",
    "\n",
    "O algoritmo mais fundamental é o **Stochastic Gradient Descent (SGD)**. A regra de atualização para um parâmetro $\\theta$ no passo de tempo $t$ é definida como:\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta \\nabla_{\\theta} J(\\theta_t)\n",
    "$$\n",
    "onde $\\eta$ é a taxa de aprendizado (*learning rate*), um hiperparâmetro que controla o tamanho do passo na direção do gradiente negativo.\n",
    "\n",
    "Otimizadores mais avançados, como o **Adam (Adaptive Moment Estimation)**, utilizam taxas de aprendizado adaptativas para cada parâmetro, mantendo uma estimativa do primeiro momento (a média) e do segundo momento (a variância não centrada) dos gradientes, o que frequentemente leva a uma convergência mais rápida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b6b718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando o modelo\n",
    "learning_rate = 0.001\n",
    "model = NeuralNetwork(input_size=784, hidden_size=128, num_classes=10)\n",
    "\n",
    "# Instanciando o otimizador Adam\n",
    "# Passamos os parâmetros do modelo que devem ser otimizados (model.parameters())\n",
    "# e a taxa de aprendizado (lr).\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# O otimizador possui métodos cruciais:\n",
    "# optimizer.zero_grad(): Zera os gradientes de todos os parâmetros antes de um novo cálculo de backward pass.\n",
    "#   Isto é necessário porque o método .backward() acumula os gradientes por padrão.\n",
    "#\n",
    "# optimizer.step(): Atualiza os parâmetros do modelo usando a lógica do otimizador (e.g., Adam) e os gradientes\n",
    "#   armazenados no atributo .grad de cada parâmetro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02499521",
   "metadata": {},
   "source": [
    "## 8. Treinamento: O Ciclo Completo\n",
    "\n",
    "O treinamento de uma rede neural consiste em um loop que itera sobre o dataset por várias \"épocas\" (*epochs*). Uma época é uma passagem completa por todo o dataset de treinamento. Dentro de cada época, iteramos sobre os *batches* de dados.\n",
    "\n",
    "Para cada *batch*, o ciclo de treinamento é:\n",
    "1.  **Zerar os gradientes**: Chamar `optimizer.zero_grad()`.\n",
    "2.  **Forward Pass**: Passar os dados de entrada pelo modelo para obter as previsões (logits).\n",
    "3.  **Calcular a Perda**: Comparar as previsões com os rótulos verdadeiros usando a função de custo.\n",
    "4.  **Backward Pass**: Chamar `loss.backward()` para calcular os gradientes da perda em relação a cada parâmetro do modelo.\n",
    "5.  **Atualizar os Pesos**: Chamar `optimizer.step()` para que o otimizador atualize os pesos com base nos gradientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21914f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Hiperparâmetros\n",
    "num_epochs = 5\n",
    "input_size = 784 # 28x28\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Dispositivo (GPU se disponível, senão CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando o dispositivo: {device}\")\n",
    "\n",
    "# Instanciando o modelo, função de custo e otimizador e movendo o modelo para o dispositivo\n",
    "model = NeuralNetwork(input_size, hidden_size, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Listas para armazenar as métricas de cada época\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Loop de treinamento\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Treinamento ---\n",
    "    model.train() # Coloca o modelo em modo de treinamento\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    progress_bar_train = tqdm(train_loader, desc=f'Época [{epoch+1}/{num_epochs}] Treino')\n",
    "    \n",
    "    for images, labels in progress_bar_train:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward e otimização\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calcula a acurácia de treino\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        progress_bar_train.set_postfix({'Perda Treino': f'{loss.item():.4f}'})\n",
    "        \n",
    "    epoch_train_loss = running_loss / len(train_loader)\n",
    "    epoch_train_acc = 100 * correct_train / total_train\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    train_accuracies.append(epoch_train_acc)\n",
    "\n",
    "    # --- Validação ---\n",
    "    model.eval() # Coloca o modelo em modo de avaliação\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    progress_bar_val = tqdm(test_loader, desc=f'Época [{epoch+1}/{num_epochs}] Validação')\n",
    "    \n",
    "    with torch.no_grad(): # Desabilita o cálculo de gradientes\n",
    "        for images, labels in progress_bar_val:\n",
    "            images = images.reshape(-1, 28*28).to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item()\n",
    "            \n",
    "            # Calcula a acurácia de validação\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "            \n",
    "            progress_bar_val.set_postfix({'Perda Val': f'{loss.item():.4f}'})\n",
    "\n",
    "    epoch_val_loss = running_val_loss / len(test_loader)\n",
    "    epoch_val_acc = 100 * correct_val / total_val\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    val_accuracies.append(epoch_val_acc)\n",
    "    \n",
    "    print(f'Fim da Época [{epoch+1}/{num_epochs}] | '\n",
    "          f'Perda Treino: {epoch_train_loss:.4f}, Acurácia Treino: {epoch_train_acc:.2f}% | '\n",
    "          f'Perda Validação: {epoch_val_loss:.4f}, Acurácia Validação: {epoch_val_acc:.2f}%')\n",
    "\n",
    "print(\"\\nTreinamento concluído!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2525033",
   "metadata": {},
   "source": [
    "### Visualização das Curvas de Aprendizado\n",
    "\n",
    "As curvas de aprendizado plotam as métricas de desempenho (como perda e acurácia) para os conjuntos de treinamento e validação ao longo das épocas. Elas são ferramentas de diagnóstico essenciais:\n",
    "\n",
    "-   **Curva de Perda (Loss Curve)**: Mostra a evolução da função de custo. Idealmente, ambas as perdas (treino e validação) devem diminuir. Se a perda de validação começar a aumentar enquanto a de treino continua caindo, é um sinal claro de *overfitting*.\n",
    "-   **Curva de Acurácia (Accuracy Curve)**: Mostra a evolução da acurácia. Idealmente, ambas devem aumentar e convergir. Uma grande diferença entre a acurácia de treino e a de validação também indica *overfitting*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a99a8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando os plots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot da Curva de Perda\n",
    "axs[0].plot(train_losses, label='Perda de Treino')\n",
    "axs[0].plot(val_losses, label='Perda de Validação')\n",
    "axs[0].set_title(\"Curvas de Perda\")\n",
    "axs[0].set_xlabel(\"Época\")\n",
    "axs[0].set_ylabel(\"Perda (Cross-Entropy Loss)\")\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Plot da Curva de Acurácia\n",
    "axs[1].plot(train_accuracies, label='Acurácia de Treino')\n",
    "axs[1].plot(val_accuracies, label='Acurácia de Validação')\n",
    "axs[1].set_title(\"Curvas de Acurácia\")\n",
    "axs[1].set_xlabel(\"Época\")\n",
    "axs[1].set_ylabel(\"Acurácia (%)\")\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0b83e7",
   "metadata": {},
   "source": [
    "### Avaliação do Modelo\n",
    "\n",
    "Após o treinamento, é fundamental avaliar a performance do modelo em dados que ele nunca viu, ou seja, o conjunto de teste. Durante a avaliação, não precisamos calcular gradientes, o que economiza memória e computação. Para isso, usamos o contexto `torch.no_grad()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c09725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colocando o modelo em modo de avaliação (desativa camadas como Dropout, se houver)\n",
    "model.eval()\n",
    "\n",
    "# O contexto torch.no_grad() desabilita o cálculo de gradientes\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # A classe com o maior logit é a previsão\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Acurácia do modelo no dataset de teste: {100 * correct / total:.2f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b287eaec",
   "metadata": {},
   "source": [
    "### Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bf7fc9",
   "metadata": {},
   "source": [
    "#### Exercício 1: Praticando com o `autograd`\n",
    "\n",
    "Calcule os gradientes da função $L = 2a^3 + 3b^2$ em relação a `a` e `b`.\n",
    "\n",
    "1.  Defina dois tensores, `a` e `b`, com os valores `a=2.0` e `b=5.0`. Lembre-se de definir `requires_grad=True`.\n",
    "2.  Escreva a operação em PyTorch para calcular `L`.\n",
    "3.  Use `.backward()` para calcular os gradientes.\n",
    "4.  Imprima `a.grad` e `b.grad`.\n",
    "5.  **Verificação:** Calcule as derivadas parciais $\\frac{\\partial L}{\\partial a}$ e $\\frac{\\partial L}{\\partial b}$ manualmente e confirme se os resultados correspondem aos do PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bf6d98",
   "metadata": {},
   "source": [
    "#### Exercício 2: Aprofundando a Arquitetura da Rede\n",
    "\n",
    "Modifique a classe `NeuralNetwork` para que ela tenha **duas camadas ocultas** em vez de uma. A nova arquitetura deve ser:\n",
    "`Entrada (784) -> Camada Linear (256 neurônios) -> ReLU -> Camada Linear (128 neurônios) -> ReLU -> Saída (10 neurônios)`. Em seguida, treine o novo modelo e compare com as curvas de treinamento do modelo original."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba96cb0",
   "metadata": {},
   "source": [
    "#### Exercício 3: Otimizadores e Taxa de Aprendizado\n",
    "\n",
    "Altere a taxa de aprendizado no loop de treinamento. Utilize valores maiores (como `lr=1.0`) e menores (como `lr=0.0001`) do que o original. Como as curvas de treinamento se comportaram em cada ocasião?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87afae1",
   "metadata": {},
   "source": [
    "#### Exercício 4: Criando um Dataset para Regressão\n",
    "\n",
    "Crie uma classe de `Dataset` customizada para um problema de regressão simples. O dataset deve gerar dados sintéticos onde `y` é uma função linear de `x` com um pouco de ruído.\n",
    "\n",
    "Crie uma classe `RegressionDataset` que herde de `torch.utils.data.Dataset`. No `__init__`, crie um tensor `X` com 1000 pontos aleatórios entre -10 e 10 e crie um tensor `y` correspondente usando a fórmula $y = 5x - 3 + \\text{ruído}$ (use `torch.randn` para o ruído). Implemente os métodos `__len__` e `__getitem__` e use um `DataLoader` para extrair o primeiro lote (batch). Imprima o formato dos tensores de features e labels do lote para confirmar que seu pipeline de dados está funcionando corretamente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
